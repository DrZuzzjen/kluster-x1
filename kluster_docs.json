{
  "scraped_at": 1749147206.9543304,
  "total_pages": 41,
  "topics": {
    "üéØ Fine-tuning": {
      "Fine Tuning": {
        "url": "https://docs.kluster.ai/get-started/fine-tuning/api/",
        "title": "Fine-tuning with the kluster.ai API | kluster.ai Docs",
        "content": "Fine-tuning with the kluster.ai API ÔºÉ The kluster.ai API lets you automate and integrate fine-tuning into your development workflows. You can create, manage, and monitor fine-tuning jobs directly from your code, making it easy to customize models for your specific needs. This guide provides a practical overview of the fine-tuning process using the API. It covers the required data format, how to upload your dataset, and how to launch and monitor a fine-tuning job. For a step-by-step walkthrough, see the linked tutorial in the tips below. Prerequisites ÔºÉ Before getting started with fine-tuning, ensure you have the following: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. Prepared dataset : You need data formatted according to kluster.ai's requirements for fine-tuning (detailed below). Supported models ÔºÉ kluster.ai currently supports fine-tuning for the following models: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Note You can query the models endpoint in the API and filter for the tag \"fine-tunable.\" Fine-tuning workflow ÔºÉ Fine‚Äëtuning a model with the kluster.ai API follows a straightforward five‚Äëstep workflow: Prepare your data : Collect and structure high‚Äëquality JSONL training examples that reflect the task you want the model to learn. Upload your training file : Send the JSONL file to kluster.ai and note the returned file_id . Create the fine‚Äëtuning job : Launch a fine‚Äëtuning job specifying the base model and training file_id (plus any optional hyperparameters). Monitor job progress : Poll the job endpoint (or subscribe to webhooks) until the job reaches the succeeded state. Use your fine‚Äëtuned model : Invoke the model name returned by the job for inference in your application or the kluster.ai playground. The following sections will provide a closer look at each step. Prepare your data ÔºÉ High-quality, well-formatted data is crucial for successful fine-tuning: Format : Data must be in JSONL format, where each line is a valid JSON object representing a training example. Structure : Each JSON object should contain a messages array with system, user, and assistant messages. Example format : { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a JSON Generation Specialist. Convert user requests into properly formatted JSON.\" }, { \"role\" : \"user\" , \"content\" : \"Create a configuration for a web application with name 'TaskMaster', version 1.2.0, and environment set to development.\" }, { \"role\" : \"assistant\" , \"content\" : \"{\\n \\\"application\\\": {\\n \\\"name\\\": \\\"TaskMaster\\\",\\n \\\"version\\\": \\\"1.2.0\\\",\\n \\\"environment\\\": \\\"development\\\"\\n }\\n}\" } ] } Quantity : The minimum requirement is 10 examples, but more diverse and high-quality examples yield better results. Quality : Ensure your data accurately represents the task you want the model to perform. Data preparation For a detailed walkthrough of data preparation, see the Fine-tuning sentiment analysis tutorial . Find Llama datasets on Hugging Face There is a wide range of datasets suitable for Llama model fine-tuning on Hugging Face Datasets . Browse trending and community-curated datasets to accelerate your data preparation. Set up the client ÔºÉ First, install the OpenAI Python library: pip install openai Then initialize the client with the kluster.ai base URL: from openai import OpenAI api_key = getpass ( \"Enter your kluster.ai API key: \" ) # Set up the client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key ) Upload your training file ÔºÉ Once your data is prepared, upload it to the kluster.ai platform: # Upload fine-tuning file (for files under 100MB) with open ( 'training_data.jsonl' , 'rb' ) as file : upload_response = client . files . create ( file = file , purpose = \"fine-tune\" # Important: specify \"fine-tune\" as the purpose ) # Get the file ID file_id = upload_response . id print ( f \"File uploaded successfully. File ID: { file_id } \" ) File size & upload limits Each fine-tuning file must be ‚â§ 100 MB on both the free and standard tiers (the standard tier simply allows more total examples). When your dataset approaches this limit, use the chunked upload method for reliable multi-part uploads. Create a fine-tuning job ÔºÉ After uploading your data, initiate the fine-tuning job: # Model model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" # Create fine-tune job fine_tuning_job = client . fine_tuning . jobs . create ( training_file = file_id , model = model , # Optional hyperparameters # hyperparameters={ # \"batch_size\": 3, # \"n_epochs\": 2, # \"learning_rate_multiplier\": 0.08 # } ) Monitor job progress ÔºÉ Track the status of your fine-tuning job: # Retrieve job status job_status = client . fine_tuning . jobs . retrieve ( fine_tuning_job . id ) print ( f \"Job status: { job_status . status } \" ) Use your fine-tuned model ÔºÉ Once your fine-tuning job completes successfully, you will receive a unique fine-tuned model name that you can use for inference: # Get the fine-tuned model name finished_job = client . fine_tuning . jobs . retrieve ( fine_tuning_job . id ) fine_tuned_model = finished_job . fine_tuned_model # Use the fine-tuned model for inference response = client . chat . completions . create ( model = fine_tuned_model , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a JSON Generation Specialist. Convert user requests into properly formatted JSON.\" }, { \"role\" : \"user\" , \"content\" : \"Create a configuration for a web application with name 'TaskMaster', version 1.2.0, and environment set to development.\" } ] ) You can view the end-to-end python script below: fine-tune.py from getpass import getpass from openai import OpenAI api_key = getpass ( \"Enter your kluster.ai API key: \" ) # Set up the client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key ) # Upload fine-tuning file (for files under 100MB) with open ( 'training_data.jsonl' , 'rb' ) as file : upload_response = client . files . create ( file = file , purpose = \"fine-tune\" # Important: specify \"fine-tune\" as the purpose ) # Get the file ID file_id = upload_response . id print ( f \"File uploaded successfully. File ID: { file_id } \" ) # Model model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" # Create fine-tune job fine_tuning_job = client . fine_tuning . jobs . create ( training_file = file_id , model = model , # Optional hyperparameters # hyperparameters={ # \"batch_size\": 3, # \"n_epochs\": 2, # \"learning_rate_multiplier\": 0.08 # } ) # Retrieve job status job_status = client . fine_tuning . jobs . retrieve ( fine_tuning_job . id ) print ( f \"Job status: { job_status . status } \" ) # Get the fine-tuned model name finished_job = client . fine_tuning . jobs . retrieve ( fine_tuning_job . id ) fine_tuned_model = finished_job . fine_tuned_model # Use the fine-tuned model for inference response = client . chat . completions . create ( model = fine_tuned_model , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a JSON Generation Specialist. Convert user requests into properly formatted JSON.\" }, { \"role\" : \"user\" , \"content\" : \"Create a configuration for a web application with name 'TaskMaster', version 1.2.0, and environment set to development.\" } ] ) Use your fine-tuned model in the playground (optional) ÔºÉ After your fine-tuned model is created, you can also test it in the kluster.ai playground: Go to the kluster.ai playground Select your fine-tuned model from the model dropdown menu Start chatting with your model to evaluate its performance on your specific task Benefits of fine-tuning ÔºÉ Fine-tuning offers several advantages over using general-purpose models: Improved performance : Fine-tuned models often outperform base models on specific tasks. Cost efficiency : Smaller fine-tuned models can outperform larger models at a lower cost. Reduced latency : Fine-tuned models can deliver faster responses for your applications. Consistency : More reliable outputs tailored to your specific task or domain. Next steps ÔºÉ Detailed tutorial : Follow the Fine-tuning sentiment analysis tutorial . API reference : Review the API reference documentation for all fine-tuning related endpoints. Explore models : See the Models page to check which foundation models support fine-tuning. Platform approach : Try the user-friendly platform interface for fine-tuning without writing code.",
        "summary": "Fine-tuning with the kluster.ai API ÔºÉ The kluster.ai API lets you automate and integrate fine-tuning into your development workflows. You can create, manage, and monitor fine-tuning jobs directly from your code, making it easy to customize models for your specific needs. This guide provides a practical overview of the fine-tuning process using the API. It covers the required data format, how to up..."
      }
    },
    "‚öôÔ∏è API Reference": {
      "Reference": {
        "url": "https://docs.kluster.ai/api-reference/reference/#list-supported-models",
        "title": "API Reference | kluster.ai Docs",
        "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
        "summary": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 M..."
      },
      "API Reference": {
        "url": "https://docs.kluster.ai/api-reference/",
        "title": "API Reference | kluster.ai Docs",
        "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
        "summary": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 M..."
      }
    },
    "üõ°Ô∏è Verify & Reliability": {
      "Reliability Checks": {
        "url": "https://docs.kluster.ai/get-started/verify/reliability/overview/",
        "title": "Reliability check by Verify | kluster.ai Docs",
        "content": "Reliability check by Verify ÔºÉ Reliability check is one of the features offered by Verify, and it is able to identify when AI responses contain fabricated or inaccurate information. With this specialized service, you can gauge the reliability of AI-generated content and build more trustworthy applications. The service can evaluate the AI response based on a given context, which makes it great for RAG applications. Without providing a specific context, the service can also be used as a real-time reliability verification service. How reliability check works ÔºÉ The service evaluates the truthfulness of an answer to a question by: Analyzing the original question, prompt or entire conversation history. Examining the provided answer (with context if provided). Determining if the answer contains unreliable or unsupported information. Providing a detailed explanation of the reasoning behind the determination as well as the search results used for verification. The service evaluates AI outputs in order to identify reliability issues or incorrect information, with the following fields: is_hallucination=true/false : Indicates whether the response contains unreliable content. explanation : Provides detailed reasoning for the determination. search_results : Shows the reference data used for verification (when applicable). For example, for the following prompt: ... { \"role\": \"user\", \"content\": \"Where is the Eiffel Tower?\" }, { \"role\": \"assistant\", \"content\": \"The Eiffel Tower is located in Rome.\" } ... The reliability check response would return: { \"is_hallucination\" : true , \"usage\" : { \"completion_tokens\" : 154 , \"prompt_tokens\" : 1100 , \"total_tokens\" : 1254 }, \"explanation\" : \"The response provides a wrong location for the Eiffel Tower.\\n\" \"The Eiffel Tower is actually located in Paris, France, not in Rome.\\n\" \"The response contains misinformation as it incorrectly states the tower's location.\" , \"search_results\" : [] } When to use reliability checking ÔºÉ The reliability check service is ideal for scenarios where you need: Model evaluation : Easily integrate the service to compare models output quality. RAG applications : Verify that generated responses accurately reflect the provided reference documents rather than introducing fabricated information. Internet-sourced verification : Validate claims against reliable online sources with transparent citation of evidence. Content moderation : Automatically flag potentially misleading information before it reaches end users. Regulatory compliance : Ensure AI-generated content meets accuracy requirements. How to integrate reliability checks ÔºÉ Verify offers multiple ways to perform reliability checks, each designed for different use cases: Guide Reliability dedicated endpoint Verify the reliability and accuracy of an answer to a specific question via a dedicated API endpoint. Visit the guide Guide Chat completion endpoint Validate responses in full conversation via the chat completions API using OpenAI libraries. Visit the guide Integration Workflow Integrations Download ready-to-use workflows for Dify, n8n, and other platforms using direct API integration. Get workflows Additional resources ÔºÉ Workflow Integrations : Download ready-to-use workflows for Dify, n8n . Tutorial : Explore the Verify tutorial with code examples.",
        "summary": "Reliability check by Verify ÔºÉ Reliability check is one of the features offered by Verify, and it is able to identify when AI responses contain fabricated or inaccurate information. With this specialized service, you can gauge the reliability of AI-generated content and build more trustworthy applications. The service can evaluate the AI response based on a given context, which makes it great for R..."
      },
      "Dedicated API": {
        "url": "https://docs.kluster.ai/get-started/verify/reliability/dedicated-api/",
        "title": "Dedicated reliability endpoint | kluster.ai Docs",
        "content": "Reliability check via the reliability endpoint ÔºÉ The verify/reliability endpoint allows you to validate whether an answer to a specific question contains unreliable information. This approach is ideal for verifying individual responses against the provided context (when the context parameter is included) or general knowledge (when no context is provided). This guide provides a quick example of how use the verify/reliability endpoint for reliability check. Prerequisites ÔºÉ Before getting started with reliability verification, ensure the following requirements are met: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. Endpoint parameters ÔºÉ The verify/reliability endpoint accepts the following input parameters: prompt ( string | required): The question asked or instruction given. output ( string |required): The LLM answer to verify for reliability. context ( string |optional): Reference material to validate against. return_search_results ( boolean |optional): Whether to include search results (default: false). The API returns a JSON object with the following structure: { \"is_hallucination\" : boolea n , \"usage\" : { \"completion_tokens\" : nu mber , \"prompt_tokens\" : nu mber , \"total_tokens\" : nu mber }, \"explanation\" : \"string\" , \"search_results\" : [] // Only included if return_search_results is true } How to use the reliability endpoint ÔºÉ The reliability check feature operates in two distinct modes depending on whether you provide context with your request: General knowledge verification : When no context is provided, the service verifies answers against general knowledge and external sources. Context validation mode : When context is provided, the service only validates answers against the specified context. General knowledge verification ÔºÉ This example checks whether an answer contains unreliable information. As no context is provided, the answer will be verified against general knowledge to identify reliability issues. Python CLI from os import environ import requests from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a reliability check request to kluster.ai... \\n \" ) # Set up request data url = \"https://api.kluster.ai/v1/verify/reliability\" headers = { \"Authorization\" : f \"Bearer { api_key } \" , \"Content-Type\" : \"application/json\" } payload = { \"prompt\" : \"Is earth flat?\" , \"output\" : \"Yes, my friend\" , \"return_search_results\" : False #Optional } # Send the request to the reliability verification endpoint response = requests . post ( url , headers = headers , json = payload ) # Convert the response to JSON result = response . json () # Extract key information is_hallucination = result . get ( \"is_hallucination\" ) explanation = result . get ( \"explanation\" ) # Print whether reliability issue was detected print ( f \" { 'üö®RELIABILITY ISSUE DETECTED' if is_hallucination else '‚úÖNO RELIABILITY ISSUE DETECTED' } \" ) # Print the explanation print ( f \" \\n üß†Explanation: { explanation } \" ) # Print full response print ( f \" \\n üîóAPI Response: { result } \" ) #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a reliability check request to kluster.ai...\\n\" # Submit reliability verification request response = $( curl --location 'https://api.kluster.ai/v1/verify/reliability' \\ --header \"Authorization: Bearer $API_KEY \" \\ --header \"Content-Type: application/json\" \\ --data '{ \"prompt\": \"Is earth flat?\", \"output\": \"Yes, 100%.\", \"return_search_results\": false }' ) # Extract key information is_hallucination = $( echo \" $response \" | jq -r '.is_hallucination' ) explanation = $( echo \" $response \" | jq -r '.explanation' ) # Print whether reliability issue was detected if [[ \" $is_hallucination \" == \"true\" ]] ; then echo -e \"\\nüö® RELIABILITY ISSUE DETECTED\" else echo -e \"\\n‚úÖ NO RELIABILITY ISSUE DETECTED\" fi # Print the explanation echo -e \"\\nüß† Explanation: $explanation \" # Print full response echo -e \"\\nüîó API Response: $response \" Context validation mode ÔºÉ When providing the context parameter, the service will not perform external verification. Instead, it focuses on whether the answer complies with the provided context. RAG applications Ensure the LLM's responses are accurate by using Verify in your Retrieval Augmented Generation (RAG) workflows. This example checks whether an answer is correct based on the provided context. Python CLI from os import environ import requests from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a reliability check request with context to kluster.ai... \\n \" ) # Set up request data url = \"https://api.kluster.ai/v1/verify/reliability\" headers = { \"Authorization\" : f \"Bearer { api_key } \" , \"Content-Type\" : \"application/json\" } payload = { \"prompt\" : \"What's the invoice date?\" , \"output\" : \"The Invoice date is: May 22, 2025 \" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"return_search_results\" : False } # Send the request to the reliability verification endpoint response = requests . post ( url , headers = headers , json = payload ) # Convert the response to JSON result = response . json () # Extract key information is_hallucination = result . get ( \"is_hallucination\" ) explanation = result . get ( \"explanation\" ) # Print whether reliability issue was detected print ( f \" { 'üö®RELIABILITY ISSUE DETECTED' if is_hallucination else '‚úÖNO RELIABILITY ISSUE DETECTED' } \" ) # Print the explanation print ( f \" \\n üß†Explanation: { explanation } \" ) # Print full response print ( f \" \\n üîóAPI Response: { result } \" ) #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a reliability check request with context to kluster.ai...\\n\" # Submit reliability verification request response = $( curl --location 'https://api.kluster.ai/v1/verify/reliability' \\ --header \"Authorization: Bearer $API_KEY \" \\ --header \"Content-Type: application/json\" \\ --data '{ \"prompt\": \"What is the invoice date?\", \"output\": \"The Invoice date is: May 22, 2025 \", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun2 Terms:N30 Ref:PO451C\", \"return_search_results\": true }' ) # Extract key information is_hallucination = $( echo \" $response \" | jq -r '.is_hallucination' ) explanation = $( echo \" $response \" | jq -r '.explanation' ) # Print whether reliability issue was detected if [[ \" $is_hallucination \" == \"true\" ]] ; then echo -e \"\\nüö® RELIABILITY ISSUE DETECTED\" else echo -e \"\\n‚úÖ NO RELIABILITY ISSUE DETECTED\" fi # Print the explanation echo -e \"\\nüß† Explanation: $explanation \" # Print full response echo -e \"\\nüîó API Response: $response \" Best practices ÔºÉ Include relevant context : When validating against specific information, provide comprehensive context. Use domain-specific context : Include authoritative references for specialized knowledge domains. Consider general verification : For widely known information, the service can verify against general knowledge sources. Review explanations : The detailed explanations provide valuable insights into the reasoning process. Next steps ÔºÉ Learn how to use Chat completion reliability verification for evaluating entire conversation histories Review the complete API documentation for detailed endpoint specifications",
        "summary": "Reliability check via the reliability endpoint ÔºÉ The verify/reliability endpoint allows you to validate whether an answer to a specific question contains unreliable information. This approach is ideal for verifying individual responses against the provided context (when the context parameter is included) or general knowledge (when no context is provided). This guide provides a quick example of how..."
      },
      "Workflow Integrations": {
        "url": "https://docs.kluster.ai/get-started/verify/reliability/workflow-integrations/",
        "title": "Workflow Integrations | kluster.ai Docs",
        "content": "Workflow integrations ÔºÉ You can integrate the Verify reliability check feature into your favorite automation platforms with ready-to-use workflow templates. These pre-configured workflows connect directly to the kluster.ai API, allowing you to add AI verification capabilities to your existing processes in minutes. Prerequisites ÔºÉ Before getting started with the workflow integrations, ensure the following requirements are met: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. Workflow platform : Set up Dify , n8n , or your preferred automation tool Available Workflows ÔºÉ Dify ÔºÉ By using Dify , you can build AI applications with built-in reliability verification. This workflow seamlessly integrates Verify into your Dify chatbots and agents, ensuring every response is validated for accuracy and trustworthiness before reaching your users. Configure kluster.ai as a Model Provider Navigate to Settings and select Model Provider Click on Add Provider and choose OpenAI-API-compatible Enter these settings: Base URL : https://api.kluster.ai/v1 API Key : Your kluster.ai API key Model : Select from available models Save and test the connection to ensure it works properly. Set up the kluster verify node: Select the HTTP Request node kluster verify Add your API key to the Authorization header Import and Configure the Workflow Download the workflow template below and import it into your Dify workspace. The workflow comes pre-configured to verify AI responses in real-time. Download Dify Workflow n8n ÔºÉ Add verification checkpoints to your n8n automation pipelines. This workflow validates AI-generated content against your source documents, tools, or real-time data, perfect for ensuring accuracy in automated content generation and data processing workflows. Set Up API Credentials Select the OpenAI and choose Credentials . Then click Create New Base URL : https://api.kluster.ai/v1 API Key : Your kluster.ai API key Model : Select from available models Set up the kluster verify node API key: Open the kluster verify node and modify the headers as follow: Header Name : Authorization Header Value : Bearer YOUR_API_KEY Import and Configure the Workflow Download the workflow template below and import it via the n8n interface. The workflow includes pre-configured HTTP nodes that connect to the /v1/verify/reliability endpoint, handle request/response formatting, and parse verification results. Connect your data sources and configure output routing as needed. Download n8n Workflow Next Steps ÔºÉ Ready to build more reliable AI applications? Explore the API : Check the complete API reference for advanced configuration options. Learn verification methods : Dive into the dedicated reliability endpoint for detailed implementation patterns. Try the tutorial : Follow the hands-on reliability check tutorial with code examples.",
        "summary": "Workflow integrations ÔºÉ You can integrate the Verify reliability check feature into your favorite automation platforms with ready-to-use workflow templates. These pre-configured workflows connect directly to the kluster.ai API, allowing you to add AI verification capabilities to your existing processes in minutes. Prerequisites ÔºÉ Before getting started with the workflow integrations, ensure the fo..."
      },
      "Verify": {
        "url": "https://docs.kluster.ai/get-started/verify/overview/",
        "title": "Overview of Verify | kluster.ai Docs",
        "content": "Verify ÔºÉ LLMs can generate non-factual or irrelevant information (hallucinations). For developers, this presents significant challenges: Difficulty in programmatically trusting LLM outputs. Increased complexity in error handling and quality assurance. Potential for cascading failures in chained AI operations. Requirement for manual review cycles, slowing down development and deployment. Traditional validation methods may involve complex rule sets, fine-tuning, or exhibit high false-positive rates, adding to the development burden. Verify is an intelligent verification service that validates LLM outputs in real-time. It's designed to give you the trust needed to deploy AI at scale in production environments where accuracy matters most. This page provides an overview of the Verify service. How Verify works ÔºÉ The Verify service functions as an intelligent agent. It assesses LLM output reliability based on three key inputs provided in the API call: prompt : The original input or question provided to the LLM. This gives context to the user's intent. output : The response generated by the LLM that requires validation. context (Optional) : Any source material or documents provided to the LLM (e.g., in RAG scenarios) against which the output's claims should be verified. Verify analyzes these inputs and can leverage real time internet access to validating claims against up-to-date public information, extending its capabilities beyond static knowledge bases. Performance benchmarks ÔºÉ Verify has been benchmarked against other solutions on HaluEval and HaluBench datasets (over 25,000 samples). Non-RAG Scenarios (Context-Free): Compared against CleanLab TLM (GPT 4o-mini, medium quality, optimized threshold). Results: Verify showed 11% higher overall accuracy, a 2.8% higher median F1 score (72.3% vs. 69.5%), and higher precision (fewer false positives). Response times are comparable (sub-10 seconds). RAG Validation (Context-Provided): Compared against Patronus AI's Lynx (70B) and CleanLab TLM. Results: On RAGTruth (factual consistency), Verify significantly outperformed Lynx 70B and CleanLab TLM. On DROP (numerical/logical reasoning), Verify showed competitive performance against Lynx and outperformed CleanLab TLM. Note: Lynx was trained on the training sets of DROP and RAGTruth, highlighting Verify's generalization capabilities to unseen data configurations. These results indicate Verify's effectiveness in diverse scenarios relevant to production AI systems. Target applications & use cases ÔºÉ Developers can integrate Verify into applications where LLM output accuracy is paramount: Automated content generation pipelines. Customer-facing chatbots and virtual assistants. Question-answering systems over private or public data (RAG). AI-driven data extraction and summarization tools. Internal workflow automation involving LLM-generated text.",
        "summary": "Verify ÔºÉ LLMs can generate non-factual or irrelevant information (hallucinations). For developers, this presents significant challenges: Difficulty in programmatically trusting LLM outputs. Increased complexity in error handling and quality assurance. Potential for cascading failures in chained AI operations. Requirement for manual review cycles, slowing down development and deployment. Traditiona..."
      },
      "Chat Completion": {
        "url": "https://docs.kluster.ai/get-started/verify/reliability/chat-completion/",
        "title": "Chat completion reliability endpoint | kluster.ai Docs",
        "content": "Reliability check via chat completion ÔºÉ Developers can access the reliability check feature via the regular chat completion endpoint. This allows you to validate responses in full conversation histories using the same format as the standard chat completions API. This approach enables verification of reliability within the complete context of a conversation. This guide provides a quick example of how the chat completion endpoint can be used for reliability checks. Prerequisites ÔºÉ Before getting started with reliability verification, ensure the following requirements are met: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. A virtual Python environment : (Optional) Recommended for developers using Python. It helps isolate Python installations in a virtual environment to reduce the risk of environment or package conflicts between your projects Required Python libraries : Install the following Python libraries: OpenAI Python API library : to access the openai module getpass : To handle API keys safely Integration options ÔºÉ You can access the reliability verification service in two flexible OpenAI compatible ways, depending on your preferred development workflow. For both, you'll need to set the model to klusterai/verify-reliability : OpenAI compatible endpoint : Use the OpenAI API /v1/chat/completions pointing to kluster.ai. OpenAI SDK : Configure kluster.ai with OpenAI libraries . Next, the chat.completions.create endpoint. Reliability checks via chat completions ÔºÉ This example shows how to use the service with the chat completion endpoint via the OpenAI /v1/chat/completions endpoint and OpenAI libraries, using the specialized klusterai/verify-reliability model to enable Verify reliability check. Python CLI from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a reliability check request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"klusterai/verify-reliability\" , # Note special model messages = [ { \"role\" : \"system\" , \"content\" : \"You are a knowledgeable assistant that provides accurate medical information.\" }, { \"role\" : \"user\" , \"content\" : \"Does vitamin C cure the common cold?\" }, { \"role\" : \"assistant\" , \"content\" : \"Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours.\" } ] ) # Extract the reliability verification response text_response = completion . choices [ 0 ] . message . content # Print response to console print ( text_response ) #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"deepseek-ai/DeepSeek-R1\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a knowledgeable assistant that provides accurate medical information.\" }, { \"role\": \"user\", \"content\": \"Does vitamin C cure the common cold?\" }, { \"role\": \"assistant\", \"content\": \"Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours.\" } ] }' Next steps ÔºÉ Learn how to use the reliability dedicated endpoint for simpler verification scenarios Review the complete API documentation for detailed endpoint specifications",
        "summary": "Reliability check via chat completion ÔºÉ Developers can access the reliability check feature via the regular chat completion endpoint. This allows you to validate responses in full conversation histories using the same format as the standard chat completions API. This approach enables verification of reliability within the complete context of a conversation. This guide provides a quick example of h..."
      }
    },
    "üöÄ Getting Started": {
      "Models": {
        "url": "https://docs.kluster.ai/get-started/models/#api-request-limits",
        "title": "Supported AI Models | kluster.ai Docs",
        "content": "Models on kluster.ai ÔºÉ kluster.ai offers a wide variety of open-source models for both real-time and batch inferences, with more being constantly added. This page covers all the models the API supports, with the API request limits for each. Model names ÔºÉ Each model supported by kluster.ai has a unique name that must be used when defining the model in the request. Model Model API name DeepSeek-R1 deepseek-ai/DeepSeek-R1 DeepSeek-R1-0528 deepseek-ai/DeepSeek-R1-0528 DeepSeek-V3-0324 deepseek-ai/DeepSeek-V3-0324 Gemma 3 27B google/gemma-3-27b-it Meta Llama 3.1 8B klusterai/Meta-Llama-3.1-8B-Instruct-Turbo Meta Llama 3.3 70B klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Meta Llama 4 Maverick meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 Meta Llama 4 Scout meta-llama/Llama-4-Scout-17B-16E-Instruct Mistral NeMo mistralai/Mistral-Nemo-Instruct-2407 Qwen2.5-VL 7B Qwen/Qwen2.5-VL-7B-Instruct Qwen3-235B-A22B Qwen/Qwen3-235B-A22B-FP8 Model comparison table ÔºÉ Model Description Real-time inference support Batch inference support Fine-tuning support Image analysis DeepSeek-R1 Mathematical problem-solving code generation complex data analysis. DeepSeek-R1-0528 Mathematical problem-solving code generation complex data analysis. DeepSeek-V3-0324 Natural language generation open-ended text creation contextually rich writing. Gemma 3 27B Multilingual applications extended-context tasks image analysis and complex reasoning. Llama 3.1 8B Low-latency or simple tasks cost-efficient inference. Llama 3.3 70B General-purpose AI balanced cost-performance. Llama 4 Maverick A state-of-the-art multimodal model with integrated vision and language understanding, optimized for complex reasoning, coding, and perception tasks Llama 4 Scout General-purpose multimodal AI extended context tasks and balanced cost-performance across text and vision. Mistral NeMo Natural language generation open-ended text creation contextually rich writing. Qwen2.5-VL 7B Visual question answering document analysis image-based reasoning multimodal chat. Qwen3-235B-A22B Qwen3's flagship 235 billion parameter model optimized with 8-bit quantization API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited",
        "summary": "Models on kluster.ai ÔºÉ kluster.ai offers a wide variety of open-source models for both real-time and batch inferences, with more being constantly added. This page covers all the models the API supports, with the API request limits for each. Model names ÔºÉ Each model supported by kluster.ai has a unique name that must be used when defining the model in the request. Model Model API name DeepSeek-R1 d..."
      },
      "Real Time": {
        "url": "https://docs.kluster.ai/get-started/start-building/real-time/",
        "title": "Perform real-time inference jobs | kluster.ai Docs",
        "content": "Perform real-time inference jobs ÔºÉ Overview ÔºÉ This guide provides guidance about how to use real-time inference with the kluster.ai API. This type of inference is best suited for use cases requiring instant, synchronous responses for user-facing features like chat interactions, live recommendations, or real-time decision-making. You will learn how to submit a request and retrieve responses, and where to find integration guides for using kluster.ai's API with some of your favorite third-party LLM interfaces. Please make sure you check the API request limits . Prerequisites ÔºÉ This guide assumes familiarity with Large Language Model (LLM) development and OpenAI libraries. Before getting started, make sure you have: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. A virtual Python environment - (optional) recommended for developers using Python. It helps isolate Python installations in a virtual environment to reduce the risk of environment or package conflicts between your projects Required Python libraries - install the following Python libraries: OpenAI Python API library - to access the openai module getpass - to handle API keys safely If you plan to use cURL via the CLI, you can export kluster.ai API key as a variable: export API_KEY = INSERT_API_KEY Supported models ÔºÉ Please visit the Models page to learn more about all the models supported by the kluster.ai batch API. In addition, you can see the complete list of available models programmatically using the list supported models endpoint. Quickstart snippets ÔºÉ The following code snippets provide a complete end-to-end real-time inference example for different models supported by kluster.ai. You can copy and paste the snippet into your local environment. Python ÔºÉ To use these snippets, run the Python script and enter your kluster.ai API key when prompted. DeepSeek-R1 # Real-time completions with the DeepSeek-R1 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"deepseek-ai/DeepSeek-R1\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) DeepSeek-R1-0528 # Real-time completions with the DeepSeek-R1-0528 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"deepseek-ai/DeepSeek-R1-0528\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) DeepSeek-V3-0324 # Real-time completions with the DeepSeek-V3-0324 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"deepseek-ai/DeepSeek-V3-0324\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Gemma 3 27B # Real-time completions with the Gemma 3 27B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image_url }}, ], } ], ) print ( f \" \\n Image URL: { image_url } \" ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Meta Llama 3.1 8B # Real-time completions with the Meta Llama 3.1 8B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Meta Llama 3.3 70B # Real-time completions with the Meta Llama 3.3 70B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Meta Llama 4 Maverick # Real-time completions with the Meta Llama 4 Maverick model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image_url }}, ], } ], ) print ( f \" \\n Image URL: { image_url } \" ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Meta Llama 4 Scout # Real-time completions with the Meta Llama 4 Scout model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image_url }}, ], } ], ) print ( f \" \\n Image URL: { image_url } \" ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Mistral NeMo # Real-time completions with the Mistral NeMo model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"mistralai/Mistral-Nemo-Instruct-2407\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Qwen2.5-VL 7B # Real-time completions with the Qwen2.5-VL 7B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"Qwen/Qwen2.5-VL-7B-Instruct\" , messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image_url }}, ], } ], ) print ( f \" \\n Image URL: { image_url } \" ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Qwen3-235B-A22B # Real-time completions with the Qwen3-235B-A22B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"Qwen/Qwen3-235B-A22B-FP8\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) CLI ÔºÉ Similarly, the following curl commands showcase how to easily send a chat completion request to kluster.ai for the different supported models. This example assumes you've exported your kluster.ai API key as the variable API_KEY . DeepSeek-R1 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"deepseek-ai/DeepSeek-R1\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" DeepSeek-R1-0528 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"deepseek-ai/DeepSeek-R1-0528\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" DeepSeek-V3-0324 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"deepseek-ai/DeepSeek-V3-0324\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" Gemma 3 27B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"google/gemma-3-27b-it\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": [ {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Who can park in the area?\\\"}, {\\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": {\\\"url\\\": \\\" $image_url \\\"}} ] } ] }\" Meta Llama 3.1 8B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" Meta Llama 3.3 70B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" Meta Llama 4 Maverick #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": [ {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Who can park in the area?\\\"}, {\\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": {\\\"url\\\": \\\" $image_url \\\"}} ] } ] }\" Meta Llama 4 Scout #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"meta-llama/Llama-4-Scout-17B-16E-Instruct\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": [ {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Who can park in the area?\\\"}, {\\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": {\\\"url\\\": \\\" $image_url \\\"}} ] } ] }\" Mistral NeMo #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"mistralai/Mistral-Nemo-Instruct-2407\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" Qwen2.5-VL 7B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"Qwen/Qwen2.5-VL-7B-Instruct\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": [ {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Who can park in the area?\\\"}, {\\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": {\\\"url\\\": \\\" $image_url \\\"}} ] } ] }\" Qwen3-235B-A22B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"Qwen/Qwen3-235B-A22B-FP8\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" Real-time inference flow ÔºÉ This section details the real-time inference process using the kluster.ai API and DeepSeek R1 model, but you can adapt it to any of the supported models . Submitting a request ÔºÉ The kluster.ai platform offers a simple, OpenAI-compatible interface, making it easy to integrate kluster.ai services seamlessly into your existing system. The following code shows how to do a chat completions request using the OpenAI library. Python import os from getpass import getpass from openai import OpenAI # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"deepseek-ai/DeepSeek-V3-0324\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) If successful, the completion variable contains a full response, which you'll need to analyze to extract the answer you are looking for. In terms of configuration for real-time inferences, there are several parameters that you need to tweak: model string required - name of one of the supported models messages array required - a list of chat messages ( system , user , or assistant roles, and also image_url for images). In this example, the query is \"What is the ultimate breakfast sandwich?\". Once these parameters are configured, run your script to send the request. Fetching the response ÔºÉ If the request is successful, the response is contained in the completion variable from the example above. It should follow the structure below and include relevant data such as the generated output, metadata, and token usage details. Response { \"id\" : \"a3af373493654dd195108b207e2faacf\" , \"choices\" : [ { \"finish_reason\" : \"stop\" , \"index\" : 0 , \"logprobs\" : null , \"message\" : { \"content\" : \"The \\\"ultimate\\\" breakfast sandwich is subjective and can vary based on personal preferences, but here‚Äôs a classic, crowd-pleasing version that combines savory, sweet, and hearty elements for a satisfying morning meal:\\n\\n### **The Ultimate Breakfast Sandwich**\\n**Ingredients:**\\n- **Bread:** A toasted brioche bun, English muffin, or sourdough slice (your choice for texture and flavor).\\n- **Protein:** Crispy bacon, sausage patty, or ham.\\n- **Egg:** Fried, scrambled, or a fluffy omelet-style egg.\\n- **Cheese:** Sharp cheddar, gooey American, or creamy Swiss.\\n- **Sauce:** Spicy mayo, hollandaise, or a drizzle of maple syrup for sweetness.\\n- **Extras:** Sliced avocado, caramelized onions, saut√©ed mushrooms, or fresh arugula for a gourmet touch.\\n- **Seasoning:** Salt, pepper, and a pinch of red pepper flakes for heat.\\n\\n**Assembly:**\\n1. Toast your bread or bun to golden perfection.\\n2. Cook your protein to your desired crispiness or doneness.\\n3. Prepare your egg‚Äîfried with a runny yolk is a classic choice.\\n4. Layer the cheese on the warm egg or protein so it melts slightly.\\n5. Add your extras (avocado, veggies, etc.) for freshness and flavor.\\n6. Spread your sauce on the bread or drizzle it over the filling.\\n7. Stack everything together, season with salt, pepper, or spices, and enjoy!\\n\\n**Optional Upgrades:**\\n- Add a hash brown patty for extra crunch.\\n- Swap regular bacon for thick-cut or maple-glazed bacon.\\n- Use a croissant instead of bread for a buttery, flaky twist.\\n\\nThe ultimate breakfast sandwich is all about balance‚Äîcrunchy, creamy, savory, and a hint of sweetness. Customize it to your taste and make it your own!\" , \"refusal\" : null , \"role\" : \"assistant\" , \"audio\" : null , \"function_call\" : null , \"tool_calls\" : null }, \"matched_stop\" : 1 } ], \"created\" : 1742378836 , \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"chat.completion\" , \"service_tier\" : null , \"system_fingerprint\" : null , \"usage\" : { \"completion_tokens\" : 398 , \"prompt_tokens\" : 10 , \"total_tokens\" : 408 , \"completion_tokens_details\" : null , \"prompt_tokens_details\" : null } } The following snippet demonstrates how to extract the data, log it to the console, and save it to a JSON file. Python def log_response_to_file ( response , filename = \"response_log.json\" ): \"\"\"Logs the full AI response to a JSON file in the same directory as the script.\"\"\" # Extract model name and AI-generated text model_name = response . model text_response = response . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) # Convert response to dictionary response_data = response . model_dump () # Get the script directory script_dir = os . path . dirname ( os . path . abspath ( __file__ )) file_path = os . path . join ( script_dir , filename ) # Write to JSON file with open ( file_path , \"w\" , encoding = \"utf-8\" ) as json_file : json . dump ( response_data , json_file , ensure_ascii = False , indent = 4 ) print ( f \"üíæ Response saved to { file_path } \" ) # Log response to file log_response_to_file ( completion ) For a detailed breakdown of the chat completion object, see the chat completion API reference section. View the complete script Python import json import os from getpass import getpass from openai import OpenAI # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"deepseek-ai/DeepSeek-V3-0324\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) def log_response_to_file ( response , filename = \"response_log.json\" ): \"\"\"Logs the full AI response to a JSON file in the same directory as the script.\"\"\" # Extract model name and AI-generated text model_name = response . model text_response = response . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) # Convert response to dictionary response_data = response . model_dump () # Get the script directory script_dir = os . path . dirname ( os . path . abspath ( __file__ )) file_path = os . path . join ( script_dir , filename ) # Write to JSON file with open ( file_path , \"w\" , encoding = \"utf-8\" ) as json_file : json . dump ( response_data , json_file , ensure_ascii = False , indent = 4 ) print ( f \"üíæ Response saved to { file_path } \" ) # Log response to file log_response_to_file ( completion ) Third-party integrations ÔºÉ You can also set up third-party LLM integrations using the kluster.ai API. For step-by-step instructions, check out the following integration guides: SillyTavern - multi-LLM chat interface LangChain - multi-turn conversational agent eliza - create and manage AI agents CrewAI - specialized agents for complex tasks LiteLLM - streaming response and multi-turn conversation handling Summary ÔºÉ You have now experienced the complete real-time inference job lifecycle using kluster.ai's chat completion API. In this guide, you've learned: How to submit a real-rime inference request How to configure real-time inference-related API parameters How to interpret the chat completion object API response The kluster.ai batch API is designed to efficiently and reliably handle your large-scale LLM workloads. If you have questions or suggestions, the support team would love to hear from you.",
        "summary": "Perform real-time inference jobs ÔºÉ Overview ÔºÉ This guide provides guidance about how to use real-time inference with the kluster.ai API. This type of inference is best suited for use cases requiring instant, synchronous responses for user-facing features like chat interactions, live recommendations, or real-time decision-making. You will learn how to submit a request and retrieve responses, and wh..."
      },
      "Integrations": {
        "url": "https://docs.kluster.ai/get-started/integrations/",
        "title": "Integrate CrewAI with kluster.ai API | kluster.ai Docs",
        "content": "Integrate CrewAI with kluster.ai ÔºÉ CrewAI is a multi-agent platform that organizes specialized AI agents‚Äîeach with defined roles, tools, and goals‚Äîwithin a structured process to tackle complex tasks efficiently. CrewAI agents streamline workflows and deliver reliable, scalable solutions by coordinating tasks and ensuring smooth collaboration. This guide walks you through integrating kluster.ai with CrewAI to create and run a simple AI agent chatbot that leverages the kluster.ai API. Prerequisites ÔºÉ Before starting, ensure you have the following prerequisites: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. CrewAI installed - the Installation Guide on the CrewAI website will walk you through installing CrewAI, setting up a virtual Python environment, and creating a new project. Note that CrewAI requires a Python version >= 3.10 and < 3.13 Create a project with the CLI ÔºÉ Open your Python virtual environment, and then follow these steps to use the CrewAI CLI to create a new project: Create a project - following the installation guide, create your first project with the following command: crewai create crew INSERT_PROJECT_NAME Select model and provider - during setup, the CLI will ask you to choose a provider and a model. Select openai as the provider and then choose any available model. Because you'll configure kluster.ai as a custom model, your initial model choice won't affect the final integration. The CLI will prompt you for an OpenAI API key, but this isn‚Äôt required. Simply press enter to skip Build a simple AI agent ÔºÉ After finishing the CLI setup, you will see a src directory with files crew.py and main.py . This guide won't use these sample files because they include extra features outside the scope. Follow these steps to continue: Create your first file - create a hello_crew.py file in src/YOUR_PROJECT_NAME to correspond to a simple AI agent chatbot Import modules and select model - open hello_crew.py to add imports and define a custom LLM for kluster.ai by setting the following parameters: provider - you can specify openai_compatible model - choose one of kluster.ai's available models based on your use case. Regardless of which model you choose, prepend its name with openai/ to ensure CrewAI, which relies on LiteLLM, processes your requests correctly base_url - use https://api.kluster.ai/v1 to send requests to the kluster.ai endpoint api_key - replace INSERT_API_KEY in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide hello_crew.py import random from crewai import LLM , Agent , Crew , Process , Task from crewai.project import CrewBase , agent , crew , task @CrewBase class HelloWorldCrew : # Override any default YAML references agents_config = {} tasks_config = {} def __init__ ( self ): \"\"\" When this crew is instantiated, create a custom LLM with your base_url. \"\"\" self . custom_llm = LLM ( provider = \"openai_compatible\" , model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_KLUSTER_API_KEY\" ) This example overrides agents_config and tasks_config with empty dictionaries to tell CrewAI to ignore all YAML files and rely solely on your code, keeping this guide as streamlined as possible. Define your agent - set the agent's role, goal, and backstory, and assign the custom LLM (via the kluster.ai API) for generating creative greetings: hello_crew.py @agent def hello_agent ( self ) -> Agent : \"\"\" A super simple agent with a single purpose: greet the user in a friendly, varied way. \"\"\" return Agent ( role = \"HelloWorldAgent\" , goal = \"Greet the user in a fun and creative way.\" , backstory = \"I'm a friendly agent who greets everyone in a slightly different manner!\" , llm = self . custom_llm , verbose = True ) Give the agent a task - define a task that prompts the agent for a unique, creative greeting using randomness to avoid repetition. Passing this prompt to hello_agent() ensures varied responses. CrewAI requires an expected_output field, defined here as a short greeting: hello_crew.py @task def hello_task ( self ) -> Task : \"\"\" A task that asks the agent to produce a dynamic greeting. \"\"\" random_factor = random . randint ( 100000 , 999999 ) prompt = f \"\"\" You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: { random_factor } Example: \"Hey there, how's your day going?\" \"\"\" return Task ( description = prompt , expected_output = \"A short, creative greeting\" , agent = self . hello_agent () ) Tie it all together with a @crew method - add the following method to return the assembled Crew object with a single agent and task. This method enables CrewAI to coordinate the agent and task you defined: hello_crew.py @crew def hello_crew ( self ) -> Crew : \"\"\" Our entire 'Hello World' crew‚Äîonly 1 agent + 1 task in sequence. \"\"\" return Crew ( agents = self . agents , tasks = self . tasks , process = Process . sequential , verbose = True ) Set up the entry point for the agent - create a new file named hello_main.py . In hello_main.py , import and initialize the HelloWorldCrew class, call its hello_crew() method, and then kickoff() to launch the task sequence: hello_main.py #!/usr/bin/env python from hello_crew import HelloWorldCrew def run (): \"\"\" Kick off the HelloWorld crew with no inputs. \"\"\" HelloWorldCrew () . hello_crew () . kickoff ( inputs = {}) if __name__ == \"__main__\" : run () View complete script hello_crew.py import random from crewai import LLM , Agent , Crew , Process , Task from crewai.project import CrewBase , agent , crew , task @CrewBase class HelloWorldCrew : # Override any default YAML references agents_config = {} tasks_config = {} def __init__ ( self ): \"\"\" When this crew is instantiated, create a custom LLM with your base_url. \"\"\" self . custom_llm = LLM ( provider = \"openai_compatible\" , model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_KLUSTER_API_KEY\" ) @agent def hello_agent ( self ) -> Agent : \"\"\" A super simple agent with a single purpose: greet the user in a friendly, varied way. \"\"\" return Agent ( role = \"HelloWorldAgent\" , goal = \"Greet the user in a fun and creative way.\" , backstory = \"I'm a friendly agent who greets everyone in a slightly different manner!\" , llm = self . custom_llm , verbose = True ) @task def hello_task ( self ) -> Task : \"\"\" A task that asks the agent to produce a dynamic greeting. \"\"\" random_factor = random . randint ( 100000 , 999999 ) prompt = f \"\"\" You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: { random_factor } Example: \"Hey there, how's your day going?\" \"\"\" return Task ( description = prompt , expected_output = \"A short, creative greeting\" , agent = self . hello_agent () ) @crew def hello_crew ( self ) -> Crew : \"\"\" Our entire 'Hello World' crew‚Äîonly 1 agent + 1 task in sequence. \"\"\" return Crew ( agents = self . agents , tasks = self . tasks , process = Process . sequential , verbose = True ) Put it all together ÔºÉ To run your agent, ensure you are in the same directory as your hello_main.py file, then use the following command: python hello_main.py Upon running the script, you'll see output that looks like the following: # Agent: HelloWorldAgent ## Task: You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: 896380 Example: \"Hey there, how's your day going?\" # Agent: HelloWorldAgent ## Final Answer: Hello, it's a beautiful day to shine, how's your sparkle today? And that's it! You've now successfully configured your AI agent harnessing CrewAI and the power of the kluster.ai API!",
        "summary": "Integrate CrewAI with kluster.ai ÔºÉ CrewAI is a multi-agent platform that organizes specialized AI agents‚Äîeach with defined roles, tools, and goals‚Äîwithin a structured process to tackle complex tasks efficiently. CrewAI agents streamline workflows and deliver reliable, scalable solutions by coordinating tasks and ensuring smooth collaboration. This guide walks you through integrating kluster.ai wit..."
      },
      "Batch": {
        "url": "https://docs.kluster.ai/get-started/start-building/batch/",
        "title": "Perform batch inference jobs | kluster.ai Docs",
        "content": "Perform batch inference jobs ÔºÉ Overview ÔºÉ This guide provides examples and instructions on how to create, submit, retrieve, and manage batch inference jobs using the kluster.ai API. You will find guidance about preparing your data, selecting a model, submitting your batch job, and retrieving your results. Please make sure you check the API request limits . Prerequisites ÔºÉ This guide assumes familiarity with Large Language Model (LLM) development and OpenAI libraries. Before getting started, make sure you have: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. A virtual Python environment - (optional) recommended for developers using Python. It helps isolate Python installations in a virtual environment to reduce the risk of environment or package conflicts between your projects Required Python libraries - install the following Python libraries: OpenAI Python API library - to access the openai module getpass - to handle API keys safely A basic understanding of JSON Lines (JSONL) - JSONL is the required text input format for performing batch inferences with the kluster.ai API If you plan to use cURL via the CLI, you can export your kluster.ai API key as a variable: export API_KEY = INSERT_API_KEY Supported models ÔºÉ Please visit the Models page to learn more about all the models supported by the kluster.ai batch API. In addition, you can see the complete list of available models programmatically using the list supported models endpoint. Batch job workflow overview ÔºÉ Working with batch jobs in the kluster.ai API involves the following steps: Create batch job file - prepare a JSON Lines file containing one or more chat completion requests to execute in the batch Upload batch job file - upload the file to kluster.ai to receive a unique file ID Start the batch job - initiate a new batch job using the file ID Monitor job progress - track the status of your batch job to ensure successful completion Retrieve results - once the job finishes, access and process the results as needed In addition to these core steps, this guide will give you hands-on experience to: Cancel a batch job - cancel an ongoing batch job before it completes List all batch jobs - review all of your batch jobs Warning For the free tier, the maximum number of batch requests (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. Quickstart snippets ÔºÉ The following code snippets provide a full end-to-end batch inference example for different models supported by kluster.ai. You can simply copy and paste the snippet into your local environment. Python ÔºÉ To use these snippets, run the Python script and enter your kluster.ai API key when prompted. DeepSeek-R1 # Batch completions with the DeepSeek-R1 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) DeepSeek-R1-0528 # Batch completions with the DeepSeek-R1-0528 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1-0528\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1-0528\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1-0528\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) DeepSeek-V3-0324 # Batch completions with the DeepSeek-V3-0324 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Gemma 3 27B # Batch completions with the Gemma 3 27B model on kluster.ai from os import environ import json import time from getpass import getpass from openai import OpenAI # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"google/gemma-3-27b-it\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is this?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image1_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"google/gemma-3-27b-it\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Extract the text, find typos if any.\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image2_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"google/gemma-3-27b-it\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image3_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again print ( f \" \\n Image1 URL: { image1_url } \" ) print ( f \" \\n Image2 URL: { image2_url } \" ) print ( f \" \\n Image3 URL: { image3_url } \" ) # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) print ( batch_status ) Meta Llama 3.1 8B # Batch completions with the Meta Llama 3.1 8B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Meta Llama 3.3 70B # Batch completions with the Meta Llama 3.3 70B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Meta Llama 4 Maverick # Batch completions with the Meta Llama 4 Maverick model on kluster.ai from os import environ import json import time from getpass import getpass from openai import OpenAI # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is this?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image1_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Extract the text, find typos if any.\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image2_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image3_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again print ( f \" \\n Image1 URL: { image1_url } \" ) print ( f \" \\n Image2 URL: { image2_url } \" ) print ( f \" \\n Image3 URL: { image3_url } \" ) # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) print ( batch_status ) Meta Llama 4 Scout # Batch completions with the Meta Llama 4 Scout model on kluster.ai from os import environ import json import time from getpass import getpass from openai import OpenAI # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is this?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image1_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Extract the text, find typos if any.\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image2_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image3_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again print ( f \" \\n Image1 URL: { image1_url } \" ) print ( f \" \\n Image2 URL: { image2_url } \" ) print ( f \" \\n Image3 URL: { image3_url } \" ) # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) print ( batch_status ) Mistral NeMo # Batch completions with the Mistral NeMo model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"mistralai/Mistral-Nemo-Instruct-2407\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"mistralai/Mistral-Nemo-Instruct-2407\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"mistralai/Mistral-Nemo-Instruct-2407\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Qwen2.5-VL 7B # Batch completions with the Qwen2.5-VL 7B model on kluster.ai from os import environ import json import time from getpass import getpass from openai import OpenAI # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is this?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image1_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Extract the text, find typos if any.\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image2_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image3_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again print ( f \" \\n Image1 URL: { image1_url } \" ) print ( f \" \\n Image2 URL: { image2_url } \" ) print ( f \" \\n Image3 URL: { image3_url } \" ) # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) print ( batch_status ) Qwen3-235B-A22B # Batch completions with the Qwen3-235B-A22B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen3-235B-A22B-FP8\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen3-235B-A22B-FP8\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen3-235B-A22B-FP8\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) CLI ÔºÉ Similarly, the following curl commands showcase how to easily send a chat completion request to kluster.ai for the different supported models. This example assumes you've exported your kluster.ai API key as the variable API_KEY . DeepSeek-R1 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" DeepSeek-R1-0528 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1-0528\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1-0528\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1-0528\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" DeepSeek-V3-0324 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Gemma 3 27B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai...\\n\" # Define image URLs # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"google/gemma-3-27b-it\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is this?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image1_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"google/gemma-3-27b-it\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Extract the text, find typos if any.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image2_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"google/gemma-3-27b-it\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who can park in the area?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image3_url\"}}]}],\"max_completion_tokens\": 1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nImage1 URL: $image1_url \" echo -e \"\\nImage2 URL: $image2_url \" echo -e \"\\nImage3 URL: $image3_url \" echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Meta Llama 3.1 8B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Meta Llama 3.3 70B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Meta Llama 4 Maverick #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai...\\n\" # Define image URLs # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is this?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image1_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Extract the text, find typos if any.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image2_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who can park in the area?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image3_url\"}}]}],\"max_completion_tokens\": 1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nImage1 URL: $image1_url \" echo -e \"\\nImage2 URL: $image2_url \" echo -e \"\\nImage3 URL: $image3_url \" echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Meta Llama 4 Scout #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai...\\n\" # Define image URLs # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is this?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image1_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Extract the text, find typos if any.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image2_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who can park in the area?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image3_url\"}}]}],\"max_completion_tokens\": 1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nImage1 URL: $image1_url \" echo -e \"\\nImage2 URL: $image2_url \" echo -e \"\\nImage3 URL: $image3_url \" echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Mistral NeMo #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"mistralai/Mistral-Nemo-Instruct-2407\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"mistralai/Mistral-Nemo-Instruct-2407\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"mistralai/Mistral-Nemo-Instruct-2407\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Qwen2.5-VL 7B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai...\\n\" # Define image URLs # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is this?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image1_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Extract the text, find typos if any.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image2_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who can park in the area?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image3_url\"}}]}],\"max_completion_tokens\": 1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nImage1 URL: $image1_url \" echo -e \"\\nImage2 URL: $image2_url \" echo -e \"\\nImage3 URL: $image3_url \" echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Qwen3-235B-A22B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen3-235B-A22B-FP8\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen3-235B-A22B-FP8\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen3-235B-A22B-FP8\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Batch inference flow ÔºÉ This section details the batch inference process using the kluster.ai API and DeepSeek R1 model, but you can adapt it to any of the supported models . Create batch jobs as JSON files ÔºÉ To begin the batch job workflow, you'll need to assemble your batch requests and add them to a JSON Lines file ( .jsonl ). Each request must include the following arguments: custom_id string - a unique request ID to match outputs to inputs method string - the HTTP method to use for the request. Currently, only POST is supported url string - the /v1/chat/completions endpoint body object - a request body containing: model string required - name of one of the supported models messages array required - a list of chat messages ( system , user , or assistant roles, and also image_url for images) Any optional chat completion parameters , such as temperature , max_completion_tokens , etc. Tip You can use a different model for each request you submit. The following examples generate requests and save them in a JSONL file, which is ready to be uploaded for processing. Python CLI import time import os from getpass import getpass from openai import OpenAI # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen3-235B-A22B-FP8\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen3-235B-A22B-FP8\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who can park in the area?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\"}}]}],\"max_completion_tokens\":1000}} EOF Warning For the free tier, the maximum number of batch requests (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. Upload batch job files ÔºÉ After you've created the JSON Lines file, you need to upload it using the files endpoint along with the intended purpose. Consequently, you need to set the purpose value to \"batch\" for batch jobs. The response will contain an id field; save this value as you'll need it in the next step, where it's referred to as input_file_id . You can view your uploaded files in the Files tab of the kluster.ai platform. Use the following command examples to upload your batch job files: Python curl # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Warning Remember that the maximum file size permitted is 100 MB. Submit a batch job ÔºÉ Next, submit a batch job by calling the batches endpoint and providing the id of the uploaded batch job file (from the previous section) as the input_file_id , and additional parameters to specify the job's configuration. The response includes an id that can be used to monitor the job's progress, as demonstrated in the next section. You can use the following snippets to submit your batch job: Python curl # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Monitor job progress ÔºÉ You can make periodic requests to the batches endpoint to monitor your batch job's progress. Use the id of the batch request from the preceding section as the batch_id to check its status. The job is complete when the status field returns \"completed\" . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. To see a complete list of the supported statuses, refer to the Retrieve a batch API reference page. You can use the following snippets to monitor your batch job: Python curl # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Retrieve results ÔºÉ To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id , which is returned from querying the batch's status (from the previous section). The output file will be a JSONL file, where each line contains the custom_id from your input file request and the corresponding response. You can use the following snippets to retrieve the results from your batch job: Python curl # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"üíæ Response saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_results.jsonl View the complete script Python import json import time import os from getpass import getpass from openai import OpenAI # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen3-235B-A22B-FP8\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"üíæ Response saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) List all batch jobs ÔºÉ To list all of your batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. You can use the following snippets to list all of your batch jobs: Python curl import os from openai import OpenAI from getpass import getpass # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) # Log all batch jobs (limit to 3) print ( client . batches . list ( limit = 3 ) . to_dict ()) curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Cancel a batch job ÔºÉ To cancel a batch job currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, and the status will show as canceling. Once complete, the status will show as cancelled . You can use the following snippets to cancel a batch job: Python curl Example import os from openai import OpenAI from getpass import getpass # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) # Cancel batch job with specified ID client . batches . cancel ( \"mybatch-123\" ) Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } Summary ÔºÉ You have now experienced the complete batch inference job lifecycle using kluster.ai's batch API. In this guide, you've learned how to: Prepare and submit batch jobs with structured request inputs Track your job's progress in real-time Retrieve and handle job results View and manage your batch jobs Cancel jobs when needed The kluster.ai batch API is designed to efficiently and reliably handle your large-scale LLM workloads. If you have questions or suggestions, the support team would love to hear from you.",
        "summary": "Perform batch inference jobs ÔºÉ Overview ÔºÉ This guide provides examples and instructions on how to create, submit, retrieve, and manage batch inference jobs using the kluster.ai API. You will find guidance about preparing your data, selecting a model, submitting your batch job, and retrieving your results. Please make sure you check the API request limits . Prerequisites ÔºÉ This guide assumes famili..."
      },
      "Get Api Key": {
        "url": "https://docs.kluster.ai/get-started/get-api-key/",
        "title": "Get a kluster.ai API key | kluster.ai Docs",
        "content": "Generate your kluster.ai API key ÔºÉ The API key is a unique identifier that authenticates requests associated with your account. You must have at least one API key to access kluster.ai 's services. This guide will help you obtain an API key, the first step to leveraging kluster.ai's powerful and cost-effective AI capabilities. Create an account ÔºÉ If you haven't already created an account with kluster.ai, visit the registration page and take the following steps: Enter your full name Provide a valid email address Create a secure password Click the Sign up button Generate a new API key ÔºÉ After you've signed up or logged into the platform through the login page , take the following steps: Select API Keys on the left-hand side menu In the API Keys section, click the Issue New API Key button Enter a descriptive name for your API key in the popup, then click Create Key Copy and secure your API key ÔºÉ Once generated, your API key will be displayed Copy the key and store it in a secure location, such as a password manager Warning For security reasons, you won't be able to view the key again. If lost, you will need to generate a new one. Security tips Keep it secret - do not share your API key publicly or commit it to version control systems Use environment variables - store your API key in environment variables instead of hardcoding them Regenerate if compromised - if you suspect your API key has been exposed, regenerate it immediately from the API Keys section Managing your API keys ÔºÉ The API Key Management section allows you to efficiently manage your kluster.ai API keys. You can create, view, and delete API keys by navigating to the API Keys section. Your API keys will be listed in the API Key Management section. To delete an API key, take the following steps: Locate the API key you wish to delete in the list Click the trash bin icon ( ) in the Actions column Confirm the deletion when prompted Warning Once deleted, the API key cannot be used again and you must generate a new one if needed. Next steps ÔºÉ Now that you have your API key, you can start integrating kluster.ai's LLMs into your applications. Refer to our Getting Started guide for detailed instructions on using the API.",
        "summary": "Generate your kluster.ai API key ÔºÉ The API key is a unique identifier that authenticates requests associated with your account. You must have at least one API key to access kluster.ai 's services. This guide will help you obtain an API key, the first step to leveraging kluster.ai's powerful and cost-effective AI capabilities. Create an account ÔºÉ If you haven't already created an account with klust..."
      },
      "Litellm": {
        "url": "https://docs.kluster.ai/get-started/integrations/litellm/",
        "title": "Integrate LiteLLM with kluster.ai | kluster.ai Docs",
        "content": "Integrate LiteLLM with kluster.ai ÔºÉ LiteLLM is an open-source Python library that streamlines access to a broad range of Large Language Model (LLM) providers through a standardized interface inspired by the OpenAI format. By providing features like fallback mechanisms, cost tracking, and streaming support, LiteLLM reduces the complexity of working with different models, ensuring a more reliable and cost-effective approach to AI-driven applications. Integrating LiteLLM with the kluster.ai API enables the use of kluster.ai's powerful models alongside LiteLLM's flexible orchestration. This combination makes it simple to switch between models on the fly, handle token usage limits with context window fallback, and monitor usage costs in real-time‚Äîleading to robust, scalable, and adaptable AI workflows. Prerequisites ÔºÉ Before starting, ensure you have the following: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. A python virtual environment - this is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial LiteLLM installed - to install the library, use the following command: pip install litellm Configure LiteLLM ÔºÉ In this section, you'll learn how to integrate kluster.ai with LiteLLM. You'll configure your environment variables, specify a kluster.ai model, and make a simple request using LiteLLM's OpenAI-like interface. Import LiteLLM and its dependencies - create a new file (e.g., hello-litellm.py ) and start by importing the necessary Python modules: import os from litellm import completion Set your kluster.ai API key and Base URL - replace INSERT_API_KEY with your actual API key. If you don't have one yet, refer to the Get an API key guide # Set environment vars, shown in script for readability os . environ [ \"OPENAI_API_KEY\" ] = \"INSERT_KLUSTER_API_KEY\" os . environ [ \"OPENAI_API_BASE\" ] = \"https://api.kluster.ai/v1\" Define your conversation (system + user messages) - set up your initial system prompt and user message. The system message defines your AI assistant's role, while the user message is the actual question or prompt # Basic Chat messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of California?\" } ] Select your kluster.ai model - choose one of kluster.ai's available models that best fits your use case. Prepend the model name with openai/ so LiteLLM recognizes it as an OpenAI-like model request # Use an \"openai/...\" model prefix so LiteLLM treats this as an OpenAI-like call model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" Call the LiteLLM completion function - finally, invoke the completion function to send your request: response = completion ( model = model , messages = messages , max_tokens = 1000 , ) print ( response ) View complete script hello-litellm.py import os from litellm import completion # Set environment vars, shown in script for readability os . environ [ \"OPENAI_API_KEY\" ] = \"INSERT_KLUSTER_API_KEY\" os . environ [ \"OPENAI_API_BASE\" ] = \"https://api.kluster.ai/v1\" # Basic Chat messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of California?\" } ] # Use an \"openai/...\" model prefix so LiteLLM treats this as an OpenAI-like call model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" response = completion ( model = model , messages = messages , max_tokens = 1000 , ) print ( response ) Use the following command to run your script: python hello - litellm . py python hello-litellm.py ModelResponse(id='chatcmpl-9877dfe6-6f1d-483f-a392-d791b89c75d6', created=1739495162, model='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The capital of California is Sacramento.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, refusal=None))], usage=Usage(completion_tokens=8, prompt_tokens=48, total_tokens=56, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, prompt_logprobs=None) That's it! You've successfully integrated LiteLLM with the kluster.ai API. Continue to learn how to experiment with more advanced features of LiteLLM. Explore LiteLLM features ÔºÉ In the previous section, you learned how to use LiteLLM with the kluster.ai API by properly configuring the model via an OpenAI-like call and configuring the API key and API base URL. The following sections demonstrate using LiteLLM's streaming response and multi-turn conversation features with the kluster.ai API. The following guide assumes you just finished the configuration exercise in the preceding section. If you haven't already done so, please complete the configuration steps in the Configure LiteLLM section before you continue. Use streaming responses ÔºÉ You can enable streaming by simply passing stream=True to the completion() function. Streaming returns a generator instead of a static response, letting you iterate over partial output chunks as they arrive. In the code sample below, each chunk is accessed in a for-in loop, allowing you to extract the textual content (e.g., chunk.choices[0].delta.content) rather than printing all metadata. To configure a streaming response, take the following steps: Update the messages system prompt and first user message - you can supply a user message or use the sample provided: messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful AI assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the significance of the California Gold Rush.\" }, ] Initiate a streaming request to the model - set stream=True in the completion() function to tell LiteLLM to return partial pieces (chunks) of the response as they become available rather than waiting for the entire response to be ready # --- 1) STREAMING CALL: Only print chunk text -------------------------------- try : response_stream = completion ( model = model , messages = messages , max_tokens = 300 , temperature = 0.3 , stream = True , # streaming enabled ) except Exception as err : print ( f \"Error calling model: { err } \" ) return print ( \" \\n --------- STREAMING RESPONSE (text only) ---------\" ) streamed_text = [] Isolate the returned text content - returning all of the streamed data will include a lot of excessive noise like token counts, etc. You can isolate the text content from the rest of the streamed response with the following code: # Iterate over each chunk from the streaming generator for chunk in response_stream : if hasattr ( chunk , \"choices\" ) and chunk . choices : # If the content is None, we replace it with \"\" (empty string) partial_text = getattr ( chunk . choices [ 0 ] . delta , \"content\" , \"\" ) or \"\" streamed_text . append ( partial_text ) print ( partial_text , end = \"\" , flush = True ) print ( \" \\n \" ) # new line after streaming ends Handle multi-turn conversation ÔºÉ LiteLLM can facilitate multi-turn conversations by maintaining message history in a sequential chain, enabling the model to consider the context of previous messages. This section demonstrates multi-turn conversation handling by updating the messages list each time we receive a new response from the assistant. This pattern can be repeated for as many turns as you need, continuously appending messages to maintain the conversational flow. Let's take a closer look at each step: Combine the streamed chunks of the first message - since the message is streamed in chunks, you must re-assemble them into a single message. After collecting partial responses in streamed_text , join them into a single string called complete_first_answer : # Combine the partial chunks into one string complete_first_answer = \"\" . join ( streamed_text ) Append the assistant's reply - to enhance the context of the conversation. Add complete_first_answer back into messages under the \"assistant\" role as follows: # Append the entire first answer to the conversation for multi-turn context messages . append ({ \"role\" : \"assistant\" , \"content\" : complete_first_answer }) Craft the second message to the assistant - append a new message object to messages with the user's next question as follows: # --- 2) SECOND CALL (non-streamed): Print just the text --------------------- messages . append ({ \"role\" : \"user\" , \"content\" : ( \"Thanks for that. Can you propose a short, 3-minute presentation outline \" \"about the Gold Rush, focusing on its broader implications?\" ), }) Ask the model to respond to the second question - this time, don't enable the streaming feature. Pass the updated messages to completion() with stream=False , prompting LiteLLM to generate a standard (single-shot) response as follows: try : response_2 = completion ( model = model , messages = messages , max_tokens = 300 , temperature = 0.6 , stream = False # non-streamed ) except Exception as err : print ( f \"Error calling model: { err } \" ) return Parse and print the second answer - extract response_2.choices[0].message[\"content\"] , store it in second_answer_text , and print to the console for your final output: print ( \"--------- RESPONSE 2 (non-streamed, text only) ---------\" ) second_answer_text = \"\" if response_2 . choices and hasattr ( response_2 . choices [ 0 ], \"message\" ): second_answer_text = response_2 . choices [ 0 ] . message . get ( \"content\" , \"\" ) or \"\" print ( second_answer_text ) You can view the full script below. It demonstrates a streamed response versus a regular response and how to handle a multi-turn conversation. View complete script hello-litellm.py import os import litellm.exceptions from litellm import completion # Set environment variables for kluster.ai os . environ [ \"OPENAI_API_KEY\" ] = \"INSERT_API_KEY\" # Replace with your key os . environ [ \"OPENAI_API_BASE\" ] = \"https://api.kluster.ai/v1\" def main (): model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful AI assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the significance of the California Gold Rush.\" }, ] # --- 1) STREAMING CALL: Only print chunk text -------------------------------- try : response_stream = completion ( model = model , messages = messages , max_tokens = 300 , temperature = 0.3 , stream = True , # streaming enabled ) except Exception as err : print ( f \"Error calling model: { err } \" ) return print ( \" \\n --------- STREAMING RESPONSE (text only) ---------\" ) streamed_text = [] # Iterate over each chunk from the streaming generator for chunk in response_stream : if hasattr ( chunk , \"choices\" ) and chunk . choices : # If the content is None, we replace it with \"\" (empty string) partial_text = getattr ( chunk . choices [ 0 ] . delta , \"content\" , \"\" ) or \"\" streamed_text . append ( partial_text ) print ( partial_text , end = \"\" , flush = True ) print ( \" \\n \" ) # new line after streaming ends # Combine the partial chunks into one string complete_first_answer = \"\" . join ( streamed_text ) # Append the entire first answer to the conversation for multi-turn context messages . append ({ \"role\" : \"assistant\" , \"content\" : complete_first_answer }) # --- 2) SECOND CALL (non-streamed): Print just the text --------------------- messages . append ({ \"role\" : \"user\" , \"content\" : ( \"Thanks for that. Can you propose a short, 3-minute presentation outline \" \"about the Gold Rush, focusing on its broader implications?\" ), }) try : response_2 = completion ( model = model , messages = messages , max_tokens = 300 , temperature = 0.6 , stream = False # non-streamed ) except Exception as err : print ( f \"Error calling model: { err } \" ) return print ( \"--------- RESPONSE 2 (non-streamed, text only) ---------\" ) second_answer_text = \"\" if response_2 . choices and hasattr ( response_2 . choices [ 0 ], \"message\" ): second_answer_text = response_2 . choices [ 0 ] . message . get ( \"content\" , \"\" ) or \"\" print ( second_answer_text ) if __name__ == \"__main__\" : main () Put it all together ÔºÉ Use the following command to run your script: python hello-litellm.py You should see output that resembles the following: python streaming-litellm.py --------- STREAMING RESPONSE (text only) --------- The California Gold Rush, which occurred from 1848 to 1855, was a pivotal event in American history that had significant economic, social, and cultural impacts on the United States and the world. Here are some of the key reasons why the California Gold Rush was important: 1. **Mass Migration and Population Growth**: The Gold Rush triggered a massive influx of people to California, with estimates suggesting that over 300,000 people arrived in the state between 1848 and 1852. This migration helped to populate the western United States and contributed to the country's westward expansion. 2. **Economic Boom**: The Gold Rush created a huge economic boom, with thousands of people striking it rich and investing their newfound wealth in businesses, infrastructure, and other ventures. The gold rush helped to stimulate economic growth, create new industries, and establish California as a major economic hub. 3. **Technological Innovations**: The Gold Rush drove technological innovations, particularly in the areas of mining and transportation. The development of new mining techniques, such as hydraulic mining, and the construction of roads, bridges, and canals, helped to facilitate the extraction and transportation of gold. 4. **Impact on Native American Populations**: The Gold Rush had a devastating impact on Native American populations in California, who were forcibly removed from their lands, killed, or displaced by the influx of miners. The Gold Rush marked the beginning of a long and tragic period of colonization and marginalization for Native American communities in --------- RESPONSE 2 (non-streamed, text only) --------- Here's a suggested 3-minute presentation outline on the California Gold Rush, focusing on its broader implications: **Title:** The California Gold Rush: A Catalyst for Change **Introduction (30 seconds)** * Briefly introduce the California Gold Rush and its significance * Thesis statement: The California Gold Rush was a pivotal event in American history that had far-reaching implications for the country's economy, society, and politics. **Section 1: Economic Implications (45 seconds)** * Discuss how the Gold Rush stimulated economic growth and helped establish California as a major economic hub * Mention the impact on trade, commerce, and industry, including the growth of San Francisco and other cities * Highlight the role of the Gold Rush in shaping the US economy and contributing to the country's westward expansion **Section 2: Social and Cultural Implications (45 seconds)** * Discuss the impact of the Gold Rush on Native American populations, including forced removals, violence, and displacement * Mention the diversity of people who came to California during the Gold Rush, including immigrants from China, Latin America, and Europe * Highlight the social and cultural changes that resulted from this diversity, including the growth of cities and the development of new communities **Section 3: Lasting Legacy (45 seconds)** * Discuss the lasting legacy of the Gold Rush, including its contribution to the development of the US West Coast and the growth of the US economy * Mention the ongoing impact of the Gold Both responses appear to trail off abruptly, but that's because we limited the output to 300 tokens each. Feel free to tweak the parameters and rerun the script at your leisure!",
        "summary": "Integrate LiteLLM with kluster.ai ÔºÉ LiteLLM is an open-source Python library that streamlines access to a broad range of Large Language Model (LLM) providers through a standardized interface inspired by the OpenAI format. By providing features like fallback mechanisms, cost tracking, and streaming support, LiteLLM reduces the complexity of working with different models, ensuring a more reliable an..."
      },
      "Setup": {
        "url": "https://docs.kluster.ai/get-started/start-building/setup/",
        "title": "Start building with the kluster.ai API | kluster.ai Docs",
        "content": "Start using the kluster.ai API ÔºÉ The kluster.ai API provides a straightforward way to work with Large Language Models (LLMs) at scale. It is compatible with OpenAI's API and SDKs , making it easy to integrate into your existing workflows with minimal code changes. Get your API key ÔºÉ Navigate to the kluster.ai developer console API Keys section and create a new key. You'll need this for all API requests. For step-by-step instructions, refer to the Get an API key guide. Set up the OpenAI client library ÔºÉ Developers can use the OpenAI libraries with kluster.ai with no changes. To start, you need to install the library: Python pip install \"openai>=1.0.0\" Once the library is installed, you can instantiate an OpenAI client pointing to kluster.ai with the following code and replacing INSERT_API_KEY : Python from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) Check the kluster.ai OpenAI compatibility page for detailed information about the integration. API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Where to go next ÔºÉ Guide Real-time inference Build AI-powered applications that deliver instant, real-time responses. Visit the guide Guide Batch inference Process large-scale data efficiently with AI-powered batch inference. Visit the guide Reference API reference Explore the complete kluster.ai API documentation and usage details. Reference",
        "summary": "Start using the kluster.ai API ÔºÉ The kluster.ai API provides a straightforward way to work with Large Language Models (LLMs) at scale. It is compatible with OpenAI's API and SDKs , making it easy to integrate into your existing workflows with minimal code changes. Get your API key ÔºÉ Navigate to the kluster.ai developer console API Keys section and create a new key. You'll need this for all API req..."
      },
      "Sillytavern": {
        "url": "https://docs.kluster.ai/get-started/integrations/sillytavern",
        "title": "Integrate SillyTavern with kluster.ai | kluster.ai Docs",
        "content": "Integrate SillyTavern with kluster.ai ÔºÉ SillyTavern is a locally installed customizable LLM user interface that focuses on persona-driven LLM interactions‚Äîletting you create unique characters or group chats for tasks like code reviews and text editing. It provides custom prompt fields, bookmarks for revisiting specific points in a conversation, and a mobile-friendly design to manage your chat sessions easily. By integrating SillyTavern with the kluster.ai API, you can tap into kluster.ai's high-performance language models as your primary or backup backend. This combination merges SillyTavern's customizable UI and advanced prompt options with kluster.ai's reliable inference, offering a scalable and tailored chat environment for casual users and AI enthusiasts. Prerequisites ÔºÉ Before starting, ensure you have the following: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. Configure SillyTavern ÔºÉ Launch SillyTavern and open it in your browser at http://127.0.0.1:8000/ (default port) Click on the API Connections icon (plug) in the top navigation menu In the API drop-down menu, select Chat Completion In the Chat Completion Source option, choose Custom (OpenAI-compatible) Enter the kluster.ai API endpoint in the Custom Endpoint (Base URL) field: https://api.kluster.ai/v1 There should be no trailing slash ( / ) at the end of the URL Paste your kluster.ai API Key into the designated field Enter a Model ID . For this example, you can enter: klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Click the Connect button. If you've configured the API correctly, you should see a üü¢ Valid message next to the button Select one of the kluster.ai-supported models from the Available Models drop-down menu That's it! You're now ready to start chatting with your bot powered by kluster.ai. Test the connection ÔºÉ Now that you've configured kluster.ai with SillyTavern, you can test the API connection by starting a new conversation. Follow these steps to get started: Click the menu icon on the bottom-left corner of the page Select Start New Chat to open a new chat with the model Type a message in the Type a message bar at the bottom and send it Verify that the chatbot has returned a response successfully Troubleshooting If you encounter errors, revisit the configuration instructions and double-check your API key and base URL and that you've received a Valid response after connecting the API (see step 8).",
        "summary": "Integrate SillyTavern with kluster.ai ÔºÉ SillyTavern is a locally installed customizable LLM user interface that focuses on persona-driven LLM interactions‚Äîletting you create unique characters or group chats for tasks like code reviews and text editing. It provides custom prompt fields, bookmarks for revisiting specific points in a conversation, and a mobile-friendly design to manage your chat sess..."
      },
      "OpenAI Compatibility": {
        "url": "https://docs.kluster.ai/get-started/openai-compatibility/#configuring-openai-to-use-klusterais-api",
        "title": "Compatibility with OpenAI client libraries | kluster.ai Docs",
        "content": "OpenAI compatibility ÔºÉ The kluster.ai API is compatible with OpenAI 's API and SDKs, allowing seamless integration into your existing applications. If you already have an application running with the OpenAI client library, you can easily switch to kluster.ai's API with minimal changes. This ensures a smooth transition without the need for significant refactoring or rework. Configuring OpenAI to use kluster.ai's API ÔºÉ Developers can use the OpenAI libraries with kluster.ai with no changes. To start, you need to install the library: Python pip install \"openai>=1.0.0\" To start using kluster.ai with OpenAI's client libraries, set your API key and change the base URL to https://api.kluster.ai/v1 : Python from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) Unsupported OpenAI features ÔºÉ While kluster.ai's API is largely compatible with OpenAI's, the following sections outline the specific features and fields that are currently unsupported. Chat completions parameters ÔºÉ When creating a chat completion via the POST https://api.kluster.ai/v1/chat/completions endpoint , the following request parameters are not supported: messages[].name - attribute in system , user , and assistant type message objects messages[].refusal - attribute in assistant type message objects messages[].audio - attribute in assistant type message objects messages[].tool_calls - attribute in assistant type message objects store n modalities response_format service_tier stream_options The following request parameters are supported only with Llama models: tools tool_choice parallel_tool_calls The following request parameters are deprecated : messages[].function_call - attribute in assistant type message objects max_tokens - use max_completion_tokens instead function_call functions For more information on these parameters, refer to OpenAI's API documentation on creating chat completions . Chat completion object ÔºÉ The following fields of the chat completion object are not supported: system_fingerprint usage.completion_tokens_details usage.prompt_tokens_details For more information on these parameters, refer to OpenAI's API documentation on the chat completion object .",
        "summary": "OpenAI compatibility ÔºÉ The kluster.ai API is compatible with OpenAI 's API and SDKs, allowing seamless integration into your existing applications. If you already have an application running with the OpenAI client library, you can easily switch to kluster.ai's API with minimal changes. This ensures a smooth transition without the need for significant refactoring or rework. Configuring OpenAI to us..."
      },
      "Crewai": {
        "url": "https://docs.kluster.ai/get-started/integrations/crewai/",
        "title": "Integrate CrewAI with kluster.ai API | kluster.ai Docs",
        "content": "Integrate CrewAI with kluster.ai ÔºÉ CrewAI is a multi-agent platform that organizes specialized AI agents‚Äîeach with defined roles, tools, and goals‚Äîwithin a structured process to tackle complex tasks efficiently. CrewAI agents streamline workflows and deliver reliable, scalable solutions by coordinating tasks and ensuring smooth collaboration. This guide walks you through integrating kluster.ai with CrewAI to create and run a simple AI agent chatbot that leverages the kluster.ai API. Prerequisites ÔºÉ Before starting, ensure you have the following prerequisites: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. CrewAI installed - the Installation Guide on the CrewAI website will walk you through installing CrewAI, setting up a virtual Python environment, and creating a new project. Note that CrewAI requires a Python version >= 3.10 and < 3.13 Create a project with the CLI ÔºÉ Open your Python virtual environment, and then follow these steps to use the CrewAI CLI to create a new project: Create a project - following the installation guide, create your first project with the following command: crewai create crew INSERT_PROJECT_NAME Select model and provider - during setup, the CLI will ask you to choose a provider and a model. Select openai as the provider and then choose any available model. Because you'll configure kluster.ai as a custom model, your initial model choice won't affect the final integration. The CLI will prompt you for an OpenAI API key, but this isn‚Äôt required. Simply press enter to skip Build a simple AI agent ÔºÉ After finishing the CLI setup, you will see a src directory with files crew.py and main.py . This guide won't use these sample files because they include extra features outside the scope. Follow these steps to continue: Create your first file - create a hello_crew.py file in src/YOUR_PROJECT_NAME to correspond to a simple AI agent chatbot Import modules and select model - open hello_crew.py to add imports and define a custom LLM for kluster.ai by setting the following parameters: provider - you can specify openai_compatible model - choose one of kluster.ai's available models based on your use case. Regardless of which model you choose, prepend its name with openai/ to ensure CrewAI, which relies on LiteLLM, processes your requests correctly base_url - use https://api.kluster.ai/v1 to send requests to the kluster.ai endpoint api_key - replace INSERT_API_KEY in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide hello_crew.py import random from crewai import LLM , Agent , Crew , Process , Task from crewai.project import CrewBase , agent , crew , task @CrewBase class HelloWorldCrew : # Override any default YAML references agents_config = {} tasks_config = {} def __init__ ( self ): \"\"\" When this crew is instantiated, create a custom LLM with your base_url. \"\"\" self . custom_llm = LLM ( provider = \"openai_compatible\" , model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_KLUSTER_API_KEY\" ) This example overrides agents_config and tasks_config with empty dictionaries to tell CrewAI to ignore all YAML files and rely solely on your code, keeping this guide as streamlined as possible. Define your agent - set the agent's role, goal, and backstory, and assign the custom LLM (via the kluster.ai API) for generating creative greetings: hello_crew.py @agent def hello_agent ( self ) -> Agent : \"\"\" A super simple agent with a single purpose: greet the user in a friendly, varied way. \"\"\" return Agent ( role = \"HelloWorldAgent\" , goal = \"Greet the user in a fun and creative way.\" , backstory = \"I'm a friendly agent who greets everyone in a slightly different manner!\" , llm = self . custom_llm , verbose = True ) Give the agent a task - define a task that prompts the agent for a unique, creative greeting using randomness to avoid repetition. Passing this prompt to hello_agent() ensures varied responses. CrewAI requires an expected_output field, defined here as a short greeting: hello_crew.py @task def hello_task ( self ) -> Task : \"\"\" A task that asks the agent to produce a dynamic greeting. \"\"\" random_factor = random . randint ( 100000 , 999999 ) prompt = f \"\"\" You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: { random_factor } Example: \"Hey there, how's your day going?\" \"\"\" return Task ( description = prompt , expected_output = \"A short, creative greeting\" , agent = self . hello_agent () ) Tie it all together with a @crew method - add the following method to return the assembled Crew object with a single agent and task. This method enables CrewAI to coordinate the agent and task you defined: hello_crew.py @crew def hello_crew ( self ) -> Crew : \"\"\" Our entire 'Hello World' crew‚Äîonly 1 agent + 1 task in sequence. \"\"\" return Crew ( agents = self . agents , tasks = self . tasks , process = Process . sequential , verbose = True ) Set up the entry point for the agent - create a new file named hello_main.py . In hello_main.py , import and initialize the HelloWorldCrew class, call its hello_crew() method, and then kickoff() to launch the task sequence: hello_main.py #!/usr/bin/env python from hello_crew import HelloWorldCrew def run (): \"\"\" Kick off the HelloWorld crew with no inputs. \"\"\" HelloWorldCrew () . hello_crew () . kickoff ( inputs = {}) if __name__ == \"__main__\" : run () View complete script hello_crew.py import random from crewai import LLM , Agent , Crew , Process , Task from crewai.project import CrewBase , agent , crew , task @CrewBase class HelloWorldCrew : # Override any default YAML references agents_config = {} tasks_config = {} def __init__ ( self ): \"\"\" When this crew is instantiated, create a custom LLM with your base_url. \"\"\" self . custom_llm = LLM ( provider = \"openai_compatible\" , model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_KLUSTER_API_KEY\" ) @agent def hello_agent ( self ) -> Agent : \"\"\" A super simple agent with a single purpose: greet the user in a friendly, varied way. \"\"\" return Agent ( role = \"HelloWorldAgent\" , goal = \"Greet the user in a fun and creative way.\" , backstory = \"I'm a friendly agent who greets everyone in a slightly different manner!\" , llm = self . custom_llm , verbose = True ) @task def hello_task ( self ) -> Task : \"\"\" A task that asks the agent to produce a dynamic greeting. \"\"\" random_factor = random . randint ( 100000 , 999999 ) prompt = f \"\"\" You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: { random_factor } Example: \"Hey there, how's your day going?\" \"\"\" return Task ( description = prompt , expected_output = \"A short, creative greeting\" , agent = self . hello_agent () ) @crew def hello_crew ( self ) -> Crew : \"\"\" Our entire 'Hello World' crew‚Äîonly 1 agent + 1 task in sequence. \"\"\" return Crew ( agents = self . agents , tasks = self . tasks , process = Process . sequential , verbose = True ) Put it all together ÔºÉ To run your agent, ensure you are in the same directory as your hello_main.py file, then use the following command: python hello_main.py Upon running the script, you'll see output that looks like the following: # Agent: HelloWorldAgent ## Task: You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: 896380 Example: \"Hey there, how's your day going?\" # Agent: HelloWorldAgent ## Final Answer: Hello, it's a beautiful day to shine, how's your sparkle today? And that's it! You've now successfully configured your AI agent harnessing CrewAI and the power of the kluster.ai API!",
        "summary": "Integrate CrewAI with kluster.ai ÔºÉ CrewAI is a multi-agent platform that organizes specialized AI agents‚Äîeach with defined roles, tools, and goals‚Äîwithin a structured process to tackle complex tasks efficiently. CrewAI agents streamline workflows and deliver reliable, scalable solutions by coordinating tasks and ensuring smooth collaboration. This guide walks you through integrating kluster.ai wit..."
      },
      "Dedicated Deployments": {
        "url": "https://docs.kluster.ai/get-started/dedicated-deployments/",
        "title": "Launch dedicated deployments | kluster.ai Docs",
        "content": "Dedicated deployments ÔºÉ Dedicated deployments let you run a private instance of any Hugging Face text model on hardware reserved just for you. Enjoy full control, predictable per‚Äëminute billing, and zero per‚Äëtoken costs. This page covers how to create, use, and stop your dedicated deployments. Create a deployment ÔºÉ Ensure you're logged in to the kluster.ai platform , then navigate to the Dedicated deployments page, then press Launch deployment . Then, complete the following fields to configure your deployment: Deployment name : Enter a clear deployment name (e.g., mydedicated ) so you can spot it later in the console. Model selection : Paste the Hugging Face model ID or URL (e.g., deepseek-ai/DeepSeek-R1 ). If the model is private, provide a Hugging Face access token. Select hardware : Confirm a GPU configuration. Specify auto-shutdown : Set an auto‚Äëshutdown window for your instance to power down after a specified period of inactivity, between 15 minutes to 12 hours. Launch : Review the estimated price and then Click Launch deployment . Spin‚Äëup takes ‚âà20‚Äì30 min; once the status shows Running , copy the endpoint ID, as you'll use that to submit requests. Use your dedicated deployment ÔºÉ After waiting 20-30 minutes for your instance to spin up, you can call it by using the endpoint ID as the model name when making a request. If you're unsure of your endpoint ID, look for it in the Dedicated deployments page . To call your dedicated deployment, you'll need to provide the endpoint ID as the model name when making a request ( INSERT_ENDPOINT_ID in the following example): Python curl from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://api.kluster.ai/v1\" ) response = client . chat . completions . create ( model = \"INSERT_ENDPOINT_ID\" , # Your endpoint ID messages = [{ \"role\" : \"user\" , \"content\" : \"What is the best taco place in SF?\" }], ) print ( response . choices [ 0 ] . message . content ) curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"INSERT_ENDPOINT_ID\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the best taco place in SF?\"}] }' Stop your deployment ÔºÉ Click Stop next to your deployment on the Dedicated deployments page to shut your VM down immediately. Billing ends the moment it powers off. Otherwise, an auto‚Äëshutdown timer kicks in after your specified auto-shutdown period (between 15 minutes and 12 hours of inactivity), depending on the period you chose when spinning up the instance. Questions? Email support@kluster.ai , and we‚Äôll be happy to help!",
        "summary": "Dedicated deployments ÔºÉ Dedicated deployments let you run a private instance of any Hugging Face text model on hardware reserved just for you. Enjoy full control, predictable per‚Äëminute billing, and zero per‚Äëtoken costs. This page covers how to create, use, and stop your dedicated deployments. Create a deployment ÔºÉ Ensure you're logged in to the kluster.ai platform , then navigate to the Dedicated..."
      },
      "Eliza": {
        "url": "https://docs.kluster.ai/get-started/integrations/eliza/",
        "title": "Integrate eliza with kluster.ai | kluster.ai Docs",
        "content": "Integrate eliza with kluster.ai ÔºÉ eliza is an open-source framework designed to create and manage AI agents that can handle a variety of tasks, from simple chat interactions to more complex automation. In this guide, you'll learn how to integrate kluster.ai into eliza to leverage its powerful models and quickly set up your AI-driven workflows. Prerequisites ÔºÉ Before starting, ensure you have the following kluster prerequisites: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. Clone and install the eliza repository - follow the installation instructions on the eliza Quick Start guide Warning Pay careful attention to the eliza prerequisites, including the minimum supported versions of Node.js and pnpm. You will not be able to successfully follow this guide using npm or yarn. Stop at the Configure Environment section in the Quick Start guide, as this guide covers those steps Configure your environment ÔºÉ After installing eliza, it's simple to utilize kluster.ai with eliza. Only three main changes to the .env file are required. Create .env file - run the following command to generate a .env file from the eliza repository example: cp .env.example .env Set variables - update the following variables in the .env file: OPENAI_API_KEY - replace INSERT_API_KEY in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide OPENAI_API_URL - use https://api.kluster.ai/v1 to send requests to the kluster.ai endpoint OPENAI_DEFAULT_MODEL - choose one of kluster.ai's available models based on your use case. You should also set SMALL_OPENAI_MODEL , MEDIUM_OPENAI_MODEL , and LARGE_OPENAI_MODEL to the same value to allow seamless experimentation as different characters use different default models The OpenAI configuration section of your .env file should resemble the following: .env # OpenAI Configuration OPENAI_API_KEY = INSERT_KLUSTER_API_KEY OPENAI_API_URL = https://api.kluster.ai/v1 # Community Plugin for OpenAI Configuration OPENAI_DEFAULT_MODEL = klusterai/Meta-Llama-3.3-70B-Instruct-Turbo SMALL_OPENAI_MODEL = klusterai/Meta-Llama-3.3-70B-Instruct-Turbo MEDIUM_OPENAI_MODEL = klusterai/Meta-Llama-3.3-70B-Instruct-Turbo LARGE_OPENAI_MODEL = klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Run and interact with your first agent ÔºÉ Now that you've configured your environment, you're ready to run your first agent! eliza has several characters you can interact with by prompting or through autonomous tasks like tweeting. This guide relies on the Dobby character for its minimal setup requirements. Verify character configuration - open the dobby.character.json file inside the characters folder. By default, Dobby uses the openai model, which you've already configured to use the kluster.ai API. The Dobby configuration should start with the following: dobby.character.json { \"name\" : \"Dobby\" , \"clients\" : [], \"modelProvider\" : \"openai\" // json truncated for clarity } Run the agent - run the following command from the project root directory to run the Dobby agent: pnpm start --character = \"characters/dobby.character.json\" Launch the UI - in another terminal window, run the following command to launch the web UI: pnpm start:client Your terminal output should resemble the following: pnpm start:client VITE v6.0.11 ready in 824 ms ‚ûú Local: http://localhost:5173/ ‚ûú Network: use --host to expose ‚ûú press h + enter to show help Open your browser - follow the prompts and open your browser to http://localhost:5173/ Put it all together ÔºÉ You can now interact with Dobby by selecting on the Send Message button and starting the conversation: That's it! You've successfully integrated eliza with the kluster.ai API. You're now ready to harness the power of AI agents with the kluster.ai API!",
        "summary": "Integrate eliza with kluster.ai ÔºÉ eliza is an open-source framework designed to create and manage AI agents that can handle a variety of tasks, from simple chat interactions to more complex automation. In this guide, you'll learn how to integrate kluster.ai into eliza to leverage its powerful models and quickly set up your AI-driven workflows. Prerequisites ÔºÉ Before starting, ensure you have the f..."
      },
      "Start Building": {
        "url": "https://docs.kluster.ai/get-started/start-building/",
        "title": "Start building with the kluster.ai API | kluster.ai Docs",
        "content": "Start using the kluster.ai API ÔºÉ The kluster.ai API provides a straightforward way to work with Large Language Models (LLMs) at scale. It is compatible with OpenAI's API and SDKs , making it easy to integrate into your existing workflows with minimal code changes. Get your API key ÔºÉ Navigate to the kluster.ai developer console API Keys section and create a new key. You'll need this for all API requests. For step-by-step instructions, refer to the Get an API key guide. Set up the OpenAI client library ÔºÉ Developers can use the OpenAI libraries with kluster.ai with no changes. To start, you need to install the library: Python pip install \"openai>=1.0.0\" Once the library is installed, you can instantiate an OpenAI client pointing to kluster.ai with the following code and replacing INSERT_API_KEY : Python from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) Check the kluster.ai OpenAI compatibility page for detailed information about the integration. API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Where to go next ÔºÉ Guide Real-time inference Build AI-powered applications that deliver instant, real-time responses. Visit the guide Guide Batch inference Process large-scale data efficiently with AI-powered batch inference. Visit the guide Reference API reference Explore the complete kluster.ai API documentation and usage details. Reference",
        "summary": "Start using the kluster.ai API ÔºÉ The kluster.ai API provides a straightforward way to work with Large Language Models (LLMs) at scale. It is compatible with OpenAI's API and SDKs , making it easy to integrate into your existing workflows with minimal code changes. Get your API key ÔºÉ Navigate to the kluster.ai developer console API Keys section and create a new key. You'll need this for all API req..."
      },
      "Langchain": {
        "url": "https://docs.kluster.ai/get-started/integrations/langchain/",
        "title": "Integrate LangChain with kluster.ai | kluster.ai Docs",
        "content": "Integrate LangChain with kluster.ai ÔºÉ LangChain offers a range of features‚Äîlike memory modules for context tracking, retrieval augmentation to feed external data into prompts, and customizable multi-step ‚Äúchains\" to break down complex tasks. By leveraging these capabilities with the kluster.ai API, you can build more robust and context-aware solutions that seamlessly handle everything from short-form answers to intricate conversations. This guide demonstrates how to integrate the ChatOpenAI class from the langchain_openai package with the kluster.ai API, then walks through building a multi-turn conversational agent that leverages LangChain's memory for context-aware interactions. Prerequisites ÔºÉ Before starting, ensure you have the following: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. A python virtual environment - this is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial LangChain packages installed - install the langchain packages : pip install langchain langchain_community langchain_core langchain_openai As a shortcut, you can also run: pip install \"langchain[all]\" Quick Start ÔºÉ It's easy to integrate kluster.ai with LangChain‚Äîwhen configuring the chat model, point your ChatOpenAI instance to the correct base URL and configure the following settings: Base URL - use https://api.kluster.ai/v1 to send requests to the kluster.ai endpoint API key - replace INSERT_API_KEY in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide Select your model - choose one of kluster.ai's available models based on your use case from langchain_openai import ChatOpenAI llm = ChatOpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , ) llm . invoke ( \"What is the capital of Nepal?\" ) That's all you need to start with LangChain and the kluster.ai API! Next, this guide will explore building a multi-turn conversational agent that showcases how memory and context can elevate your chatbot to a more interactive, intelligent experience. Build a multi-turn conversational agent ÔºÉ This section will explore what LangChain can do beyond a single prompt-and-response interaction. One standout feature of LangChain is its built-in memory, which tracks conversation context across multiple user queries. In the following steps, you'll set up a multi-turn conversational agent that takes advantage of this memory and seamlessly integrates with the kluster.ai API. Create file - create a new file called langchain-advanced.py using the following command in your terminal: touch langchain-advanced.py Import LangChain components - inside your new file, import the following components for memory management, prompt handling, and kluster.ai integration: from langchain.chains.conversation.memory import ConversationBufferMemory from langchain_community.chat_message_histories import ChatMessageHistory from langchain_core.messages import HumanMessage from langchain_core.prompts import ChatPromptTemplate , MessagesPlaceholder from langchain_openai import ChatOpenAI Create a memory instance - to store and manage the conversation's context, allowing the chatbot to remember previous user messages. # Create a memory instance to store the conversation message_history = ChatMessageHistory () memory = ConversationBufferMemory ( memory_key = \"chat_history\" , return_messages = True ) Configure the ChatOpenAI model - point to kluster.ai's endpoint with your API key and chosen model. Remember, you can always change the selected model based on your needs # Create your LLM, pointing to kluster.ai's endpoint llm = ChatOpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , ) Define a prompt template - include a system instruction for the assistant, a placeholder for the conversation history, and an input slot for the user's query # Define the prompt template, including the system instruction and placeholders prompt = ChatPromptTemplate . from_messages ([ ( \"system\" , \"You are a helpful assistant.\" ), MessagesPlaceholder ( variable_name = \"chat_history\" ), ( \"human\" , \" {input} \" ) ]) Create the ConversationChain - pass in the LLM, memory, and this prompt template so every new user query is automatically enriched with the stored conversation context and guided by the assistant's role # Create the conversation chain conversation = ConversationChain ( llm = llm , memory = memory , prompt = prompt ) Prompt the model with the first question - you can prompt the model with any question. The example chosen here is designed to demonstrate context awareness between questions # Send the first user prompt question1 = \"Hello! Can you tell me something interesting about the city of Kathmandu?\" print ( \"Question 1:\" , question1 ) response1 = conversation . predict ( input = question1 ) print ( \"Response 1:\" , response1 ) Pose a follow-up question - ask another question without resupplying the city name and notice how LangChain's memory implicitly handles the context. Return and print the questions and responses to see how the conversation informs each new query to create multi-turn interactions # Send a follow-up question referencing previous context question2 = \"What is the population of that city?\" print ( \" \\n Question 2:\" , question2 ) response2 = conversation . predict ( input = question2 ) print ( \"Response 2:\" , response2 ) View complete script langchain-advanced.py from langchain.chains import ConversationChain from langchain.chains.conversation.memory import ConversationBufferMemory from langchain_community.chat_message_histories import ChatMessageHistory from langchain_core.messages import HumanMessage from langchain_core.prompts import ChatPromptTemplate , MessagesPlaceholder from langchain_openai import ChatOpenAI # Create a memory instance to store the conversation message_history = ChatMessageHistory () memory = ConversationBufferMemory ( memory_key = \"chat_history\" , return_messages = True ) # Create your LLM, pointing to kluster.ai's endpoint llm = ChatOpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , ) # Define the prompt template, including the system instruction and placeholders prompt = ChatPromptTemplate . from_messages ([ ( \"system\" , \"You are a helpful assistant.\" ), MessagesPlaceholder ( variable_name = \"chat_history\" ), ( \"human\" , \" {input} \" ) ]) # Create the conversation chain conversation = ConversationChain ( llm = llm , memory = memory , prompt = prompt ) # Send the first user prompt question1 = \"Hello! Can you tell me something interesting about the city of Kathmandu?\" print ( \"Question 1:\" , question1 ) response1 = conversation . predict ( input = question1 ) print ( \"Response 1:\" , response1 ) # Send a follow-up question referencing previous context question2 = \"What is the population of that city?\" print ( \" \\n Question 2:\" , question2 ) response2 = conversation . predict ( input = question2 ) print ( \"Response 2:\" , response2 ) Put it all together ÔºÉ Use the following command to run your script: python langchain-advanced.py You should see output that resembles the following: python langchain.py Question 1: Hello! Can you tell me something interesting about the city of Kathmandu? Response 1: Kathmandu, the capital city of Nepal, is indeed a treasure trove of history, culture, and natural beauty. Here's something interesting: Kathmandu is home to the famous Boudhanath Stupa, a UNESCO World Heritage Site. It's one of the largest Buddhist stupas in the world and is considered a sacred site by Buddhists. The stupa is over 36 meters (118 feet) high and is built in a unique octagonal shape. Its massive size is so prominent that it can be seen from many parts of the city. Another fascinating fact is that Kathmandu has managed to conserve its rich cultural heritage, which dates back to the 12th century. You can see ancient temples, palaces, streets, and marketplaces that have been beautifully preserved and restored. Lastly, Kathmandu is also known for its Newar culture, which is the indigenous culture of the city. The Newars have a rich tradition of art, music, and cuisine, which is reflected in the vibrant festivals and celebrations that take place throughout the year. Would you like to know more about Kathmandu's culture, history, or maybe some of its modern attractions? Question 2: What is the population of that city? Response 2: Kathmandu, the capital city of Nepal, has a population of around 374,405 people (as per the 2021 estimates). However, the Kathmandu Valley, which includes the surrounding municipalities and areas, has a population of over 3.2 million people. When considering the larger metropolitan area that includes the neighboring cities like Lalitpur (Patan) and Bhaktapur, the population exceeds 5 million people, making it one of the largest urban agglomerations in Nepal. It's worth noting that Nepal's population density is relatively high, with many people living in urban areas. The Kathmandu Valley, in particular, is one of the most densely populated regions in the country. That's it! You've successfully integrated LangChain with the kluster.ai API, and your configured multi-turn conversational agent is ready to leverage the power of LangChain and the kluster.ai API. For more information about the capabilities of LangChain, be sure to check out the LangChain docs .",
        "summary": "Integrate LangChain with kluster.ai ÔºÉ LangChain offers a range of features‚Äîlike memory modules for context tracking, retrieval augmentation to feed external data into prompts, and customizable multi-step ‚Äúchains\" to break down complex tasks. By leveraging these capabilities with the kluster.ai API, you can build more robust and context-aware solutions that seamlessly handle everything from short-f..."
      }
    },
    "üìà Use Cases & Tutorials": {
      "Use Cases & Tutorials": {
        "url": "https://docs.kluster.ai/tutorials/",
        "title": "Using OpenAI API | kluster.ai Docs",
        "content": "Text classification with kluster.ai API ¬∂ Text classification is assigning a class/label to a given text, and it is a common go-to example to demonstrate how helpful an AI model can be. This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to classify a dataset based on a predefined set of categories. The example uses an extract from the IMDB top 1000 movies dataset and categorizes them into \"Action,\" \"Adventure,\" \"Comedy,\" \"Crime,\" \"Documentary,\" \"Drama,\" \"Fantasy,\" \"Horror,\" \"Romance,\" or \"Sci-Fi.\" You can adapt this example by using your data and categories relevant to your use case. With this approach, you can effortlessly process datasets of any scale, big or small, and obtain categorized results powered by a state-of-the-art language model. Prerequisites ¬∂ Before getting started, ensure you have the following: A kluster.ai account - sign up on the kluster.ai platform if you don't have one A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide Setup ¬∂ In this notebook, we'll use Python's getpass module to safely input the key. After execution, please provide your unique kluster.ai API key (ensure no spaces). In [1]: Copied! from getpass import getpass api_key = getpass ( \"Enter your kluster.ai API key: \" ) from getpass import getpass api_key = getpass(\"Enter your kluster.ai API key: \") Next, ensure you've installed OpenAI Python library: In [2]: Copied! % pip install - q openai %pip install -q openai Note: you may need to restart the kernel to use updated packages. With the OpenAI Python library installed, we import the necessary dependencies for the tutorial: In [3]: Copied! from openai import OpenAI import pandas as pd import time import json import os from IPython.display import clear_output , display from openai import OpenAI import pandas as pd import time import json import os from IPython.display import clear_output, display And then, initialize the client by pointing it to the kluster.ai endpoint, and passing your API key. In [4]: Copied! # Set up the client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) # Set up the client client = OpenAI( base_url=\"https://api.kluster.ai/v1\", api_key=api_key, ) Get the data ¬∂ Now that you've initialized an OpenAI-compatible client pointing to kluster.ai, we can talk about the data. This notebook includes a preloaded sample dataset derived from the Top 1000 IMDb Movies dataset. It contains movie descriptions ready for classification. No additional setup is needed. Simply proceed to the next steps to begin working with this data. In [5]: Copied! df = pd . DataFrame ({ \"text\" : [ \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\" , \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\" , \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\" , \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\" , \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\" ] }) df = pd.DataFrame({ \"text\": [ \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\", \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\", \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\", \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\", \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\" ] }) Perform batch inference ¬∂ To execute the batch inference job, we'll take the following steps: Create the batch job file - we'll generate a JSON lines file with the desired requests to be processed by the model Upload the batch job file - once it is ready, we'll upload it to the kluster.ai platform using the API, where it will be processed. We'll receive a unique ID associated with our file Start the batch job - after the file is uploaded, we'll initiate the job to process the uploaded data, using the file ID obtained before Monitor job progress - (optional) track the status of the batch job to ensure it has been successfully completed Retrieve results - once the job has completed execution, we can access and process the resultant data This notebook is prepared for you to follow along. Run the cells below to watch it all come together. Create the batch job file ¬∂ This example selects the deepseek-ai/DeepSeek-V3-0324 model. If you'd like to use a different model, feel free to change it by modifying the model field. Please refer to the Supported models section for a list of the models we support. The following snippets prepare the JSONL file, where each line represents a different request. Note that each separate batch request can have its own model. Also, we are using a temperature of 0.5 but feel free to change it and play around with the different outcomes (but we are only asking to respond with a single word, the genre). In [6]: Copied! # Prompt SYSTEM_PROMPT = ''' Classify the main genre of the given movie description based on the following genres (Respond with only the genre): ‚ÄúAction‚Äù, ‚ÄúAdventure‚Äù, ‚ÄúComedy‚Äù, ‚ÄúCrime‚Äù, ‚ÄúDocumentary‚Äù, ‚ÄúDrama‚Äù, ‚ÄúFantasy‚Äù, ‚ÄúHorror‚Äù, ‚ÄúRomance‚Äù, ‚ÄúSci-Fi‚Äù. ''' # Models #model=\"deepseek-ai/DeepSeek-R1\" model = \"deepseek-ai/DeepSeek-V3-0324\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\" # Ensure the directory exists os . makedirs ( \"text_clasification\" , exist_ok = True ) # Create the batch job file with the prompt and content def create_batch_file ( df ): batch_list = [] for index , row in df . iterrows (): content = row [ 'text' ] request = { \"custom_id\" : f \"movie_classification- { index } \" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : model , \"temperature\" : 0.5 , \"messages\" : [ { \"role\" : \"system\" , \"content\" : SYSTEM_PROMPT }, { \"role\" : \"user\" , \"content\" : content } ], } } batch_list . append ( request ) return batch_list # Save file def save_batch_file ( batch_list ): filename = f \"text_clasification/batch_job_request.jsonl\" with open ( filename , 'w' ) as file : for request in batch_list : file . write ( json . dumps ( request ) + ' \\n ' ) return filename # Prompt SYSTEM_PROMPT = ''' Classify the main genre of the given movie description based on the following genres (Respond with only the genre): ‚ÄúAction‚Äù, ‚ÄúAdventure‚Äù, ‚ÄúComedy‚Äù, ‚ÄúCrime‚Äù, ‚ÄúDocumentary‚Äù, ‚ÄúDrama‚Äù, ‚ÄúFantasy‚Äù, ‚ÄúHorror‚Äù, ‚ÄúRomance‚Äù, ‚ÄúSci-Fi‚Äù. ''' # Models #model=\"deepseek-ai/DeepSeek-R1\" model=\"deepseek-ai/DeepSeek-V3-0324\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\" # Ensure the directory exists os.makedirs(\"text_clasification\", exist_ok=True) # Create the batch job file with the prompt and content def create_batch_file(df): batch_list = [] for index, row in df.iterrows(): content = row['text'] request = { \"custom_id\": f\"movie_classification-{index}\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": { \"model\": model, \"temperature\": 0.5, \"messages\": [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": content} ], } } batch_list.append(request) return batch_list # Save file def save_batch_file(batch_list): filename = f\"text_clasification/batch_job_request.jsonl\" with open(filename, 'w') as file: for request in batch_list: file.write(json.dumps(request) + '\\n') return filename Let's run the functions we've defined before: In [7]: Copied! batch_list = create_batch_file ( df ) data_dir = save_batch_file ( batch_list ) print ( data_dir ) batch_list = create_batch_file(df) data_dir = save_batch_file(batch_list) print(data_dir) text_clasification/batch_job_request.jsonl Next, we can preview what that batch job file looks like: In [8]: Copied! ! head - n 1 text_clasification / batch_job_request . jsonl !head -n 1 text_clasification/batch_job_request.jsonl {\"custom_id\": \"movie_classification-0\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n Classify the main genre of the given movie description based on the following genres (Respond with only the genre):\\n \\u201cAction\\u201d, \\u201cAdventure\\u201d, \\u201cComedy\\u201d, \\u201cCrime\\u201d, \\u201cDocumentary\\u201d, \\u201cDrama\\u201d, \\u201cFantasy\\u201d, \\u201cHorror\\u201d, \\u201cRomance\\u201d, \\u201cSci-Fi\\u201d.\\n \"}, {\"role\": \"user\", \"content\": \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\"}]}} Upload batch job file to kluster.ai ¬∂ Now that we‚Äôve prepared our input file, it‚Äôs time to upload it to the kluster.ai platform. To do so, you can use the files.create endpoint of the client, where the purpose is set to batch . This will return the file ID, which we need to log for the next steps. In [9]: Copied! # Upload batch job request file with open ( data_dir , 'rb' ) as file : upload_response = client . files . create ( file = file , purpose = \"batch\" ) # Print job ID file_id = upload_response . id print ( f \"File uploaded successfully. File ID: { file_id } \" ) # Upload batch job request file with open(data_dir, 'rb') as file: upload_response = client.files.create( file=file, purpose=\"batch\" ) # Print job ID file_id = upload_response.id print(f\"File uploaded successfully. File ID: {file_id}\") File uploaded successfully. File ID: 6801403efba4aabfd069a682 Start the batch job ¬∂ Once the file has been successfully uploaded, we're ready to start (create) the batch job by providing the file ID we got in the previous step. To do so, we use the batches.create method, for which we need to set the endpoint to /v1/chat/completions . This will return the batch job details, with the ID. In [10]: Copied! # Create batch job with completions endpoint batch_job = client . batches . create ( input_file_id = file_id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" ) print ( \" \\n Batch job created:\" ) batch_dict = batch_job . model_dump () print ( json . dumps ( batch_dict , indent = 2 )) # Create batch job with completions endpoint batch_job = client.batches.create( input_file_id=file_id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\" ) print(\"\\nBatch job created:\") batch_dict = batch_job.model_dump() print(json.dumps(batch_dict, indent=2)) Batch job created: { \"id\": \"6801403e0969ab67de09ec8d\", \"completion_window\": \"24h\", \"created_at\": 1744912446, \"endpoint\": \"/v1/chat/completions\", \"input_file_id\": \"6801403efba4aabfd069a682\", \"object\": \"batch\", \"status\": \"pre_schedule\", \"cancelled_at\": null, \"cancelling_at\": null, \"completed_at\": null, \"error_file_id\": null, \"errors\": [], \"expired_at\": null, \"expires_at\": 1744998846, \"failed_at\": null, \"finalizing_at\": null, \"in_progress_at\": null, \"metadata\": {}, \"output_file_id\": null, \"request_counts\": { \"completed\": 0, \"failed\": 0, \"total\": 0 } } /Users/kevin/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings: Expected `Errors` but got `list` with value `[]` - serialized value may not be as expected return self.__pydantic_serializer__.to_python( Check job progress ¬∂ Now that your batch job has been created, you can track its progress. To monitor the job's progress, we can use the batches.retrieve method and pass the batch job ID. The response contains an status field that tells us if it is completed or not, and the subsequent status of each job separately. The following snippet checks the status every 10 seconds until the entire batch is completed: In [11]: Copied! all_completed = False # Loop to check status every 10 seconds while not all_completed : all_completed = True output_lines = [] updated_job = client . batches . retrieve ( batch_job . id ) if updated_job . status != \"completed\" : all_completed = False completed = updated_job . request_counts . completed total = updated_job . request_counts . total output_lines . append ( f \"Job status: { updated_job . status } - Progress: { completed } / { total } \" ) else : output_lines . append ( f \"Job completed!\" ) # Clear the output and display updated status clear_output ( wait = True ) for line in output_lines : display ( line ) if not all_completed : time . sleep ( 10 ) all_completed = False # Loop to check status every 10 seconds while not all_completed: all_completed = True output_lines = [] updated_job = client.batches.retrieve(batch_job.id) if updated_job.status != \"completed\": all_completed = False completed = updated_job.request_counts.completed total = updated_job.request_counts.total output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\") else: output_lines.append(f\"Job completed!\") # Clear the output and display updated status clear_output(wait=True) for line in output_lines: display(line) if not all_completed: time.sleep(10) 'Job completed!' Get the results ¬∂ With the job completed, we'll retrieve the results and review the responses generated for each request. The results are parsed. To fetch the results from the platform, you need to retrieve the output_file_id from the batch job, and then use the files.content endpoint, providing that specific file ID. Note that the job status must be completed for you to retrieve the results! In [12]: Copied! #Parse results as a JSON object def parse_json_objects ( data_string ): if isinstance ( data_string , bytes ): data_string = data_string . decode ( 'utf-8' ) json_strings = data_string . strip () . split ( ' \\n ' ) json_objects = [] for json_str in json_strings : try : json_obj = json . loads ( json_str ) json_objects . append ( json_obj ) except json . JSONDecodeError as e : print ( f \"Error parsing JSON: { e } \" ) return json_objects # Retrieve results with job ID job = client . batches . retrieve ( batch_job . id ) result_file_id = job . output_file_id result = client . files . content ( result_file_id ) . content # Parse JSON results parsed_result = parse_json_objects ( result ) # Extract and print only the content of each response print ( \" \\n Extracted Responses:\" ) for item in parsed_result : try : content = item [ \"response\" ][ \"body\" ][ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ] print ( content ) except KeyError as e : print ( f \"Missing key in response: { e } \" ) #Parse results as a JSON object def parse_json_objects(data_string): if isinstance(data_string, bytes): data_string = data_string.decode('utf-8') json_strings = data_string.strip().split('\\n') json_objects = [] for json_str in json_strings: try: json_obj = json.loads(json_str) json_objects.append(json_obj) except json.JSONDecodeError as e: print(f\"Error parsing JSON: {e}\") return json_objects # Retrieve results with job ID job = client.batches.retrieve(batch_job.id) result_file_id = job.output_file_id result = client.files.content(result_file_id).content # Parse JSON results parsed_result = parse_json_objects(result) # Extract and print only the content of each response print(\"\\nExtracted Responses:\") for item in parsed_result: try: content = item[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"] print(content) except KeyError as e: print(f\"Missing key in response: {e}\") Extracted Responses: Romance Drama Drama Drama Crime Summary ¬∂ This tutorial used the chat completion endpoint to perform a simple text classification task with batch inference. This particular example clasified a series of movies based on their description. To submit a batch job we've: Created the JSONL file, where each line of the file represented a separate request Submitted the file to the platform Started the batch job, and monitored its progress Once completed, we fetched the results All of this using the OpenAI Python library and API, no changes needed! Kluster.ai's batch API empowers you to scale your workflows seamlessly, making it an invaluable tool for processing extensive datasets. As next steps, feel free to create your own dataset, or expand on top of this existing example. Good luck!",
        "summary": "Text classification with kluster.ai API ¬∂ Text classification is assigning a class/label to a given text, and it is a common go-to example to demonstrate how helpful an AI model can be. This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to classify a dataset based on a predefined set of categories. The example uses an extract from the IMDB top 1000 movies ..."
      },
      "Reliability Check": {
        "url": "https://docs.kluster.ai/tutorials/klusterai-api/reliability-check/",
        "title": "Reliability check | kluster.ai Docs",
        "content": "Reliability check with the kluster.ai API ¬∂ Introduction ¬∂ Reliability issues in AI occur when models generate information that appears plausible but is unreliable or unsupported by the provided context. This poses significant risks in production applications, particularly in domains where accuracy is critical. This tutorial demonstrates how to use Verify to identify and prevent reliability issues in your applications. We'll explore available methods: a dedicated API endpoint and via the OpenAI compatible chat completions endpoint. The service can evaluate AI responses based on provided context (perfect for RAG applications) or perform real-time verification against general knowledge. By following this tutorial, you'll learn how to: Verify reliability in individual Q&A pairs. Compare general knowledge verification vs. context validation modes. Validate responses in full conversation histories. Prerequisites ¬∂ Before getting started, ensure you have the following: A kluster.ai account : sign up on the kluster.ai platform if you don't have one A kluster.ai API key : after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide Setup ¬∂ In this notebook, we'll use Python's getpass module to input the key safely. After execution, please provide your unique kluster.ai API key (ensure no spaces). In [1]: Copied! from getpass import getpass api_key = getpass ( \"Enter your kluster.ai API key: \" ) from getpass import getpass api_key = getpass(\"Enter your kluster.ai API key: \") Next, ensure you've installed OpenAI Python and other required libraries: In [2]: Copied! % pip install - q openai requests %pip install -q openai requests Note: you may need to restart the kernel to use updated packages. With the OpenAI Python library installed, we import the necessary dependencies for the tutorial: In [3]: Copied! from openai import OpenAI import time import json import requests from openai import OpenAI import time import json import requests And then, initialize the client by pointing it to the kluster.ai endpoint, and passing your API key. In [4]: Copied! # Define the base URL for both methods base_url_endpoint = \"https://api.kluster.ai/v1/verify/reliability\" #To test with HTTP requests base_url = \"https://api.kluster.ai/v1\" # To test with OpenAI client # Set up the client client = OpenAI ( base_url = base_url_endpoint , api_key = api_key , ) # Define the base URL for both methods base_url_endpoint = \"https://api.kluster.ai/v1/verify/reliability\" #To test with HTTP requests base_url= \"https://api.kluster.ai/v1\" # To test with OpenAI client # Set up the client client = OpenAI( base_url=base_url_endpoint, api_key=api_key, ) Dedicated reliability endpoint ¬∂ The reliability check dedicated endpoint validates whether an answer to a specific question contains unreliable or incorrect information. It operates in two modes: General knowledge verification : when no context is provided, the service verifies answers by comparing it to other sources. Context validation mode : when context is provided, the service only validates answers against that context. For our example, we'll create diverse test cases to demonstrate the reliability check capabilities: General knowledge verification examples : questions where the service verifies against external sources. Context validation examples : scenarios where responses must align with provided context. Search results demonstration : see how enabling return_search_results provides sources used for verification, helping you understand and trust the service's decisions. Invoice extraction example : a practical use case for document processing. Chat completions example : use the convenient OpenAI SDK to check for reliability issues. To call the endpoint, we'll use the following function: In [5]: Copied! # Function that runs the reliability check for general knowledge examples def check_reliability_qa ( prompt , output , context = None , return_search_results = False ): \"\"\"Check reliability using the dedicated endpoint\"\"\" url = base_url_endpoint headers = { \"Authorization\" : f \"Bearer { api_key } \" , \"Content-Type\" : \"application/json\" } # Prepare the payload payload = { \"prompt\" : prompt , \"output\" : output , \"return_search_results\" : return_search_results } # Add context if provided if context : payload [ \"context\" ] = context # Make the POST request to the API response = requests . post ( url , headers = headers , json = payload ) return response . json () # Function that runs the reliability check for general knowledge examples def check_reliability_qa(prompt, output, context=None, return_search_results=False): \"\"\"Check reliability using the dedicated endpoint\"\"\" url = base_url_endpoint headers = { \"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\" } # Prepare the payload payload = { \"prompt\": prompt, \"output\": output, \"return_search_results\": return_search_results } # Add context if provided if context: payload[\"context\"] = context # Make the POST request to the API response = requests.post(url, headers=headers, json=payload) return response.json() Prepare the data ¬∂ In all scenarios, a prompt and output most be provided. The prompt is the message/question from the user, and the output is the answer from the Model. In addition, we are also providing the ground truth in regards to hallucination. In [6]: Copied! # Create test datasets general_knowledge_examples = [ { \"prompt\" : \"What is the capital of France?\" , \"output\" : \"The capital of France is London.\" , \"expected_hallucination\" : True }, { \"prompt\" : \"When was the Eiffel Tower built?\" , \"output\" : \"The Eiffel Tower was built in 1889 for the Paris Exposition.\" , \"expected_hallucination\" : False }, { \"prompt\" : \"Are ghosts real?\" , \"output\" : \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\" , \"expected_hallucination\" : True } ] # Create test datasets general_knowledge_examples = [ { \"prompt\": \"What is the capital of France?\", \"output\": \"The capital of France is London.\", \"expected_hallucination\": True }, { \"prompt\": \"When was the Eiffel Tower built?\", \"output\": \"The Eiffel Tower was built in 1889 for the Paris Exposition.\", \"expected_hallucination\": False }, { \"prompt\": \"Are ghosts real?\", \"output\": \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\", \"expected_hallucination\": True } ] For context validation, the necessary data must be provided via the context field. In [7]: Copied! context_validation_examples = [ { \"prompt\" : \"What's the invoice date?\" , \"output\" : \"The invoice date is May 22, 2025.\" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"expected_hallucination\" : False }, { \"prompt\" : \"What's the total amount on the invoice?\" , \"output\" : \"The total amount is 8500 USD.\" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"expected_hallucination\" : True }, { \"prompt\" : \"Who is the client mentioned in the document?\" , \"output\" : \"The client is Acme.\" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"expected_hallucination\" : False } ] context_validation_examples = [ { \"prompt\": \"What's the invoice date?\", \"output\": \"The invoice date is May 22, 2025.\", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\", \"expected_hallucination\": False }, { \"prompt\": \"What's the total amount on the invoice?\", \"output\": \"The total amount is 8500 USD.\", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\", \"expected_hallucination\": True }, { \"prompt\": \"Who is the client mentioned in the document?\", \"output\": \"The client is Acme.\", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\", \"expected_hallucination\": False } ] General knowledge verification ¬∂ Let's test general knowledge verification mode with our examples: In [8]: Copied! # Test general knowledge verification mode verification_results = [] for i , example in enumerate ( general_knowledge_examples ): print ( f \"=== General Knowledge Verification Example { i + 1 } ===\" ) print ( f \"Question: { example [ 'prompt' ] } \" ) print ( f \"Answer: { example [ 'output' ] } \" ) print ( f \"Expected Unreliable: { example [ 'expected_hallucination' ] } \" ) print () result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], return_search_results = False ) verification_results . append ({ 'example' : example , 'result' : result }) print ( \"Check Result:\" ) print ( f \"Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) print ( f \"Explanation: { result . get ( 'explanation' , 'N/A' ) } \" ) print ( f \"Tokens Used: { result . get ( 'usage' , {}) } \" ) print ( \" \\n \" + \"=\" * 80 + \" \\n \" ) # Test general knowledge verification mode verification_results = [] for i, example in enumerate(general_knowledge_examples): print(f\"=== General Knowledge Verification Example {i+1} ===\") print(f\"Question: {example['prompt']}\") print(f\"Answer: {example['output']}\") print(f\"Expected Unreliable: {example['expected_hallucination']}\") print() result = check_reliability_qa( prompt=example['prompt'], output=example['output'], return_search_results=False ) verification_results.append({ 'example': example, 'result': result }) print(\"Check Result:\") print(f\"Is Unreliable: {result.get('is_hallucination', 'N/A')}\") print(f\"Explanation: {result.get('explanation', 'N/A')}\") print(f\"Tokens Used: {result.get('usage', {})}\") print(\"\\n\" + \"=\"*80 + \"\\n\") === General Knowledge Verification Example 1 === Question: What is the capital of France? Answer: The capital of France is London. Expected Unreliable: True Check Result: Is Unreliable: True Explanation: The user asked for the capital of France. The correct capital of France is Paris, not London. London is the capital of England, not France, making the response factually incorrect. Tokens Used: {'completion_tokens': 118, 'prompt_tokens': 937, 'total_tokens': 1055} ================================================================================ === General Knowledge Verification Example 2 === Question: When was the Eiffel Tower built? Answer: The Eiffel Tower was built in 1889 for the Paris Exposition. Expected Unreliable: False Check Result: Is Unreliable: False Explanation: The response correctly states that the Eiffel Tower was built in 1889. The Eiffel Tower was indeed constructed for the 1889 World's Fair in Paris, making the additional context accurate. The information provided is verifiable and aligns with historical facts about the Eiffel Tower. Tokens Used: {'completion_tokens': 418, 'prompt_tokens': 957, 'total_tokens': 1375} ================================================================================ === General Knowledge Verification Example 3 === Question: Are ghosts real? Answer: Yes, there is a recent scientific study from Harvard that confirms ghosts exist. Expected Unreliable: True Check Result: Is Unreliable: True Explanation: The original user request asks if ghosts are real. The response from the other LLM claims that a recent scientific study from Harvard confirms the existence of ghosts. The search results provide several links related to Harvard and the study of ghosts or supernatural phenomena, but none of them directly confirm the existence of ghosts. The snippets from the search results indicate that Harvard has conducted studies and courses on the topic of ghosts and supernatural phenomena, but these are primarily focused on folklore, mythology, and the cultural or psychological aspects of belief in ghosts. There is no clear evidence in the search results of a scientific study from Harvard that confirms the existence of ghosts. The response from the other LLM is an example of hallucination because it presents a factual claim (a recent scientific study from Harvard confirming ghosts exist) that is not supported by the search results. Tokens Used: {'completion_tokens': 282, 'prompt_tokens': 1744, 'total_tokens': 2026} ================================================================================ Enable search results ¬∂ When enabling the property return_search_results=true , the reliability check feature will return the sources used for the verification. In [9]: Copied! # Test general knowledge verification with search results enabled print ( \"=== General Knowledge Verification with Search Results ===\" ) example = { \"prompt\" : \"Are ghosts real?\" , \"output\" : \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\" } # Let's run the check with search results enabled result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], return_search_results = True # Enable search results ) # Display the result print ( f \"Question: { example [ 'prompt' ] } \" ) print ( f \"Answer: { example [ 'output' ] } \" ) print ( f \" \\n Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) # Display search results if 'search_results' in result and result [ 'search_results' ]: print ( f \" \\n üìö Search Results Used ( { len ( result [ 'search_results' ]) } sources):\" ) for idx , source in enumerate ( result [ 'search_results' ][: 5 ], 1 ): # Show first 5 print ( f \" \\n { idx } . { source . get ( 'title' , 'No title' ) } \" ) print ( f \" üìÑ { source . get ( 'snippet' , 'No snippet' )[: 150 ] } ...\" ) print ( f \" üîó { source . get ( 'link' , 'No link' ) } \" ) else : print ( \" \\n No search results returned\" ) print ( f \" \\n Tokens Used: { result . get ( 'usage' , {}) } \" ) # Test general knowledge verification with search results enabled print(\"=== General Knowledge Verification with Search Results ===\") example = { \"prompt\": \"Are ghosts real?\", \"output\": \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\" } # Let's run the check with search results enabled result = check_reliability_qa( prompt=example['prompt'], output=example['output'], return_search_results=True # Enable search results ) # Display the result print(f\"Question: {example['prompt']}\") print(f\"Answer: {example['output']}\") print(f\"\\nIs Unreliable: {result.get('is_hallucination', 'N/A')}\") # Display search results if 'search_results' in result and result['search_results']: print(f\"\\nüìö Search Results Used ({len(result['search_results'])} sources):\") for idx, source in enumerate(result['search_results'][:5], 1): # Show first 5 print(f\"\\n{idx}. {source.get('title', 'No title')}\") print(f\" üìÑ {source.get('snippet', 'No snippet')[:150]}...\") print(f\" üîó {source.get('link', 'No link')}\") else: print(\"\\nNo search results returned\") print(f\"\\nTokens Used: {result.get('usage', {})}\") === General Knowledge Verification with Search Results === Question: Are ghosts real? Answer: Yes, there is a recent scientific study from Harvard that confirms ghosts exist. Is Unreliable: True üìö Search Results Used (10 sources): 1. The Allure of the Supernatural | Harvard Independent üìÑ Focusing in on ghosts and other such spirits, the study revealed a greater proportion of belief in the mystical than the national averages ...... üîó https://harvardindependent.com/the-allure-of-the-supernatural/ 2. Harvard class studies supernatural stories üìÑ Folklore & Mythology course examines how tales of spirits and ghosts from the past affect the present and the future.... üîó https://news.harvard.edu/gazette/story/2021/10/harvard-class-studies-supernatural-stories/ 3. The Ghost Studies: New Perspectives on the Origins of Paranormal ... üìÑ New and exciting scientific theories that explain apparitions, hauntings, and communications from the dead.... üîó https://www.harvard.com/book/9781632651211 4. Did Scientists Just Discover the Cause of Ghost Sightings? | Unveiled üìÑ Ghosts & the Afterlife: Science Unveils the Mystery of Spirits ¬∑ Ghosts Aren't Real: 4 Scientific Explanations for Paranormal Activity ¬∑ Harvard ...... üîó https://www.youtube.com/watch?v=fuFOGYxb6bI 5. The Ivy and the Occult | Harvard Independent üìÑ While ghost stories and psychical research seem to have largely disappeared from Harvard over the years, there is still an eclectic mix of ...... üîó https://harvardindependent.com/the-ivy-and-the-occult/ Tokens Used: {'completion_tokens': 308, 'prompt_tokens': 1778, 'total_tokens': 2086} Context validation mode ¬∂ The context validation mode uses the context property as the ground truth. When enabled, the service does not verify the answer using external knowledge; instead, it focuses on identifying reliability issues based solely on the information within the provided context . In [10]: Copied! # Test context validation mode context_results = [] # for i , example in enumerate ( context_validation_examples ): print ( f \"=== Context Validation Example { i + 1 } ===\" ) print ( f \"Context: { example [ 'context' ] } \" ) print ( f \"Question: { example [ 'prompt' ] } \" ) print ( f \"Answer: { example [ 'output' ] } \" ) print ( f \"Expected Unreliable: { example [ 'expected_hallucination' ] } \" ) print () # Run the reliability check with context result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], context = example [ 'context' ], return_search_results = False ) context_results . append ({ 'example' : example , 'result' : result }) # Display the results print ( \"Check Result:\" ) print ( f \"Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) print ( f \"Explanation: { result . get ( 'explanation' , 'N/A' ) } \" ) print ( f \"Tokens Used: { result . get ( 'usage' , {}) } \" ) print ( \" \\n \" + \"=\" * 80 + \" \\n \" ) # Test context validation mode context_results = [] # for i, example in enumerate(context_validation_examples): print(f\"=== Context Validation Example {i+1} ===\") print(f\"Context: {example['context']}\") print(f\"Question: {example['prompt']}\") print(f\"Answer: {example['output']}\") print(f\"Expected Unreliable: {example['expected_hallucination']}\") print() # Run the reliability check with context result = check_reliability_qa( prompt=example['prompt'], output=example['output'], context=example['context'], return_search_results=False ) context_results.append({ 'example': example, 'result': result }) # Display the results print(\"Check Result:\") print(f\"Is Unreliable: {result.get('is_hallucination', 'N/A')}\") print(f\"Explanation: {result.get('explanation', 'N/A')}\") print(f\"Tokens Used: {result.get('usage', {})}\") print(\"\\n\" + \"=\"*80 + \"\\n\") === Context Validation Example 1 === Context: InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C Question: What's the invoice date? Answer: The invoice date is May 22, 2025. Expected Unreliable: False Check Result: Is Unreliable: False Explanation: The answer accurately reflects the information given in the document regarding the invoice date, making a reasonable assumption about the abbreviated year. Tokens Used: {'completion_tokens': 438, 'prompt_tokens': 267, 'total_tokens': 705} ================================================================================ === Context Validation Example 2 === Context: InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C Question: What's the total amount on the invoice? Answer: The total amount is 8500 USD. Expected Unreliable: True Check Result: Is Unreliable: True Explanation: The answer contradicts the document by stating a different amount and currency. Tokens Used: {'completion_tokens': 426, 'prompt_tokens': 267, 'total_tokens': 693} ================================================================================ === Context Validation Example 3 === Context: InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C Question: Who is the client mentioned in the document? Answer: The client is Acme. Expected Unreliable: False Check Result: Is Unreliable: False Explanation: To determine whether the answer is faithful to the contents of the document, we need to analyze the provided information. The document contains a specific entry: \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\". Within this entry, it is explicitly stated that the \"Client:Acme\". The question asks, \"Who is the client mentioned in the document?\" The answer provided is \"The client is Acme.\" To assess the faithfulness of the answer to the document: 1. The document directly states that the client is \"Acme\". 2. The answer directly corresponds to this information by stating \"The client is Acme\". 3. There is no additional information introduced in the answer that is not present in the document. 4. The answer does not contradict any information provided in the document. Given these observations, the answer accurately reflects the information contained within the document. Therefore, the verdict is: {\"REASONING\": \"The answer directly corresponds to the information provided in the document without introducing new information or contradicting existing information.\", \"HALLUCINATION\": 0} Tokens Used: {'completion_tokens': 246, 'prompt_tokens': 265, 'total_tokens': 511} ================================================================================ Extended context ¬∂ A very common use case is document extraction. Let's see how a lengthy invoice used as context helps us to check if our model is producing reliable output. In [11]: Copied! # Invoice Example invoice = ''' { \"invoiceId\": \"INV-20250523-XG-74920B\", \"orderReference\": \"ORD-PROC-Q2-2025-ALPHA-99374-DELTA\", \"customerIdentification\": \"CUST-EAGLECORP-GLOBAL-007\", \"dateIssued\": \"2025-05-23\", \"dueDate\": \"2025-06-22\", \"paymentTerms\": \"Net 30 Days\", \"currency\": \"USD\", \"issuerDetails\": { \"companyName\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd.\", \"taxId\": \"VAT-GB-293847261\", \"registrationNumber\": \"REG-LND-09876543X\", \"address\": { \"street\": \"121B, Innovation Drive, Silicon Roundabout, Tech City East\", \"city\": \"London\", \"postalCode\": \"EC1Y 8XZ\", \"country\": \"United Kingdom\", \"planet\": \"Earth\", \"dimension\": \"Sigma-7\" }, \"contact\": { \"primaryPhone\": \"+44-20-7946-0001 ext. 777\", \"secondaryPhone\": \"+44-20-7946-0002\", \"fax\": \"+44-20-7946-0003\", \"email\": \"billing@quantumsynergistics-ans.co.uk\", \"website\": \"www.quantumsynergistics-ans.co.uk\" }, \"bankDetails\": { \"bankName\": \"Universal Interstellar Bank PLC\", \"accountName\": \"Quantum Synergistics & ANS Ltd.\", \"accountNumber\": \"9876543210123456\", \"swiftBic\": \"UNIVGB2LXXX\", \"iban\": \"GB29 UNIV 9876 5432 1012 3456 78\", \"reference\": \"INV-20250523-XG-74920B\" } }, \"billingInformation\": { \"companyName\": \"EagleCorp Global Holdings Inc. & Subsidiaries\", \"department\": \"Strategic Procurement & Interstellar Logistics Division\", \"attentionTo\": \"Ms. Evelyn Reed, Chief Procurement Officer (CPO)\", \"taxId\": \"EIN-US-98-7654321X\", \"clientReferenceId\": \"EGL-PROC-REF-Q2-2025-7734-GAMMA\", \"address\": { \"street\": \"Suite 9870, Eagle Tower One, 1500 Constitution Avenue NW\", \"city\": \"Washington D.C.\", \"state\": \"District of Columbia\", \"postalCode\": \"20001-1500\", \"country\": \"United States of America\" }, \"contact\": { \"phone\": \"+1-202-555-0189 ext. 1234\", \"email\": \"e.reed.procurement@eaglecorpglobal.com\" } }, \"shippingInformation\": [ { \"shipmentId\": \"SHIP-ALPHA-001-XG74920B\", \"recipientName\": \"Dr. Aris Thorne, Head of R&D\", \"facilityName\": \"EagleCorp Advanced Research Facility - Sector Gamma-7\", \"address\": { \"street\": \"Docking Bay 7, 47 Industrial Park Road\", \"city\": \"New Chicago\", \"state\": \"Illinois\", \"postalCode\": \"60699-0047\", \"country\": \"United States of America\", \"deliveryZone\": \"Restricted Access - Level 3 Clearance Required\" }, \"shippingMethod\": \"Cryo-Stasis Freight - Priority Overnight\", \"trackingNumber\": \"TRK-CSFPON-9988776655-A01\", \"notes\": \"Deliver between 08:00 - 10:00 Local Time. Handle with Extreme Care. Temperature Sensitive Materials.\" }, { \"shipmentId\": \"SHIP-BETA-002-XG74920B\", \"recipientName\": \"Mr. Jian Li, Operations Manager\", \"facilityName\": \"EagleCorp Manufacturing Plant - Unit 42\", \"address\": { \"street\": \"88 Manufacturing Drive, Innovation Valley Industrial Estate\", \"city\": \"Shenzhen\", \"province\": \"Guangdong\", \"postalCode\": \"518000\", \"country\": \"China\", \"deliveryZone\": \"Loading Dock B - Heavy Goods\" }, \"shippingMethod\": \"Secure Air Cargo - Expedited\", \"trackingNumber\": \"TRK-SACEXP-CN7766554433-B02\", \"notes\": \"Requires Forklift. Confirm delivery appointment 24hrs prior.\" } ], \"lineItems\": [ { \"itemId\": \"QN-CORE-X9000-PRO\", \"productCode\": \"PQC-SYS-001A-REV4\", \"description\": \"Quantum Entanglement Core Processor - Model X9000 Professional Edition. Includes integrated cryo-cooler and temporal displacement shielding. Firmware v7.8.2-alpha.\", \"servicePeriod\": \"N/A\", \"quantity\": 2, \"unit\": \"Unit(s)\", \"unitPrice\": 750000.00, \"discountPercentage\": 5.0, \"discountAmount\": 75000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 285000.00, \"subtotal\": 1425000.00, \"totalLineAmount\": 1710000.00, \"serialNumbers\": [\"SN-QECX9P-0000A1F8\", \"SN-QECX9P-0000A2C4\"], \"warrantyId\": \"WARR-QECX9P-5YR-PREM-001\" }, { \"itemId\": \"NANO-FAB-M7-ULTRA\", \"productCode\": \"NFM-DEV-007B-REV2\", \"description\": \"Advanced Nanite Fabricator - Model M7 Ultra. High precision, multi-material capability. Includes 12-month software subscription (Tier 1).\", \"servicePeriod\": \"N/A\", \"quantity\": 1, \"unit\": \"System\", \"unitPrice\": 1250000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 250000.00, \"subtotal\": 1250000.00, \"totalLineAmount\": 1500000.00, \"serialNumbers\": [\"SN-NFM7U-XYZ001B\"], \"warrantyId\": \"WARR-NFM7U-3YR-STD-002\" }, { \"itemId\": \"SVC-CONSULT-QIP-PH1\", \"productCode\": \"CS-QIP-001-PHASE1\", \"description\": \"Quantum Implementation Project - Phase 1 Consultation Services. On-site engineering support, system integration planning, and initial staff training (400 hours block).\", \"servicePeriod\": \"2025-06-01 to 2025-08-31\", \"quantity\": 400, \"unit\": \"Hour(s)\", \"unitPrice\": 850.00, \"discountPercentage\": 10.0, \"discountAmount\": 34000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 306000.00, \"totalLineAmount\": 306000.00, \"projectCode\": \"PROJ-EAGLE-QIP-2025\", \"consultantId\": [\"CONS-DR-EVA-ROSTOVA\", \"CONS-RAJ-SINGH-ENG\"] }, { \"itemId\": \"MAT-CRYOFLUID-XF100\", \"productCode\": \"CHEM-CRYO-003C\", \"description\": \"Cryogenic Cooling Fluid - Type XF-100. Ultra-low temperature stability. Non-conductive. (Sold in 200L insulated containers)\", \"servicePeriod\": \"N/A\", \"quantity\": 10, \"unit\": \"Container(s)\", \"unitPrice\": 15000.00, \"discountPercentage\": 2.5, \"discountAmount\": 3750.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 29250.00, \"subtotal\": 146250.00, \"totalLineAmount\": 175500.00, \"batchNumbers\": [\"BATCH-XF100-2501A01\", \"BATCH-XF100-2501A02\", \"BATCH-XF100-2501A03\", \"BATCH-XF100-2501A04\", \"BATCH-XF100-2501A05\", \"BATCH-XF100-2501A06\", \"BATCH-XF100-2501A07\", \"BATCH-XF100-2501A08\", \"BATCH-XF100-2501A09\", \"BATCH-XF100-2501A10\"], \"shelfLife\": \"24 Months from DOM\" }, { \"itemId\": \"SOFT-LICENSE-QAI-ENT\", \"productCode\": \"SL-QAI-ENT-001-5YR\", \"description\": \"Quantum AI Algorithmic Suite - Enterprise License. 5-Year Subscription. Unlimited User Access. Includes Premium Support Package (PSP-GOLD-001).\", \"servicePeriod\": \"2025-06-01 to 2030-05-31\", \"quantity\": 1, \"unit\": \"License\", \"unitPrice\": 450000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 450000.00, \"totalLineAmount\": 450000.00, \"licenseKey\": \"LIC-QAIENT-XG74920B-ABC123XYZ789-EAGLECORP\", \"supportContractId\": \"SUP-PSP-GOLD-001-XG74920B\" }, { \"itemId\": \"COMP-SENSOR-ARRAY-SIGMA\", \"productCode\": \"SNS-ARR-SGM-004D\", \"description\": \"Multi-Dimensional Sensor Array - Sigma Series. High-sensitivity, wide spectrum coverage. Includes calibration certificate traceable to NIST/NPL.\", \"servicePeriod\": \"N/A\", \"quantity\": 8, \"unit\": \"Unit(s)\", \"unitPrice\": 22000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 35200.00, \"subtotal\": 176000.00, \"totalLineAmount\": 211200.00, \"serialNumbers\": [\"SN-MDSA-SGM-0101\", \"SN-MDSA-SGM-0102\", \"SN-MDSA-SGM-0103\", \"SN-MDSA-SGM-0104\", \"SN-MDSA-SGM-0105\", \"SN-MDSA-SGM-0106\", \"SN-MDSA-SGM-0107\", \"SN-MDSA-SGM-0108\"], \"calibrationDate\": \"2025-05-15\" }, { \"itemId\": \"MAINT-KIT-ADV-ROBOTICS\", \"productCode\": \"MNT-KIT-ROBO-002A\", \"description\": \"Advanced Robotics Maintenance Toolkit. Includes specialized diagnostic tools and Class-5 cleanroom consumables. For AR-700 and AR-800 series.\", \"servicePeriod\": \"N/A\", \"quantity\": 5, \"unit\": \"Kit(s)\", \"unitPrice\": 7500.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 7500.00, \"subtotal\": 37500.00, \"totalLineAmount\": 45000.00, \"componentListId\": \"CL-MNTROBO-002A-V3\" }, { \"itemId\": \"DATA-STORAGE-CRYSTAL-1PB\", \"productCode\": \"DSC-1PB-HG-009\", \"description\": \"Holographic Data Storage Crystal - 1 Petabyte Capacity. Archival Grade. Read/Write Speed: 50 GB/s. Phase-change matrix type.\", \"servicePeriod\": \"N/A\", \"quantity\": 20, \"unit\": \"Crystal(s)\", \"unitPrice\": 18000.00, \"discountPercentage\": 10.0, \"discountAmount\": 36000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 64800.00, \"subtotal\": 324000.00, \"totalLineAmount\": 388800.00, \"serialNumbers\": [\"SN-DSC1PB-HG-A001F to SN-DSC1PB-HG-A001P\", \"SN-DSC1PB-HG-B002A to SN-DSC1PB-HG-B002D\"], \"dataIntegrityCert\": \"DIC-HG9-20250520-BATCH01\" } ], \"summary\": { \"subtotalBeforeDiscounts\": 4128500.00, \"totalDiscountAmount\": 148750.00, \"subtotalAfterDiscounts\": 3979750.00, \"totalTaxAmount\": 671750.00, \"shippingAndHandling\": [ { \"description\": \"Cryo-Stasis Freight - Priority Overnight (SHIP-ALPHA-001)\", \"chargeCode\": \"SHP-CRYO-PRIO-INTL\", \"amount\": 12500.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Secure Air Cargo - Expedited (SHIP-BETA-002)\", \"chargeCode\": \"SHP-SAC-EXP-CN\", \"amount\": 8800.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Special Handling - Temperature Sensitive & High Value Goods\", \"chargeCode\": \"HDL-SPECREQ-HVTS\", \"amount\": 5500.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 1100.00 }, { \"description\": \"Customs Clearance & Documentation Fee - International\", \"chargeCode\": \"FEE-CUSTOMS-INTL-001\", \"amount\": 2750.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Transit Insurance - Full Value Coverage\", \"chargeCode\": \"INS-TRANSIT-FULL-XG74920B\", \"amount\": 25000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 } ], \"totalShippingAndHandling\": 54550.00, \"totalShippingAndHandlingTax\": 1100.00, \"grandTotal\": 4707150.00, \"amountPaid\": 0.00, \"amountDue\": 4707150.00 }, \"paymentInstructions\": { \"preferredMethod\": \"Wire Transfer\", \"paymentReference\": \"INV-20250523-XG-74920B / CUST-EAGLECORP-GLOBAL-007\", \"latePaymentPenalty\": \"1.5% per month on outstanding balance after due date.\", \"earlyPaymentDiscount\": \"1 % d iscount if paid within 10 days (Amount: $47071.50, New Total: $4660078.50). Reference EPD-XG74920B if claiming.\", \"alternativePayments\": [ { \"method\": \"Secured Crypto Transfer (USDC or ETH)\", \"details\": \"Wallet Address: 0x1234ABCD5678EFGH9012IJKL3456MNOP7890QRST. Memo: XG74920B. Confirmation required via secure_payments@quantumsynergistics-ans.co.uk\" }, { \"method\": \"Irrevocable Letter of Credit (ILOC)\", \"details\": \"To be issued by a Prime Bank, acceptable to Quantum Synergistics. Contact accounts_receivable@quantumsynergistics-ans.co.uk for ILOC requirements.\" } ] }, \"notesAndRemarks\": [ \"All hardware components are subject to export control regulations (EAR/ITAR where applicable). Compliance documentation attached separately (DOC-REF: EXPCOMPL-XG74920B).\", \"Software licenses are non-transferable and subject to the End User License Agreement (EULA-QSANS-V4.2).\", \"On-site consultation hours are estimates. Additional hours will be billed separately under addendum A1 of contract CS-QIP-001.\", \"Warranty claims must be submitted via the online portal at support.quantumsynergistics-ans.co.uk using the provided Warranty IDs.\", \"Return Material Authorization (RMA) required for all returns. Contact customer support for RMA number. Restocking fees may apply (15-25% based on product type and condition). See detailed Return Policy (POL-RET-QSANS-2025-V2).\", \"Projected delivery dates for back-ordered sub-components (Ref: SUBCOMP-BO-LIST-XG74920B-01) will be communicated by your account manager within 7 business days.\" ], \"attachments\": [ {\"documentName\": \"QSANS_Product_Specification_Sheets_Q2_2025.pdf\", \"fileId\": \"DOC-SPECS-QSANS-Q22025-V1.3\"}, {\"documentName\": \"EULA_QSANS_Software_V4.2.pdf\", \"fileId\": \"DOC-EULA-QSANS-V4.2\"}, {\"documentName\": \"Warranty_Terms_and_Conditions_Premium_Standard.pdf\", \"fileId\": \"DOC-WARR-QSANS-PREMSTD-V3.1\"}, {\"documentName\": \"Export_Compliance_Declaration_XG74920B.pdf\", \"fileId\": \"DOC-EXPCOMPL-XG74920B\"}, {\"documentName\": \"Return_Policy_QSANS_2025_V2.pdf\", \"fileId\": \"DOC-POL-RET-QSANS-2025-V2\"}, {\"documentName\": \"Consultation_Services_SOW_PROJ-EAGLE-QIP-2025.pdf\", \"fileId\": \"DOC-SOW-EAGLE-QIP-2025-PH1\"} ], \"approvalWorkflow\": { \"issuerApproval\": { \"approverName\": \"Mr. Alistair Finch\", \"approverTitle\": \"Head of Commercial Operations\", \"approvalDate\": \"2025-05-23\", \"signatureId\": \"SIG-AFINCH-QSANS-20250523-001A\" }, \"clientAcknowledgmentRequired\": true, \"clientAcknowledgmentInstructions\": \"Please sign and return a copy of this invoice or confirm receipt and acceptance via email to billing@quantumsynergistics-ans.co.uk within 5 business days.\" }, \"versionHistory\": [ {\"version\": 1.0, \"date\": \"2025-05-23\", \"reason\": \"Initial Draft\", \"editorId\": \"SYS-AUTOINV-GEN\"}, {\"version\": 1.1, \"date\": \"2025-05-23\", \"reason\": \"Added shipping details and corrected tax calculation for item QN-CORE-X9000-PRO.\", \"editorId\": \"USER-CFO-REVIEW-BOT\"} ], \"footerMessage\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd. - Pioneering the Future, Today. Thank you for your business. For support, please visit our dedicated portal or contact your account representative. All transactions are governed by the laws of England and Wales. Registered Office: 121B, Innovation Drive, London, EC1Y 8XZ, UK. Company Reg No: REG-LND-09876543X. VAT No: VAT-GB-293847261.\" } ''' # Invoice Example invoice=''' { \"invoiceId\": \"INV-20250523-XG-74920B\", \"orderReference\": \"ORD-PROC-Q2-2025-ALPHA-99374-DELTA\", \"customerIdentification\": \"CUST-EAGLECORP-GLOBAL-007\", \"dateIssued\": \"2025-05-23\", \"dueDate\": \"2025-06-22\", \"paymentTerms\": \"Net 30 Days\", \"currency\": \"USD\", \"issuerDetails\": { \"companyName\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd.\", \"taxId\": \"VAT-GB-293847261\", \"registrationNumber\": \"REG-LND-09876543X\", \"address\": { \"street\": \"121B, Innovation Drive, Silicon Roundabout, Tech City East\", \"city\": \"London\", \"postalCode\": \"EC1Y 8XZ\", \"country\": \"United Kingdom\", \"planet\": \"Earth\", \"dimension\": \"Sigma-7\" }, \"contact\": { \"primaryPhone\": \"+44-20-7946-0001 ext. 777\", \"secondaryPhone\": \"+44-20-7946-0002\", \"fax\": \"+44-20-7946-0003\", \"email\": \"billing@quantumsynergistics-ans.co.uk\", \"website\": \"www.quantumsynergistics-ans.co.uk\" }, \"bankDetails\": { \"bankName\": \"Universal Interstellar Bank PLC\", \"accountName\": \"Quantum Synergistics & ANS Ltd.\", \"accountNumber\": \"9876543210123456\", \"swiftBic\": \"UNIVGB2LXXX\", \"iban\": \"GB29 UNIV 9876 5432 1012 3456 78\", \"reference\": \"INV-20250523-XG-74920B\" } }, \"billingInformation\": { \"companyName\": \"EagleCorp Global Holdings Inc. & Subsidiaries\", \"department\": \"Strategic Procurement & Interstellar Logistics Division\", \"attentionTo\": \"Ms. Evelyn Reed, Chief Procurement Officer (CPO)\", \"taxId\": \"EIN-US-98-7654321X\", \"clientReferenceId\": \"EGL-PROC-REF-Q2-2025-7734-GAMMA\", \"address\": { \"street\": \"Suite 9870, Eagle Tower One, 1500 Constitution Avenue NW\", \"city\": \"Washington D.C.\", \"state\": \"District of Columbia\", \"postalCode\": \"20001-1500\", \"country\": \"United States of America\" }, \"contact\": { \"phone\": \"+1-202-555-0189 ext. 1234\", \"email\": \"e.reed.procurement@eaglecorpglobal.com\" } }, \"shippingInformation\": [ { \"shipmentId\": \"SHIP-ALPHA-001-XG74920B\", \"recipientName\": \"Dr. Aris Thorne, Head of R&D\", \"facilityName\": \"EagleCorp Advanced Research Facility - Sector Gamma-7\", \"address\": { \"street\": \"Docking Bay 7, 47 Industrial Park Road\", \"city\": \"New Chicago\", \"state\": \"Illinois\", \"postalCode\": \"60699-0047\", \"country\": \"United States of America\", \"deliveryZone\": \"Restricted Access - Level 3 Clearance Required\" }, \"shippingMethod\": \"Cryo-Stasis Freight - Priority Overnight\", \"trackingNumber\": \"TRK-CSFPON-9988776655-A01\", \"notes\": \"Deliver between 08:00 - 10:00 Local Time. Handle with Extreme Care. Temperature Sensitive Materials.\" }, { \"shipmentId\": \"SHIP-BETA-002-XG74920B\", \"recipientName\": \"Mr. Jian Li, Operations Manager\", \"facilityName\": \"EagleCorp Manufacturing Plant - Unit 42\", \"address\": { \"street\": \"88 Manufacturing Drive, Innovation Valley Industrial Estate\", \"city\": \"Shenzhen\", \"province\": \"Guangdong\", \"postalCode\": \"518000\", \"country\": \"China\", \"deliveryZone\": \"Loading Dock B - Heavy Goods\" }, \"shippingMethod\": \"Secure Air Cargo - Expedited\", \"trackingNumber\": \"TRK-SACEXP-CN7766554433-B02\", \"notes\": \"Requires Forklift. Confirm delivery appointment 24hrs prior.\" } ], \"lineItems\": [ { \"itemId\": \"QN-CORE-X9000-PRO\", \"productCode\": \"PQC-SYS-001A-REV4\", \"description\": \"Quantum Entanglement Core Processor - Model X9000 Professional Edition. Includes integrated cryo-cooler and temporal displacement shielding. Firmware v7.8.2-alpha.\", \"servicePeriod\": \"N/A\", \"quantity\": 2, \"unit\": \"Unit(s)\", \"unitPrice\": 750000.00, \"discountPercentage\": 5.0, \"discountAmount\": 75000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 285000.00, \"subtotal\": 1425000.00, \"totalLineAmount\": 1710000.00, \"serialNumbers\": [\"SN-QECX9P-0000A1F8\", \"SN-QECX9P-0000A2C4\"], \"warrantyId\": \"WARR-QECX9P-5YR-PREM-001\" }, { \"itemId\": \"NANO-FAB-M7-ULTRA\", \"productCode\": \"NFM-DEV-007B-REV2\", \"description\": \"Advanced Nanite Fabricator - Model M7 Ultra. High precision, multi-material capability. Includes 12-month software subscription (Tier 1).\", \"servicePeriod\": \"N/A\", \"quantity\": 1, \"unit\": \"System\", \"unitPrice\": 1250000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 250000.00, \"subtotal\": 1250000.00, \"totalLineAmount\": 1500000.00, \"serialNumbers\": [\"SN-NFM7U-XYZ001B\"], \"warrantyId\": \"WARR-NFM7U-3YR-STD-002\" }, { \"itemId\": \"SVC-CONSULT-QIP-PH1\", \"productCode\": \"CS-QIP-001-PHASE1\", \"description\": \"Quantum Implementation Project - Phase 1 Consultation Services. On-site engineering support, system integration planning, and initial staff training (400 hours block).\", \"servicePeriod\": \"2025-06-01 to 2025-08-31\", \"quantity\": 400, \"unit\": \"Hour(s)\", \"unitPrice\": 850.00, \"discountPercentage\": 10.0, \"discountAmount\": 34000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 306000.00, \"totalLineAmount\": 306000.00, \"projectCode\": \"PROJ-EAGLE-QIP-2025\", \"consultantId\": [\"CONS-DR-EVA-ROSTOVA\", \"CONS-RAJ-SINGH-ENG\"] }, { \"itemId\": \"MAT-CRYOFLUID-XF100\", \"productCode\": \"CHEM-CRYO-003C\", \"description\": \"Cryogenic Cooling Fluid - Type XF-100. Ultra-low temperature stability. Non-conductive. (Sold in 200L insulated containers)\", \"servicePeriod\": \"N/A\", \"quantity\": 10, \"unit\": \"Container(s)\", \"unitPrice\": 15000.00, \"discountPercentage\": 2.5, \"discountAmount\": 3750.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 29250.00, \"subtotal\": 146250.00, \"totalLineAmount\": 175500.00, \"batchNumbers\": [\"BATCH-XF100-2501A01\", \"BATCH-XF100-2501A02\", \"BATCH-XF100-2501A03\", \"BATCH-XF100-2501A04\", \"BATCH-XF100-2501A05\", \"BATCH-XF100-2501A06\", \"BATCH-XF100-2501A07\", \"BATCH-XF100-2501A08\", \"BATCH-XF100-2501A09\", \"BATCH-XF100-2501A10\"], \"shelfLife\": \"24 Months from DOM\" }, { \"itemId\": \"SOFT-LICENSE-QAI-ENT\", \"productCode\": \"SL-QAI-ENT-001-5YR\", \"description\": \"Quantum AI Algorithmic Suite - Enterprise License. 5-Year Subscription. Unlimited User Access. Includes Premium Support Package (PSP-GOLD-001).\", \"servicePeriod\": \"2025-06-01 to 2030-05-31\", \"quantity\": 1, \"unit\": \"License\", \"unitPrice\": 450000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 450000.00, \"totalLineAmount\": 450000.00, \"licenseKey\": \"LIC-QAIENT-XG74920B-ABC123XYZ789-EAGLECORP\", \"supportContractId\": \"SUP-PSP-GOLD-001-XG74920B\" }, { \"itemId\": \"COMP-SENSOR-ARRAY-SIGMA\", \"productCode\": \"SNS-ARR-SGM-004D\", \"description\": \"Multi-Dimensional Sensor Array - Sigma Series. High-sensitivity, wide spectrum coverage. Includes calibration certificate traceable to NIST/NPL.\", \"servicePeriod\": \"N/A\", \"quantity\": 8, \"unit\": \"Unit(s)\", \"unitPrice\": 22000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 35200.00, \"subtotal\": 176000.00, \"totalLineAmount\": 211200.00, \"serialNumbers\": [\"SN-MDSA-SGM-0101\", \"SN-MDSA-SGM-0102\", \"SN-MDSA-SGM-0103\", \"SN-MDSA-SGM-0104\", \"SN-MDSA-SGM-0105\", \"SN-MDSA-SGM-0106\", \"SN-MDSA-SGM-0107\", \"SN-MDSA-SGM-0108\"], \"calibrationDate\": \"2025-05-15\" }, { \"itemId\": \"MAINT-KIT-ADV-ROBOTICS\", \"productCode\": \"MNT-KIT-ROBO-002A\", \"description\": \"Advanced Robotics Maintenance Toolkit. Includes specialized diagnostic tools and Class-5 cleanroom consumables. For AR-700 and AR-800 series.\", \"servicePeriod\": \"N/A\", \"quantity\": 5, \"unit\": \"Kit(s)\", \"unitPrice\": 7500.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 7500.00, \"subtotal\": 37500.00, \"totalLineAmount\": 45000.00, \"componentListId\": \"CL-MNTROBO-002A-V3\" }, { \"itemId\": \"DATA-STORAGE-CRYSTAL-1PB\", \"productCode\": \"DSC-1PB-HG-009\", \"description\": \"Holographic Data Storage Crystal - 1 Petabyte Capacity. Archival Grade. Read/Write Speed: 50 GB/s. Phase-change matrix type.\", \"servicePeriod\": \"N/A\", \"quantity\": 20, \"unit\": \"Crystal(s)\", \"unitPrice\": 18000.00, \"discountPercentage\": 10.0, \"discountAmount\": 36000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 64800.00, \"subtotal\": 324000.00, \"totalLineAmount\": 388800.00, \"serialNumbers\": [\"SN-DSC1PB-HG-A001F to SN-DSC1PB-HG-A001P\", \"SN-DSC1PB-HG-B002A to SN-DSC1PB-HG-B002D\"], \"dataIntegrityCert\": \"DIC-HG9-20250520-BATCH01\" } ], \"summary\": { \"subtotalBeforeDiscounts\": 4128500.00, \"totalDiscountAmount\": 148750.00, \"subtotalAfterDiscounts\": 3979750.00, \"totalTaxAmount\": 671750.00, \"shippingAndHandling\": [ { \"description\": \"Cryo-Stasis Freight - Priority Overnight (SHIP-ALPHA-001)\", \"chargeCode\": \"SHP-CRYO-PRIO-INTL\", \"amount\": 12500.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Secure Air Cargo - Expedited (SHIP-BETA-002)\", \"chargeCode\": \"SHP-SAC-EXP-CN\", \"amount\": 8800.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Special Handling - Temperature Sensitive & High Value Goods\", \"chargeCode\": \"HDL-SPECREQ-HVTS\", \"amount\": 5500.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 1100.00 }, { \"description\": \"Customs Clearance & Documentation Fee - International\", \"chargeCode\": \"FEE-CUSTOMS-INTL-001\", \"amount\": 2750.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Transit Insurance - Full Value Coverage\", \"chargeCode\": \"INS-TRANSIT-FULL-XG74920B\", \"amount\": 25000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 } ], \"totalShippingAndHandling\": 54550.00, \"totalShippingAndHandlingTax\": 1100.00, \"grandTotal\": 4707150.00, \"amountPaid\": 0.00, \"amountDue\": 4707150.00 }, \"paymentInstructions\": { \"preferredMethod\": \"Wire Transfer\", \"paymentReference\": \"INV-20250523-XG-74920B / CUST-EAGLECORP-GLOBAL-007\", \"latePaymentPenalty\": \"1.5% per month on outstanding balance after due date.\", \"earlyPaymentDiscount\": \"1% discount if paid within 10 days (Amount: $47071.50, New Total: $4660078.50). Reference EPD-XG74920B if claiming.\", \"alternativePayments\": [ { \"method\": \"Secured Crypto Transfer (USDC or ETH)\", \"details\": \"Wallet Address: 0x1234ABCD5678EFGH9012IJKL3456MNOP7890QRST. Memo: XG74920B. Confirmation required via secure_payments@quantumsynergistics-ans.co.uk\" }, { \"method\": \"Irrevocable Letter of Credit (ILOC)\", \"details\": \"To be issued by a Prime Bank, acceptable to Quantum Synergistics. Contact accounts_receivable@quantumsynergistics-ans.co.uk for ILOC requirements.\" } ] }, \"notesAndRemarks\": [ \"All hardware components are subject to export control regulations (EAR/ITAR where applicable). Compliance documentation attached separately (DOC-REF: EXPCOMPL-XG74920B).\", \"Software licenses are non-transferable and subject to the End User License Agreement (EULA-QSANS-V4.2).\", \"On-site consultation hours are estimates. Additional hours will be billed separately under addendum A1 of contract CS-QIP-001.\", \"Warranty claims must be submitted via the online portal at support.quantumsynergistics-ans.co.uk using the provided Warranty IDs.\", \"Return Material Authorization (RMA) required for all returns. Contact customer support for RMA number. Restocking fees may apply (15-25% based on product type and condition). See detailed Return Policy (POL-RET-QSANS-2025-V2).\", \"Projected delivery dates for back-ordered sub-components (Ref: SUBCOMP-BO-LIST-XG74920B-01) will be communicated by your account manager within 7 business days.\" ], \"attachments\": [ {\"documentName\": \"QSANS_Product_Specification_Sheets_Q2_2025.pdf\", \"fileId\": \"DOC-SPECS-QSANS-Q22025-V1.3\"}, {\"documentName\": \"EULA_QSANS_Software_V4.2.pdf\", \"fileId\": \"DOC-EULA-QSANS-V4.2\"}, {\"documentName\": \"Warranty_Terms_and_Conditions_Premium_Standard.pdf\", \"fileId\": \"DOC-WARR-QSANS-PREMSTD-V3.1\"}, {\"documentName\": \"Export_Compliance_Declaration_XG74920B.pdf\", \"fileId\": \"DOC-EXPCOMPL-XG74920B\"}, {\"documentName\": \"Return_Policy_QSANS_2025_V2.pdf\", \"fileId\": \"DOC-POL-RET-QSANS-2025-V2\"}, {\"documentName\": \"Consultation_Services_SOW_PROJ-EAGLE-QIP-2025.pdf\", \"fileId\": \"DOC-SOW-EAGLE-QIP-2025-PH1\"} ], \"approvalWorkflow\": { \"issuerApproval\": { \"approverName\": \"Mr. Alistair Finch\", \"approverTitle\": \"Head of Commercial Operations\", \"approvalDate\": \"2025-05-23\", \"signatureId\": \"SIG-AFINCH-QSANS-20250523-001A\" }, \"clientAcknowledgmentRequired\": true, \"clientAcknowledgmentInstructions\": \"Please sign and return a copy of this invoice or confirm receipt and acceptance via email to billing@quantumsynergistics-ans.co.uk within 5 business days.\" }, \"versionHistory\": [ {\"version\": 1.0, \"date\": \"2025-05-23\", \"reason\": \"Initial Draft\", \"editorId\": \"SYS-AUTOINV-GEN\"}, {\"version\": 1.1, \"date\": \"2025-05-23\", \"reason\": \"Added shipping details and corrected tax calculation for item QN-CORE-X9000-PRO.\", \"editorId\": \"USER-CFO-REVIEW-BOT\"} ], \"footerMessage\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd. - Pioneering the Future, Today. Thank you for your business. For support, please visit our dedicated portal or contact your account representative. All transactions are governed by the laws of England and Wales. Registered Office: 121B, Innovation Drive, London, EC1Y 8XZ, UK. Company Reg No: REG-LND-09876543X. VAT No: VAT-GB-293847261.\" } ''' With the context above, let's create two examples, one where the answer from the model is the correct ID and the other is missing just one character. In [12]: Copied! # Long context examples to test reliability check in invoice processing invoice_examples = [ { \"prompt\" : \"What is the Secure Air Cargo code?\" , \"output\" : \"The Secure Air Cargo code is SHP-SAC-EXP-CN.\" , \"context\" : invoice , \"expected_unreliable\" : False }, { \"prompt\" : \"What is the Secure Air Cargo code?\" , \"output\" : \"The Secure Air Cargo code is HP-SAC-EXP-CN.\" , \"context\" : invoice , \"expected_unreliable\" : True } ] # Long context examples to test reliability check in invoice processing invoice_examples = [ { \"prompt\": \"What is the Secure Air Cargo code?\", \"output\": \"The Secure Air Cargo code is SHP-SAC-EXP-CN.\", \"context\": invoice, \"expected_unreliable\": False }, { \"prompt\": \"What is the Secure Air Cargo code?\", \"output\": \"The Secure Air Cargo code is HP-SAC-EXP-CN.\", \"context\": invoice, \"expected_unreliable\": True } ] Now, by comparing these two answers, we can test the Verify reliability check response: In [15]: Copied! # Test long context validation with invoice print ( \"=== Long Context - Invoice Processing Examples === \\n \" ) print ( f \"Question: { invoice_examples [ 0 ][ 'prompt' ] } \\n \" ) # Run the first example for i , example in enumerate ( invoice_examples ): # Print the question and expected answer type answer_type = \"Correct Answer\" if not example [ 'expected_unreliable' ] else \"Wrong Answer\" print ( f \"#Run { i + 1 } - { answer_type } \" ) # Run the reliability check on the invoice example result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], context = example [ 'context' ], return_search_results = False ) # Print the results print ( f \"a- Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) print ( f \"b- Expected value: { example [ 'expected_unreliable' ] } \" ) explanation = result . get ( 'explanation' , 'N/A' ) # Limit explanation to max 400 characters max_chars = 400 short_explanation = explanation [: max_chars ] + \"...\" if len ( explanation ) > max_chars else explanation print ( f \"c- Short summary of explanation: { short_explanation } \" ) print ( \"--\" ) print () # Test long context validation with invoice print(\"=== Long Context - Invoice Processing Examples ===\\n\") print(f\"Question: {invoice_examples[0]['prompt']}\\n\") # Run the first example for i, example in enumerate(invoice_examples): # Print the question and expected answer type answer_type = \"Correct Answer\" if not example['expected_unreliable'] else \"Wrong Answer\" print(f\"#Run {i+1} - {answer_type}\") # Run the reliability check on the invoice example result = check_reliability_qa( prompt=example['prompt'], output=example['output'], context=example['context'], return_search_results=False ) # Print the results print(f\"a- Is Unreliable: {result.get('is_hallucination', 'N/A')}\") print(f\"b- Expected value: {example['expected_unreliable']}\") explanation = result.get('explanation', 'N/A') # Limit explanation to max 400 characters max_chars = 400 short_explanation = explanation[:max_chars] + \"...\" if len(explanation) > max_chars else explanation print(f\"c- Short summary of explanation: {short_explanation}\") print(\"--\") print() === Long Context - Invoice Processing Examples === Question: What is the Secure Air Cargo code? #Run 1 - Correct Answer a- Is Unreliable: False b- Expected value: False c- Short summary of explanation: The DOCUMENT contains a JSON object representing an invoice with various details including shipping information. Under the 'shippingAndHandling' section within 'summary', there is a list of charges, one of which is described as 'Secure Air Cargo - Expedited (SHIP-BETA-002)' with the charge code 'SHP-SAC-EXP-CN'. This matches the information given in the ANSWER. -- #Run 2 - Wrong Answer a- Is Unreliable: True b- Expected value: True c- Short summary of explanation: To determine whether the provided answer is faithful to the contents of the DOCUMENT, we need to examine the information given in the DOCUMENT and compare it with the ANSWER. The QUESTION asks for the Secure Air Cargo code. Upon reviewing the DOCUMENT, we find that it contains detailed information about an invoice, including shipping information for various items. Specifically, under \"shippingInf... -- OpenAI chat completion endpoint ¬∂ The reliability check conducted via the OpenAI chat completion endpoint method validates multi-turn conversations for reliability issues. This is ideal for conversational AI systems and chatbots. Prepare the data ¬∂ For this scenario, we need to provide the promp via the user role, and the answer from the LLM via the assistant role. We set expected_unreliable as the ground truth for comparison. In [18]: Copied! # Chat conversation examples for Chat Completion checks chat_examples = [ { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant that provides accurate medical information.\" }, { \"role\" : \"user\" , \"content\" : \"Does vitamin C cure the common cold?\" }, { \"role\" : \"assistant\" , \"content\" : \"Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours.\" } ], \"expected_unreliable\" : True }, { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a knowledgeable financial advisor.\" }, { \"role\" : \"user\" , \"content\" : \"What is compound interest?\" }, { \"role\" : \"assistant\" , \"content\" : \"Compound interest is when you earn interest on both your original investment and the interest that has already been earned. It causes your money to grow exponentially over time.\" } ], \"expected_unreliable\" : False } ] # Chat conversation examples for Chat Completion checks chat_examples = [ { \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate medical information.\"}, {\"role\": \"user\", \"content\": \"Does vitamin C cure the common cold?\"}, {\"role\": \"assistant\", \"content\": \"Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours.\"} ], \"expected_unreliable\": True }, { \"messages\": [ {\"role\": \"system\", \"content\": \"You are a knowledgeable financial advisor.\"}, {\"role\": \"user\", \"content\": \"What is compound interest?\"}, {\"role\": \"assistant\", \"content\": \"Compound interest is when you earn interest on both your original investment and the interest that has already been earned. It causes your money to grow exponentially over time.\"} ], \"expected_unreliable\": False } ] Reliability check via the OpenAI client ¬∂ In [19]: Copied! # Function for Chat Completion reliability check def check_reliability_chat ( messages ): \"\"\"Check reliability in chat conversations using OpenAI library\"\"\" # Create a separate client for chat completions with the correct base URL client = OpenAI ( base_url = base_url , api_key = api_key , ) # Make the request using OpenAI client - pass parameters directly response = client . chat . completions . create ( model = \"klusterai/verify-reliability\" , #Reliability model messages = messages ) # Parse the response - kluster.ai returns check results in a specific format return response . choices [ 0 ] . message . content # Function for Chat Completion reliability check def check_reliability_chat(messages): \"\"\"Check reliability in chat conversations using OpenAI library\"\"\" # Create a separate client for chat completions with the correct base URL client = OpenAI( base_url=base_url, api_key=api_key, ) # Make the request using OpenAI client - pass parameters directly response = client.chat.completions.create( model=\"klusterai/verify-reliability\", #Reliability model messages=messages ) # Parse the response - kluster.ai returns check results in a specific format return response.choices[0].message.content In [20]: Copied! # Test Chat Completion checks with our examples chat_results = [] for i , example in enumerate ( chat_examples ): print ( f \"=== Chat Example { i + 1 } ===\" ) print ( f \"System: { example [ 'messages' ][ 0 ][ 'content' ] } \" ) print ( f \"User: { example [ 'messages' ][ 1 ][ 'content' ] } \" ) print ( f \"Assistant: { example [ 'messages' ][ 2 ][ 'content' ] } \" ) print ( f \"Expected Unreliable: { example [ 'expected_unreliable' ] } \" ) print () try : result = check_reliability_chat ( messages = example [ 'messages' ], ) chat_results . append ({ 'example' : example , 'result' : result }) print ( \"Check Result:\" ) print ( f \"Result: { result } \" ) except Exception as e : print ( f \"Error processing chat example: { e } \" ) print ( \" \\n \" + \"=\" * 80 + \" \\n \" ) # Summary of chat checks print ( \"### Chat Check Summary\" ) print ( f \"Processed { len ( chat_results ) } chat conversations\" ) for i , result in enumerate ( chat_results ): expected = result [ 'example' ][ 'expected_unreliable' ] print ( f \"Chat { i + 1 } : Expected unreliable = { expected } \" ) # Test Chat Completion checks with our examples chat_results = [] for i, example in enumerate(chat_examples): print(f\"=== Chat Example {i+1} ===\") print(f\"System: {example['messages'][0]['content']}\") print(f\"User: {example['messages'][1]['content']}\") print(f\"Assistant: {example['messages'][2]['content']}\") print(f\"Expected Unreliable: {example['expected_unreliable']}\") print() try: result = check_reliability_chat( messages=example['messages'], ) chat_results.append({ 'example': example, 'result': result }) print(\"Check Result:\") print(f\"Result: {result}\") except Exception as e: print(f\"Error processing chat example: {e}\") print(\"\\n\" + \"=\"*80 + \"\\n\") # Summary of chat checks print(\"### Chat Check Summary\") print(f\"Processed {len(chat_results)} chat conversations\") for i, result in enumerate(chat_results): expected = result['example']['expected_unreliable'] print(f\"Chat {i+1}: Expected unreliable = {expected}\") === Chat Example 1 === System: You are a helpful assistant that provides accurate medical information. User: Does vitamin C cure the common cold? Assistant: Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours. Expected Unreliable: True Error processing chat example: Error code: 500 - {'error': {'message': 'Unexpected error occurred', 'errorCode': 4000, 'type': 'invalid_request_error'}} ================================================================================ === Chat Example 2 === System: You are a knowledgeable financial advisor. User: What is compound interest? Assistant: Compound interest is when you earn interest on both your original investment and the interest that has already been earned. It causes your money to grow exponentially over time. Expected Unreliable: False Error processing chat example: Error code: 500 - {'error': {'message': 'Unexpected error occurred', 'errorCode': 4000, 'type': 'invalid_request_error'}} ================================================================================ ### Chat Check Summary Processed 0 chat conversations Summary ¬∂ This tutorial demonstrated how to use the reliability check feature of the Verify service to identify and prevent reliability issues in AI outputs. In this particular example, we used the dedicated reliability endpoint and the OpenAI-compatible method via the chat completions endpoint. Some key takeaways: Two reliability check methods : A dedicated endpoint for Q/A verifications, and the OpenAI-compatible chat completion endpoint for conversations. Two operation modes : General knowledge verification and context-based validation. Detailed explanations : The service provides clear reasoning for its determinations. Transparent verification : With return_search_results enabled, the service provides a list of sources used for verification. This helps users understand the basis for each reliability decision thereby increasing trust in the results.",
        "summary": "Reliability check with the kluster.ai API ¬∂ Introduction ¬∂ Reliability issues in AI occur when models generate information that appears plausible but is unreliable or unsupported by the provided context. This poses significant risks in production applications, particularly in domains where accuracy is critical. This tutorial demonstrates how to use Verify to identify and prevent reliability issues..."
      },
      "Text Classification Openai Api": {
        "url": "https://docs.kluster.ai/tutorials/klusterai-api/text-classification/text-classification-openai-api/",
        "title": "Using OpenAI API | kluster.ai Docs",
        "content": "Text classification with kluster.ai API ¬∂ Text classification is assigning a class/label to a given text, and it is a common go-to example to demonstrate how helpful an AI model can be. This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to classify a dataset based on a predefined set of categories. The example uses an extract from the IMDB top 1000 movies dataset and categorizes them into \"Action,\" \"Adventure,\" \"Comedy,\" \"Crime,\" \"Documentary,\" \"Drama,\" \"Fantasy,\" \"Horror,\" \"Romance,\" or \"Sci-Fi.\" You can adapt this example by using your data and categories relevant to your use case. With this approach, you can effortlessly process datasets of any scale, big or small, and obtain categorized results powered by a state-of-the-art language model. Prerequisites ¬∂ Before getting started, ensure you have the following: A kluster.ai account - sign up on the kluster.ai platform if you don't have one A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide Setup ¬∂ In this notebook, we'll use Python's getpass module to safely input the key. After execution, please provide your unique kluster.ai API key (ensure no spaces). In [1]: Copied! from getpass import getpass api_key = getpass ( \"Enter your kluster.ai API key: \" ) from getpass import getpass api_key = getpass(\"Enter your kluster.ai API key: \") Next, ensure you've installed OpenAI Python library: In [2]: Copied! % pip install - q openai %pip install -q openai Note: you may need to restart the kernel to use updated packages. With the OpenAI Python library installed, we import the necessary dependencies for the tutorial: In [3]: Copied! from openai import OpenAI import pandas as pd import time import json import os from IPython.display import clear_output , display from openai import OpenAI import pandas as pd import time import json import os from IPython.display import clear_output, display And then, initialize the client by pointing it to the kluster.ai endpoint, and passing your API key. In [4]: Copied! # Set up the client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) # Set up the client client = OpenAI( base_url=\"https://api.kluster.ai/v1\", api_key=api_key, ) Get the data ¬∂ Now that you've initialized an OpenAI-compatible client pointing to kluster.ai, we can talk about the data. This notebook includes a preloaded sample dataset derived from the Top 1000 IMDb Movies dataset. It contains movie descriptions ready for classification. No additional setup is needed. Simply proceed to the next steps to begin working with this data. In [5]: Copied! df = pd . DataFrame ({ \"text\" : [ \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\" , \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\" , \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\" , \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\" , \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\" ] }) df = pd.DataFrame({ \"text\": [ \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\", \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\", \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\", \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\", \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\" ] }) Perform batch inference ¬∂ To execute the batch inference job, we'll take the following steps: Create the batch job file - we'll generate a JSON lines file with the desired requests to be processed by the model Upload the batch job file - once it is ready, we'll upload it to the kluster.ai platform using the API, where it will be processed. We'll receive a unique ID associated with our file Start the batch job - after the file is uploaded, we'll initiate the job to process the uploaded data, using the file ID obtained before Monitor job progress - (optional) track the status of the batch job to ensure it has been successfully completed Retrieve results - once the job has completed execution, we can access and process the resultant data This notebook is prepared for you to follow along. Run the cells below to watch it all come together. Create the batch job file ¬∂ This example selects the deepseek-ai/DeepSeek-V3-0324 model. If you'd like to use a different model, feel free to change it by modifying the model field. Please refer to the Supported models section for a list of the models we support. The following snippets prepare the JSONL file, where each line represents a different request. Note that each separate batch request can have its own model. Also, we are using a temperature of 0.5 but feel free to change it and play around with the different outcomes (but we are only asking to respond with a single word, the genre). In [6]: Copied! # Prompt SYSTEM_PROMPT = ''' Classify the main genre of the given movie description based on the following genres (Respond with only the genre): ‚ÄúAction‚Äù, ‚ÄúAdventure‚Äù, ‚ÄúComedy‚Äù, ‚ÄúCrime‚Äù, ‚ÄúDocumentary‚Äù, ‚ÄúDrama‚Äù, ‚ÄúFantasy‚Äù, ‚ÄúHorror‚Äù, ‚ÄúRomance‚Äù, ‚ÄúSci-Fi‚Äù. ''' # Models #model=\"deepseek-ai/DeepSeek-R1\" model = \"deepseek-ai/DeepSeek-V3-0324\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\" # Ensure the directory exists os . makedirs ( \"text_clasification\" , exist_ok = True ) # Create the batch job file with the prompt and content def create_batch_file ( df ): batch_list = [] for index , row in df . iterrows (): content = row [ 'text' ] request = { \"custom_id\" : f \"movie_classification- { index } \" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : model , \"temperature\" : 0.5 , \"messages\" : [ { \"role\" : \"system\" , \"content\" : SYSTEM_PROMPT }, { \"role\" : \"user\" , \"content\" : content } ], } } batch_list . append ( request ) return batch_list # Save file def save_batch_file ( batch_list ): filename = f \"text_clasification/batch_job_request.jsonl\" with open ( filename , 'w' ) as file : for request in batch_list : file . write ( json . dumps ( request ) + ' \\n ' ) return filename # Prompt SYSTEM_PROMPT = ''' Classify the main genre of the given movie description based on the following genres (Respond with only the genre): ‚ÄúAction‚Äù, ‚ÄúAdventure‚Äù, ‚ÄúComedy‚Äù, ‚ÄúCrime‚Äù, ‚ÄúDocumentary‚Äù, ‚ÄúDrama‚Äù, ‚ÄúFantasy‚Äù, ‚ÄúHorror‚Äù, ‚ÄúRomance‚Äù, ‚ÄúSci-Fi‚Äù. ''' # Models #model=\"deepseek-ai/DeepSeek-R1\" model=\"deepseek-ai/DeepSeek-V3-0324\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\" # Ensure the directory exists os.makedirs(\"text_clasification\", exist_ok=True) # Create the batch job file with the prompt and content def create_batch_file(df): batch_list = [] for index, row in df.iterrows(): content = row['text'] request = { \"custom_id\": f\"movie_classification-{index}\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": { \"model\": model, \"temperature\": 0.5, \"messages\": [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": content} ], } } batch_list.append(request) return batch_list # Save file def save_batch_file(batch_list): filename = f\"text_clasification/batch_job_request.jsonl\" with open(filename, 'w') as file: for request in batch_list: file.write(json.dumps(request) + '\\n') return filename Let's run the functions we've defined before: In [7]: Copied! batch_list = create_batch_file ( df ) data_dir = save_batch_file ( batch_list ) print ( data_dir ) batch_list = create_batch_file(df) data_dir = save_batch_file(batch_list) print(data_dir) text_clasification/batch_job_request.jsonl Next, we can preview what that batch job file looks like: In [8]: Copied! ! head - n 1 text_clasification / batch_job_request . jsonl !head -n 1 text_clasification/batch_job_request.jsonl {\"custom_id\": \"movie_classification-0\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n Classify the main genre of the given movie description based on the following genres (Respond with only the genre):\\n \\u201cAction\\u201d, \\u201cAdventure\\u201d, \\u201cComedy\\u201d, \\u201cCrime\\u201d, \\u201cDocumentary\\u201d, \\u201cDrama\\u201d, \\u201cFantasy\\u201d, \\u201cHorror\\u201d, \\u201cRomance\\u201d, \\u201cSci-Fi\\u201d.\\n \"}, {\"role\": \"user\", \"content\": \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\"}]}} Upload batch job file to kluster.ai ¬∂ Now that we‚Äôve prepared our input file, it‚Äôs time to upload it to the kluster.ai platform. To do so, you can use the files.create endpoint of the client, where the purpose is set to batch . This will return the file ID, which we need to log for the next steps. In [9]: Copied! # Upload batch job request file with open ( data_dir , 'rb' ) as file : upload_response = client . files . create ( file = file , purpose = \"batch\" ) # Print job ID file_id = upload_response . id print ( f \"File uploaded successfully. File ID: { file_id } \" ) # Upload batch job request file with open(data_dir, 'rb') as file: upload_response = client.files.create( file=file, purpose=\"batch\" ) # Print job ID file_id = upload_response.id print(f\"File uploaded successfully. File ID: {file_id}\") File uploaded successfully. File ID: 6801403efba4aabfd069a682 Start the batch job ¬∂ Once the file has been successfully uploaded, we're ready to start (create) the batch job by providing the file ID we got in the previous step. To do so, we use the batches.create method, for which we need to set the endpoint to /v1/chat/completions . This will return the batch job details, with the ID. In [10]: Copied! # Create batch job with completions endpoint batch_job = client . batches . create ( input_file_id = file_id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" ) print ( \" \\n Batch job created:\" ) batch_dict = batch_job . model_dump () print ( json . dumps ( batch_dict , indent = 2 )) # Create batch job with completions endpoint batch_job = client.batches.create( input_file_id=file_id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\" ) print(\"\\nBatch job created:\") batch_dict = batch_job.model_dump() print(json.dumps(batch_dict, indent=2)) Batch job created: { \"id\": \"6801403e0969ab67de09ec8d\", \"completion_window\": \"24h\", \"created_at\": 1744912446, \"endpoint\": \"/v1/chat/completions\", \"input_file_id\": \"6801403efba4aabfd069a682\", \"object\": \"batch\", \"status\": \"pre_schedule\", \"cancelled_at\": null, \"cancelling_at\": null, \"completed_at\": null, \"error_file_id\": null, \"errors\": [], \"expired_at\": null, \"expires_at\": 1744998846, \"failed_at\": null, \"finalizing_at\": null, \"in_progress_at\": null, \"metadata\": {}, \"output_file_id\": null, \"request_counts\": { \"completed\": 0, \"failed\": 0, \"total\": 0 } } /Users/kevin/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings: Expected `Errors` but got `list` with value `[]` - serialized value may not be as expected return self.__pydantic_serializer__.to_python( Check job progress ¬∂ Now that your batch job has been created, you can track its progress. To monitor the job's progress, we can use the batches.retrieve method and pass the batch job ID. The response contains an status field that tells us if it is completed or not, and the subsequent status of each job separately. The following snippet checks the status every 10 seconds until the entire batch is completed: In [11]: Copied! all_completed = False # Loop to check status every 10 seconds while not all_completed : all_completed = True output_lines = [] updated_job = client . batches . retrieve ( batch_job . id ) if updated_job . status != \"completed\" : all_completed = False completed = updated_job . request_counts . completed total = updated_job . request_counts . total output_lines . append ( f \"Job status: { updated_job . status } - Progress: { completed } / { total } \" ) else : output_lines . append ( f \"Job completed!\" ) # Clear the output and display updated status clear_output ( wait = True ) for line in output_lines : display ( line ) if not all_completed : time . sleep ( 10 ) all_completed = False # Loop to check status every 10 seconds while not all_completed: all_completed = True output_lines = [] updated_job = client.batches.retrieve(batch_job.id) if updated_job.status != \"completed\": all_completed = False completed = updated_job.request_counts.completed total = updated_job.request_counts.total output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\") else: output_lines.append(f\"Job completed!\") # Clear the output and display updated status clear_output(wait=True) for line in output_lines: display(line) if not all_completed: time.sleep(10) 'Job completed!' Get the results ¬∂ With the job completed, we'll retrieve the results and review the responses generated for each request. The results are parsed. To fetch the results from the platform, you need to retrieve the output_file_id from the batch job, and then use the files.content endpoint, providing that specific file ID. Note that the job status must be completed for you to retrieve the results! In [12]: Copied! #Parse results as a JSON object def parse_json_objects ( data_string ): if isinstance ( data_string , bytes ): data_string = data_string . decode ( 'utf-8' ) json_strings = data_string . strip () . split ( ' \\n ' ) json_objects = [] for json_str in json_strings : try : json_obj = json . loads ( json_str ) json_objects . append ( json_obj ) except json . JSONDecodeError as e : print ( f \"Error parsing JSON: { e } \" ) return json_objects # Retrieve results with job ID job = client . batches . retrieve ( batch_job . id ) result_file_id = job . output_file_id result = client . files . content ( result_file_id ) . content # Parse JSON results parsed_result = parse_json_objects ( result ) # Extract and print only the content of each response print ( \" \\n Extracted Responses:\" ) for item in parsed_result : try : content = item [ \"response\" ][ \"body\" ][ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ] print ( content ) except KeyError as e : print ( f \"Missing key in response: { e } \" ) #Parse results as a JSON object def parse_json_objects(data_string): if isinstance(data_string, bytes): data_string = data_string.decode('utf-8') json_strings = data_string.strip().split('\\n') json_objects = [] for json_str in json_strings: try: json_obj = json.loads(json_str) json_objects.append(json_obj) except json.JSONDecodeError as e: print(f\"Error parsing JSON: {e}\") return json_objects # Retrieve results with job ID job = client.batches.retrieve(batch_job.id) result_file_id = job.output_file_id result = client.files.content(result_file_id).content # Parse JSON results parsed_result = parse_json_objects(result) # Extract and print only the content of each response print(\"\\nExtracted Responses:\") for item in parsed_result: try: content = item[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"] print(content) except KeyError as e: print(f\"Missing key in response: {e}\") Extracted Responses: Romance Drama Drama Drama Crime Summary ¬∂ This tutorial used the chat completion endpoint to perform a simple text classification task with batch inference. This particular example clasified a series of movies based on their description. To submit a batch job we've: Created the JSONL file, where each line of the file represented a separate request Submitted the file to the platform Started the batch job, and monitored its progress Once completed, we fetched the results All of this using the OpenAI Python library and API, no changes needed! Kluster.ai's batch API empowers you to scale your workflows seamlessly, making it an invaluable tool for processing extensive datasets. As next steps, feel free to create your own dataset, or expand on top of this existing example. Good luck!",
        "summary": "Text classification with kluster.ai API ¬∂ Text classification is assigning a class/label to a given text, and it is a common go-to example to demonstrate how helpful an AI model can be. This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to classify a dataset based on a predefined set of categories. The example uses an extract from the IMDB top 1000 movies ..."
      }
    }
  },
  "raw_data": {
    "https://docs.kluster.ai/get-started/fine-tuning/overview/": {
      "url": "https://docs.kluster.ai/get-started/fine-tuning/overview/",
      "title": "Overview of Fine-tuning models | kluster.ai Docs",
      "content": "Overview of fine-tuning models ÔºÉ Fine-tuning lets you transform general-purpose models into specialized AI assistants that excel at your unique tasks. With kluster.ai , you can fine-tune models using either the platform or the API ‚Äîchoose the path that fits your workflow and technical needs. Fine-tuning flow ÔºÉ Fine-tuning on kluster.ai follows a simple, seven-step loop: Prepare your dataset : Collect representative examples for the task and save them as a JSON Lines ( .jsonl ) file. Upload the dataset : Use the Platform ‚ÄúUpload‚Äù dialog or call files.upload via the API to receive a file_id . Configure & launch a job : Choose a base model, set LoRA-specific (Low-Rank Adaptation) hyper-parameters (epochs, learning rate, adapter rank, etc.), and start the job. Monitor training : Track status and metrics in the dashboard or poll fine_tuning.jobs.retrieve until the job reaches succeeded or failed . Retrieve the fine-tuned model : When the job finishes, grab the returned fine_tuned_model ID and treat it like any other model. Evaluate & iterate : Test the model on unseen prompts, compare against the base model, and re-run fine-tuning with refined data or parameters if needed. Deploy & integrate : Call the model in production, export its LoRA adapter, or share it with teammates through kluster.ai‚Äôs model registry. Fine-tuning is your go-to when you need reliable, domain-specific outputs (e.g., JSON-formatted responses, brand-aligned tone) that prompt engineering alone can‚Äôt guarantee. When to fine-tune your model ÔºÉ Fine-tuning is ideal for scenarios where you need: Domain specialization : Create models that excel in specific fields like medicine, law, finance, or technical documentation. Brand-aligned responses : Train models to match your company's voice, style, and communication guidelines. Format consistency : Ensure reliable output in specific formats like JSON, XML, or Markdown. Enhanced reasoning : Improve analytical capabilities for specific types of problems. Custom behavior : Develop assistants that follow your unique processes and workflows. Benefits of fine-tuning ÔºÉ Fine-tuning delivers several key advantages over using general-purpose models: Improved performance : Fine-tuned models consistently outperform base models on specific tasks. Cost efficiency : Smaller fine-tuned models can match or exceed the performance of larger models at a lower cost. Reduced latency : Fine-tuned models provide faster responses, enhancing the user experience. Consistency : Achieve more reliable outputs tailored to your specific requirements. Data privacy : Train models on your data without exposing sensitive information in prompts. Supported models ÔºÉ kluster.ai currently supports fine-tuning for the following models: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Choose your fine-tuning approach ÔºÉ kluster.ai offers two ways to fine-tune models, each designed for different user preferences and requirements: Guide Platform Use the platform to fine-tune without writing code. The platform is ideal for users who want a guided, interactive experience and real-time feedback on training progress. Visit the guide Guide API Fine-tune models with code for maximum flexibility and automation. The API is best for developers who need advanced customization, integration, or workflow automation. Visit the guide Additional resources ÔºÉ Step-by-step tutorial : Learn the fundamentals with our Fine-tuning sentiment analysis tutorial . Available models : Explore our Models page to see all foundation models that support fine-tuning. API reference : Review the complete API documentation for all fine-tuning related endpoints.",
      "scraped_at": 1749147204.0140274
    },
    "https://docs.kluster.ai/api-reference/reference/#submit-a-batch-job": {
      "url": "https://docs.kluster.ai/api-reference/reference/#submit-a-batch-job",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147203.2548766
    },
    "https://docs.kluster.ai/get-started/verify/reliability/overview/": {
      "url": "https://docs.kluster.ai/get-started/verify/reliability/overview/",
      "title": "Reliability check by Verify | kluster.ai Docs",
      "content": "Reliability check by Verify ÔºÉ Reliability check is one of the features offered by Verify, and it is able to identify when AI responses contain fabricated or inaccurate information. With this specialized service, you can gauge the reliability of AI-generated content and build more trustworthy applications. The service can evaluate the AI response based on a given context, which makes it great for RAG applications. Without providing a specific context, the service can also be used as a real-time reliability verification service. How reliability check works ÔºÉ The service evaluates the truthfulness of an answer to a question by: Analyzing the original question, prompt or entire conversation history. Examining the provided answer (with context if provided). Determining if the answer contains unreliable or unsupported information. Providing a detailed explanation of the reasoning behind the determination as well as the search results used for verification. The service evaluates AI outputs in order to identify reliability issues or incorrect information, with the following fields: is_hallucination=true/false : Indicates whether the response contains unreliable content. explanation : Provides detailed reasoning for the determination. search_results : Shows the reference data used for verification (when applicable). For example, for the following prompt: ... { \"role\": \"user\", \"content\": \"Where is the Eiffel Tower?\" }, { \"role\": \"assistant\", \"content\": \"The Eiffel Tower is located in Rome.\" } ... The reliability check response would return: { \"is_hallucination\" : true , \"usage\" : { \"completion_tokens\" : 154 , \"prompt_tokens\" : 1100 , \"total_tokens\" : 1254 }, \"explanation\" : \"The response provides a wrong location for the Eiffel Tower.\\n\" \"The Eiffel Tower is actually located in Paris, France, not in Rome.\\n\" \"The response contains misinformation as it incorrectly states the tower's location.\" , \"search_results\" : [] } When to use reliability checking ÔºÉ The reliability check service is ideal for scenarios where you need: Model evaluation : Easily integrate the service to compare models output quality. RAG applications : Verify that generated responses accurately reflect the provided reference documents rather than introducing fabricated information. Internet-sourced verification : Validate claims against reliable online sources with transparent citation of evidence. Content moderation : Automatically flag potentially misleading information before it reaches end users. Regulatory compliance : Ensure AI-generated content meets accuracy requirements. How to integrate reliability checks ÔºÉ Verify offers multiple ways to perform reliability checks, each designed for different use cases: Guide Reliability dedicated endpoint Verify the reliability and accuracy of an answer to a specific question via a dedicated API endpoint. Visit the guide Guide Chat completion endpoint Validate responses in full conversation via the chat completions API using OpenAI libraries. Visit the guide Integration Workflow Integrations Download ready-to-use workflows for Dify, n8n, and other platforms using direct API integration. Get workflows Additional resources ÔºÉ Workflow Integrations : Download ready-to-use workflows for Dify, n8n . Tutorial : Explore the Verify tutorial with code examples.",
      "scraped_at": 1749147202.711875
    },
    "https://docs.kluster.ai/get-started/models/#model-comparison-table": {
      "url": "https://docs.kluster.ai/get-started/models/#model-comparison-table",
      "title": "Supported AI Models | kluster.ai Docs",
      "content": "Models on kluster.ai ÔºÉ kluster.ai offers a wide variety of open-source models for both real-time and batch inferences, with more being constantly added. This page covers all the models the API supports, with the API request limits for each. Model names ÔºÉ Each model supported by kluster.ai has a unique name that must be used when defining the model in the request. Model Model API name DeepSeek-R1 deepseek-ai/DeepSeek-R1 DeepSeek-R1-0528 deepseek-ai/DeepSeek-R1-0528 DeepSeek-V3-0324 deepseek-ai/DeepSeek-V3-0324 Gemma 3 27B google/gemma-3-27b-it Meta Llama 3.1 8B klusterai/Meta-Llama-3.1-8B-Instruct-Turbo Meta Llama 3.3 70B klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Meta Llama 4 Maverick meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 Meta Llama 4 Scout meta-llama/Llama-4-Scout-17B-16E-Instruct Mistral NeMo mistralai/Mistral-Nemo-Instruct-2407 Qwen2.5-VL 7B Qwen/Qwen2.5-VL-7B-Instruct Qwen3-235B-A22B Qwen/Qwen3-235B-A22B-FP8 Model comparison table ÔºÉ Model Description Real-time inference support Batch inference support Fine-tuning support Image analysis DeepSeek-R1 Mathematical problem-solving code generation complex data analysis. DeepSeek-R1-0528 Mathematical problem-solving code generation complex data analysis. DeepSeek-V3-0324 Natural language generation open-ended text creation contextually rich writing. Gemma 3 27B Multilingual applications extended-context tasks image analysis and complex reasoning. Llama 3.1 8B Low-latency or simple tasks cost-efficient inference. Llama 3.3 70B General-purpose AI balanced cost-performance. Llama 4 Maverick A state-of-the-art multimodal model with integrated vision and language understanding, optimized for complex reasoning, coding, and perception tasks Llama 4 Scout General-purpose multimodal AI extended context tasks and balanced cost-performance across text and vision. Mistral NeMo Natural language generation open-ended text creation contextually rich writing. Qwen2.5-VL 7B Visual question answering document analysis image-based reasoning multimodal chat. Qwen3-235B-A22B Qwen3's flagship 235 billion parameter model optimized with 8-bit quantization API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited",
      "scraped_at": 1749147202.3604326
    },
    "https://docs.kluster.ai/tutorials/": {
      "url": "https://docs.kluster.ai/tutorials/",
      "title": "Using OpenAI API | kluster.ai Docs",
      "content": "Text classification with kluster.ai API ¬∂ Text classification is assigning a class/label to a given text, and it is a common go-to example to demonstrate how helpful an AI model can be. This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to classify a dataset based on a predefined set of categories. The example uses an extract from the IMDB top 1000 movies dataset and categorizes them into \"Action,\" \"Adventure,\" \"Comedy,\" \"Crime,\" \"Documentary,\" \"Drama,\" \"Fantasy,\" \"Horror,\" \"Romance,\" or \"Sci-Fi.\" You can adapt this example by using your data and categories relevant to your use case. With this approach, you can effortlessly process datasets of any scale, big or small, and obtain categorized results powered by a state-of-the-art language model. Prerequisites ¬∂ Before getting started, ensure you have the following: A kluster.ai account - sign up on the kluster.ai platform if you don't have one A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide Setup ¬∂ In this notebook, we'll use Python's getpass module to safely input the key. After execution, please provide your unique kluster.ai API key (ensure no spaces). In [1]: Copied! from getpass import getpass api_key = getpass ( \"Enter your kluster.ai API key: \" ) from getpass import getpass api_key = getpass(\"Enter your kluster.ai API key: \") Next, ensure you've installed OpenAI Python library: In [2]: Copied! % pip install - q openai %pip install -q openai Note: you may need to restart the kernel to use updated packages. With the OpenAI Python library installed, we import the necessary dependencies for the tutorial: In [3]: Copied! from openai import OpenAI import pandas as pd import time import json import os from IPython.display import clear_output , display from openai import OpenAI import pandas as pd import time import json import os from IPython.display import clear_output, display And then, initialize the client by pointing it to the kluster.ai endpoint, and passing your API key. In [4]: Copied! # Set up the client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) # Set up the client client = OpenAI( base_url=\"https://api.kluster.ai/v1\", api_key=api_key, ) Get the data ¬∂ Now that you've initialized an OpenAI-compatible client pointing to kluster.ai, we can talk about the data. This notebook includes a preloaded sample dataset derived from the Top 1000 IMDb Movies dataset. It contains movie descriptions ready for classification. No additional setup is needed. Simply proceed to the next steps to begin working with this data. In [5]: Copied! df = pd . DataFrame ({ \"text\" : [ \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\" , \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\" , \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\" , \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\" , \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\" ] }) df = pd.DataFrame({ \"text\": [ \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\", \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\", \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\", \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\", \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\" ] }) Perform batch inference ¬∂ To execute the batch inference job, we'll take the following steps: Create the batch job file - we'll generate a JSON lines file with the desired requests to be processed by the model Upload the batch job file - once it is ready, we'll upload it to the kluster.ai platform using the API, where it will be processed. We'll receive a unique ID associated with our file Start the batch job - after the file is uploaded, we'll initiate the job to process the uploaded data, using the file ID obtained before Monitor job progress - (optional) track the status of the batch job to ensure it has been successfully completed Retrieve results - once the job has completed execution, we can access and process the resultant data This notebook is prepared for you to follow along. Run the cells below to watch it all come together. Create the batch job file ¬∂ This example selects the deepseek-ai/DeepSeek-V3-0324 model. If you'd like to use a different model, feel free to change it by modifying the model field. Please refer to the Supported models section for a list of the models we support. The following snippets prepare the JSONL file, where each line represents a different request. Note that each separate batch request can have its own model. Also, we are using a temperature of 0.5 but feel free to change it and play around with the different outcomes (but we are only asking to respond with a single word, the genre). In [6]: Copied! # Prompt SYSTEM_PROMPT = ''' Classify the main genre of the given movie description based on the following genres (Respond with only the genre): ‚ÄúAction‚Äù, ‚ÄúAdventure‚Äù, ‚ÄúComedy‚Äù, ‚ÄúCrime‚Äù, ‚ÄúDocumentary‚Äù, ‚ÄúDrama‚Äù, ‚ÄúFantasy‚Äù, ‚ÄúHorror‚Äù, ‚ÄúRomance‚Äù, ‚ÄúSci-Fi‚Äù. ''' # Models #model=\"deepseek-ai/DeepSeek-R1\" model = \"deepseek-ai/DeepSeek-V3-0324\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\" # Ensure the directory exists os . makedirs ( \"text_clasification\" , exist_ok = True ) # Create the batch job file with the prompt and content def create_batch_file ( df ): batch_list = [] for index , row in df . iterrows (): content = row [ 'text' ] request = { \"custom_id\" : f \"movie_classification- { index } \" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : model , \"temperature\" : 0.5 , \"messages\" : [ { \"role\" : \"system\" , \"content\" : SYSTEM_PROMPT }, { \"role\" : \"user\" , \"content\" : content } ], } } batch_list . append ( request ) return batch_list # Save file def save_batch_file ( batch_list ): filename = f \"text_clasification/batch_job_request.jsonl\" with open ( filename , 'w' ) as file : for request in batch_list : file . write ( json . dumps ( request ) + ' \\n ' ) return filename # Prompt SYSTEM_PROMPT = ''' Classify the main genre of the given movie description based on the following genres (Respond with only the genre): ‚ÄúAction‚Äù, ‚ÄúAdventure‚Äù, ‚ÄúComedy‚Äù, ‚ÄúCrime‚Äù, ‚ÄúDocumentary‚Äù, ‚ÄúDrama‚Äù, ‚ÄúFantasy‚Äù, ‚ÄúHorror‚Äù, ‚ÄúRomance‚Äù, ‚ÄúSci-Fi‚Äù. ''' # Models #model=\"deepseek-ai/DeepSeek-R1\" model=\"deepseek-ai/DeepSeek-V3-0324\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\" # Ensure the directory exists os.makedirs(\"text_clasification\", exist_ok=True) # Create the batch job file with the prompt and content def create_batch_file(df): batch_list = [] for index, row in df.iterrows(): content = row['text'] request = { \"custom_id\": f\"movie_classification-{index}\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": { \"model\": model, \"temperature\": 0.5, \"messages\": [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": content} ], } } batch_list.append(request) return batch_list # Save file def save_batch_file(batch_list): filename = f\"text_clasification/batch_job_request.jsonl\" with open(filename, 'w') as file: for request in batch_list: file.write(json.dumps(request) + '\\n') return filename Let's run the functions we've defined before: In [7]: Copied! batch_list = create_batch_file ( df ) data_dir = save_batch_file ( batch_list ) print ( data_dir ) batch_list = create_batch_file(df) data_dir = save_batch_file(batch_list) print(data_dir) text_clasification/batch_job_request.jsonl Next, we can preview what that batch job file looks like: In [8]: Copied! ! head - n 1 text_clasification / batch_job_request . jsonl !head -n 1 text_clasification/batch_job_request.jsonl {\"custom_id\": \"movie_classification-0\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n Classify the main genre of the given movie description based on the following genres (Respond with only the genre):\\n \\u201cAction\\u201d, \\u201cAdventure\\u201d, \\u201cComedy\\u201d, \\u201cCrime\\u201d, \\u201cDocumentary\\u201d, \\u201cDrama\\u201d, \\u201cFantasy\\u201d, \\u201cHorror\\u201d, \\u201cRomance\\u201d, \\u201cSci-Fi\\u201d.\\n \"}, {\"role\": \"user\", \"content\": \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\"}]}} Upload batch job file to kluster.ai ¬∂ Now that we‚Äôve prepared our input file, it‚Äôs time to upload it to the kluster.ai platform. To do so, you can use the files.create endpoint of the client, where the purpose is set to batch . This will return the file ID, which we need to log for the next steps. In [9]: Copied! # Upload batch job request file with open ( data_dir , 'rb' ) as file : upload_response = client . files . create ( file = file , purpose = \"batch\" ) # Print job ID file_id = upload_response . id print ( f \"File uploaded successfully. File ID: { file_id } \" ) # Upload batch job request file with open(data_dir, 'rb') as file: upload_response = client.files.create( file=file, purpose=\"batch\" ) # Print job ID file_id = upload_response.id print(f\"File uploaded successfully. File ID: {file_id}\") File uploaded successfully. File ID: 6801403efba4aabfd069a682 Start the batch job ¬∂ Once the file has been successfully uploaded, we're ready to start (create) the batch job by providing the file ID we got in the previous step. To do so, we use the batches.create method, for which we need to set the endpoint to /v1/chat/completions . This will return the batch job details, with the ID. In [10]: Copied! # Create batch job with completions endpoint batch_job = client . batches . create ( input_file_id = file_id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" ) print ( \" \\n Batch job created:\" ) batch_dict = batch_job . model_dump () print ( json . dumps ( batch_dict , indent = 2 )) # Create batch job with completions endpoint batch_job = client.batches.create( input_file_id=file_id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\" ) print(\"\\nBatch job created:\") batch_dict = batch_job.model_dump() print(json.dumps(batch_dict, indent=2)) Batch job created: { \"id\": \"6801403e0969ab67de09ec8d\", \"completion_window\": \"24h\", \"created_at\": 1744912446, \"endpoint\": \"/v1/chat/completions\", \"input_file_id\": \"6801403efba4aabfd069a682\", \"object\": \"batch\", \"status\": \"pre_schedule\", \"cancelled_at\": null, \"cancelling_at\": null, \"completed_at\": null, \"error_file_id\": null, \"errors\": [], \"expired_at\": null, \"expires_at\": 1744998846, \"failed_at\": null, \"finalizing_at\": null, \"in_progress_at\": null, \"metadata\": {}, \"output_file_id\": null, \"request_counts\": { \"completed\": 0, \"failed\": 0, \"total\": 0 } } /Users/kevin/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings: Expected `Errors` but got `list` with value `[]` - serialized value may not be as expected return self.__pydantic_serializer__.to_python( Check job progress ¬∂ Now that your batch job has been created, you can track its progress. To monitor the job's progress, we can use the batches.retrieve method and pass the batch job ID. The response contains an status field that tells us if it is completed or not, and the subsequent status of each job separately. The following snippet checks the status every 10 seconds until the entire batch is completed: In [11]: Copied! all_completed = False # Loop to check status every 10 seconds while not all_completed : all_completed = True output_lines = [] updated_job = client . batches . retrieve ( batch_job . id ) if updated_job . status != \"completed\" : all_completed = False completed = updated_job . request_counts . completed total = updated_job . request_counts . total output_lines . append ( f \"Job status: { updated_job . status } - Progress: { completed } / { total } \" ) else : output_lines . append ( f \"Job completed!\" ) # Clear the output and display updated status clear_output ( wait = True ) for line in output_lines : display ( line ) if not all_completed : time . sleep ( 10 ) all_completed = False # Loop to check status every 10 seconds while not all_completed: all_completed = True output_lines = [] updated_job = client.batches.retrieve(batch_job.id) if updated_job.status != \"completed\": all_completed = False completed = updated_job.request_counts.completed total = updated_job.request_counts.total output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\") else: output_lines.append(f\"Job completed!\") # Clear the output and display updated status clear_output(wait=True) for line in output_lines: display(line) if not all_completed: time.sleep(10) 'Job completed!' Get the results ¬∂ With the job completed, we'll retrieve the results and review the responses generated for each request. The results are parsed. To fetch the results from the platform, you need to retrieve the output_file_id from the batch job, and then use the files.content endpoint, providing that specific file ID. Note that the job status must be completed for you to retrieve the results! In [12]: Copied! #Parse results as a JSON object def parse_json_objects ( data_string ): if isinstance ( data_string , bytes ): data_string = data_string . decode ( 'utf-8' ) json_strings = data_string . strip () . split ( ' \\n ' ) json_objects = [] for json_str in json_strings : try : json_obj = json . loads ( json_str ) json_objects . append ( json_obj ) except json . JSONDecodeError as e : print ( f \"Error parsing JSON: { e } \" ) return json_objects # Retrieve results with job ID job = client . batches . retrieve ( batch_job . id ) result_file_id = job . output_file_id result = client . files . content ( result_file_id ) . content # Parse JSON results parsed_result = parse_json_objects ( result ) # Extract and print only the content of each response print ( \" \\n Extracted Responses:\" ) for item in parsed_result : try : content = item [ \"response\" ][ \"body\" ][ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ] print ( content ) except KeyError as e : print ( f \"Missing key in response: { e } \" ) #Parse results as a JSON object def parse_json_objects(data_string): if isinstance(data_string, bytes): data_string = data_string.decode('utf-8') json_strings = data_string.strip().split('\\n') json_objects = [] for json_str in json_strings: try: json_obj = json.loads(json_str) json_objects.append(json_obj) except json.JSONDecodeError as e: print(f\"Error parsing JSON: {e}\") return json_objects # Retrieve results with job ID job = client.batches.retrieve(batch_job.id) result_file_id = job.output_file_id result = client.files.content(result_file_id).content # Parse JSON results parsed_result = parse_json_objects(result) # Extract and print only the content of each response print(\"\\nExtracted Responses:\") for item in parsed_result: try: content = item[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"] print(content) except KeyError as e: print(f\"Missing key in response: {e}\") Extracted Responses: Romance Drama Drama Drama Crime Summary ¬∂ This tutorial used the chat completion endpoint to perform a simple text classification task with batch inference. This particular example clasified a series of movies based on their description. To submit a batch job we've: Created the JSONL file, where each line of the file represented a separate request Submitted the file to the platform Started the batch job, and monitored its progress Once completed, we fetched the results All of this using the OpenAI Python library and API, no changes needed! Kluster.ai's batch API empowers you to scale your workflows seamlessly, making it an invaluable tool for processing extensive datasets. As next steps, feel free to create your own dataset, or expand on top of this existing example. Good luck!",
      "scraped_at": 1749147204.075558
    },
    "https://docs.kluster.ai/get-started/start-building/real-time/": {
      "url": "https://docs.kluster.ai/get-started/start-building/real-time/",
      "title": "Perform real-time inference jobs | kluster.ai Docs",
      "content": "Perform real-time inference jobs ÔºÉ Overview ÔºÉ This guide provides guidance about how to use real-time inference with the kluster.ai API. This type of inference is best suited for use cases requiring instant, synchronous responses for user-facing features like chat interactions, live recommendations, or real-time decision-making. You will learn how to submit a request and retrieve responses, and where to find integration guides for using kluster.ai's API with some of your favorite third-party LLM interfaces. Please make sure you check the API request limits . Prerequisites ÔºÉ This guide assumes familiarity with Large Language Model (LLM) development and OpenAI libraries. Before getting started, make sure you have: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. A virtual Python environment - (optional) recommended for developers using Python. It helps isolate Python installations in a virtual environment to reduce the risk of environment or package conflicts between your projects Required Python libraries - install the following Python libraries: OpenAI Python API library - to access the openai module getpass - to handle API keys safely If you plan to use cURL via the CLI, you can export kluster.ai API key as a variable: export API_KEY = INSERT_API_KEY Supported models ÔºÉ Please visit the Models page to learn more about all the models supported by the kluster.ai batch API. In addition, you can see the complete list of available models programmatically using the list supported models endpoint. Quickstart snippets ÔºÉ The following code snippets provide a complete end-to-end real-time inference example for different models supported by kluster.ai. You can copy and paste the snippet into your local environment. Python ÔºÉ To use these snippets, run the Python script and enter your kluster.ai API key when prompted. DeepSeek-R1 # Real-time completions with the DeepSeek-R1 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"deepseek-ai/DeepSeek-R1\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) DeepSeek-R1-0528 # Real-time completions with the DeepSeek-R1-0528 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"deepseek-ai/DeepSeek-R1-0528\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) DeepSeek-V3-0324 # Real-time completions with the DeepSeek-V3-0324 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"deepseek-ai/DeepSeek-V3-0324\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Gemma 3 27B # Real-time completions with the Gemma 3 27B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image_url }}, ], } ], ) print ( f \" \\n Image URL: { image_url } \" ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Meta Llama 3.1 8B # Real-time completions with the Meta Llama 3.1 8B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Meta Llama 3.3 70B # Real-time completions with the Meta Llama 3.3 70B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Meta Llama 4 Maverick # Real-time completions with the Meta Llama 4 Maverick model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image_url }}, ], } ], ) print ( f \" \\n Image URL: { image_url } \" ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Meta Llama 4 Scout # Real-time completions with the Meta Llama 4 Scout model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image_url }}, ], } ], ) print ( f \" \\n Image URL: { image_url } \" ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Mistral NeMo # Real-time completions with the Mistral NeMo model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"mistralai/Mistral-Nemo-Instruct-2407\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Qwen2.5-VL 7B # Real-time completions with the Qwen2.5-VL 7B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"Qwen/Qwen2.5-VL-7B-Instruct\" , messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image_url }}, ], } ], ) print ( f \" \\n Image URL: { image_url } \" ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) Qwen3-235B-A22B # Real-time completions with the Qwen3-235B-A22B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"Qwen/Qwen3-235B-A22B-FP8\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) \"\"\"Logs the full AI response to terminal.\"\"\" # Extract model name and AI-generated text model_name = completion . model text_response = completion . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) CLI ÔºÉ Similarly, the following curl commands showcase how to easily send a chat completion request to kluster.ai for the different supported models. This example assumes you've exported your kluster.ai API key as the variable API_KEY . DeepSeek-R1 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"deepseek-ai/DeepSeek-R1\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" DeepSeek-R1-0528 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"deepseek-ai/DeepSeek-R1-0528\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" DeepSeek-V3-0324 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"deepseek-ai/DeepSeek-V3-0324\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" Gemma 3 27B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"google/gemma-3-27b-it\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": [ {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Who can park in the area?\\\"}, {\\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": {\\\"url\\\": \\\" $image_url \\\"}} ] } ] }\" Meta Llama 3.1 8B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" Meta Llama 3.3 70B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" Meta Llama 4 Maverick #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": [ {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Who can park in the area?\\\"}, {\\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": {\\\"url\\\": \\\" $image_url \\\"}} ] } ] }\" Meta Llama 4 Scout #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"meta-llama/Llama-4-Scout-17B-16E-Instruct\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": [ {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Who can park in the area?\\\"}, {\\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": {\\\"url\\\": \\\" $image_url \\\"}} ] } ] }\" Mistral NeMo #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"mistralai/Mistral-Nemo-Instruct-2407\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" Qwen2.5-VL 7B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" image_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"Qwen/Qwen2.5-VL-7B-Instruct\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": [ {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Who can park in the area?\\\"}, {\\\"type\\\": \\\"image_url\\\", \\\"image_url\\\": {\\\"url\\\": \\\" $image_url \\\"}} ] } ] }\" Qwen3-235B-A22B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"model\\\": \\\"Qwen/Qwen3-235B-A22B-FP8\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the ultimate breakfast sandwich?\\\" } ] }\" Real-time inference flow ÔºÉ This section details the real-time inference process using the kluster.ai API and DeepSeek R1 model, but you can adapt it to any of the supported models . Submitting a request ÔºÉ The kluster.ai platform offers a simple, OpenAI-compatible interface, making it easy to integrate kluster.ai services seamlessly into your existing system. The following code shows how to do a chat completions request using the OpenAI library. Python import os from getpass import getpass from openai import OpenAI # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"deepseek-ai/DeepSeek-V3-0324\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) If successful, the completion variable contains a full response, which you'll need to analyze to extract the answer you are looking for. In terms of configuration for real-time inferences, there are several parameters that you need to tweak: model string required - name of one of the supported models messages array required - a list of chat messages ( system , user , or assistant roles, and also image_url for images). In this example, the query is \"What is the ultimate breakfast sandwich?\". Once these parameters are configured, run your script to send the request. Fetching the response ÔºÉ If the request is successful, the response is contained in the completion variable from the example above. It should follow the structure below and include relevant data such as the generated output, metadata, and token usage details. Response { \"id\" : \"a3af373493654dd195108b207e2faacf\" , \"choices\" : [ { \"finish_reason\" : \"stop\" , \"index\" : 0 , \"logprobs\" : null , \"message\" : { \"content\" : \"The \\\"ultimate\\\" breakfast sandwich is subjective and can vary based on personal preferences, but here‚Äôs a classic, crowd-pleasing version that combines savory, sweet, and hearty elements for a satisfying morning meal:\\n\\n### **The Ultimate Breakfast Sandwich**\\n**Ingredients:**\\n- **Bread:** A toasted brioche bun, English muffin, or sourdough slice (your choice for texture and flavor).\\n- **Protein:** Crispy bacon, sausage patty, or ham.\\n- **Egg:** Fried, scrambled, or a fluffy omelet-style egg.\\n- **Cheese:** Sharp cheddar, gooey American, or creamy Swiss.\\n- **Sauce:** Spicy mayo, hollandaise, or a drizzle of maple syrup for sweetness.\\n- **Extras:** Sliced avocado, caramelized onions, saut√©ed mushrooms, or fresh arugula for a gourmet touch.\\n- **Seasoning:** Salt, pepper, and a pinch of red pepper flakes for heat.\\n\\n**Assembly:**\\n1. Toast your bread or bun to golden perfection.\\n2. Cook your protein to your desired crispiness or doneness.\\n3. Prepare your egg‚Äîfried with a runny yolk is a classic choice.\\n4. Layer the cheese on the warm egg or protein so it melts slightly.\\n5. Add your extras (avocado, veggies, etc.) for freshness and flavor.\\n6. Spread your sauce on the bread or drizzle it over the filling.\\n7. Stack everything together, season with salt, pepper, or spices, and enjoy!\\n\\n**Optional Upgrades:**\\n- Add a hash brown patty for extra crunch.\\n- Swap regular bacon for thick-cut or maple-glazed bacon.\\n- Use a croissant instead of bread for a buttery, flaky twist.\\n\\nThe ultimate breakfast sandwich is all about balance‚Äîcrunchy, creamy, savory, and a hint of sweetness. Customize it to your taste and make it your own!\" , \"refusal\" : null , \"role\" : \"assistant\" , \"audio\" : null , \"function_call\" : null , \"tool_calls\" : null }, \"matched_stop\" : 1 } ], \"created\" : 1742378836 , \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"chat.completion\" , \"service_tier\" : null , \"system_fingerprint\" : null , \"usage\" : { \"completion_tokens\" : 398 , \"prompt_tokens\" : 10 , \"total_tokens\" : 408 , \"completion_tokens_details\" : null , \"prompt_tokens_details\" : null } } The following snippet demonstrates how to extract the data, log it to the console, and save it to a JSON file. Python def log_response_to_file ( response , filename = \"response_log.json\" ): \"\"\"Logs the full AI response to a JSON file in the same directory as the script.\"\"\" # Extract model name and AI-generated text model_name = response . model text_response = response . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) # Convert response to dictionary response_data = response . model_dump () # Get the script directory script_dir = os . path . dirname ( os . path . abspath ( __file__ )) file_path = os . path . join ( script_dir , filename ) # Write to JSON file with open ( file_path , \"w\" , encoding = \"utf-8\" ) as json_file : json . dump ( response_data , json_file , ensure_ascii = False , indent = 4 ) print ( f \"üíæ Response saved to { file_path } \" ) # Log response to file log_response_to_file ( completion ) For a detailed breakdown of the chat completion object, see the chat completion API reference section. View the complete script Python import json import os from getpass import getpass from openai import OpenAI # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) print ( f \"üì§ Sending a chat completion request to kluster.ai... \\n \" ) # Create chat completion request completion = client . chat . completions . create ( model = \"deepseek-ai/DeepSeek-V3-0324\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" } ] ) def log_response_to_file ( response , filename = \"response_log.json\" ): \"\"\"Logs the full AI response to a JSON file in the same directory as the script.\"\"\" # Extract model name and AI-generated text model_name = response . model text_response = response . choices [ 0 ] . message . content # Print response to console print ( f \" \\n üîç AI response (model: { model_name } ):\" ) print ( text_response ) # Convert response to dictionary response_data = response . model_dump () # Get the script directory script_dir = os . path . dirname ( os . path . abspath ( __file__ )) file_path = os . path . join ( script_dir , filename ) # Write to JSON file with open ( file_path , \"w\" , encoding = \"utf-8\" ) as json_file : json . dump ( response_data , json_file , ensure_ascii = False , indent = 4 ) print ( f \"üíæ Response saved to { file_path } \" ) # Log response to file log_response_to_file ( completion ) Third-party integrations ÔºÉ You can also set up third-party LLM integrations using the kluster.ai API. For step-by-step instructions, check out the following integration guides: SillyTavern - multi-LLM chat interface LangChain - multi-turn conversational agent eliza - create and manage AI agents CrewAI - specialized agents for complex tasks LiteLLM - streaming response and multi-turn conversation handling Summary ÔºÉ You have now experienced the complete real-time inference job lifecycle using kluster.ai's chat completion API. In this guide, you've learned: How to submit a real-rime inference request How to configure real-time inference-related API parameters How to interpret the chat completion object API response The kluster.ai batch API is designed to efficiently and reliably handle your large-scale LLM workloads. If you have questions or suggestions, the support team would love to hear from you.",
      "scraped_at": 1749147203.39215
    },
    "https://docs.kluster.ai/get-started/verify/reliability/dedicated-api/": {
      "url": "https://docs.kluster.ai/get-started/verify/reliability/dedicated-api/",
      "title": "Dedicated reliability endpoint | kluster.ai Docs",
      "content": "Reliability check via the reliability endpoint ÔºÉ The verify/reliability endpoint allows you to validate whether an answer to a specific question contains unreliable information. This approach is ideal for verifying individual responses against the provided context (when the context parameter is included) or general knowledge (when no context is provided). This guide provides a quick example of how use the verify/reliability endpoint for reliability check. Prerequisites ÔºÉ Before getting started with reliability verification, ensure the following requirements are met: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. Endpoint parameters ÔºÉ The verify/reliability endpoint accepts the following input parameters: prompt ( string | required): The question asked or instruction given. output ( string |required): The LLM answer to verify for reliability. context ( string |optional): Reference material to validate against. return_search_results ( boolean |optional): Whether to include search results (default: false). The API returns a JSON object with the following structure: { \"is_hallucination\" : boolea n , \"usage\" : { \"completion_tokens\" : nu mber , \"prompt_tokens\" : nu mber , \"total_tokens\" : nu mber }, \"explanation\" : \"string\" , \"search_results\" : [] // Only included if return_search_results is true } How to use the reliability endpoint ÔºÉ The reliability check feature operates in two distinct modes depending on whether you provide context with your request: General knowledge verification : When no context is provided, the service verifies answers against general knowledge and external sources. Context validation mode : When context is provided, the service only validates answers against the specified context. General knowledge verification ÔºÉ This example checks whether an answer contains unreliable information. As no context is provided, the answer will be verified against general knowledge to identify reliability issues. Python CLI from os import environ import requests from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a reliability check request to kluster.ai... \\n \" ) # Set up request data url = \"https://api.kluster.ai/v1/verify/reliability\" headers = { \"Authorization\" : f \"Bearer { api_key } \" , \"Content-Type\" : \"application/json\" } payload = { \"prompt\" : \"Is earth flat?\" , \"output\" : \"Yes, my friend\" , \"return_search_results\" : False #Optional } # Send the request to the reliability verification endpoint response = requests . post ( url , headers = headers , json = payload ) # Convert the response to JSON result = response . json () # Extract key information is_hallucination = result . get ( \"is_hallucination\" ) explanation = result . get ( \"explanation\" ) # Print whether reliability issue was detected print ( f \" { 'üö®RELIABILITY ISSUE DETECTED' if is_hallucination else '‚úÖNO RELIABILITY ISSUE DETECTED' } \" ) # Print the explanation print ( f \" \\n üß†Explanation: { explanation } \" ) # Print full response print ( f \" \\n üîóAPI Response: { result } \" ) #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a reliability check request to kluster.ai...\\n\" # Submit reliability verification request response = $( curl --location 'https://api.kluster.ai/v1/verify/reliability' \\ --header \"Authorization: Bearer $API_KEY \" \\ --header \"Content-Type: application/json\" \\ --data '{ \"prompt\": \"Is earth flat?\", \"output\": \"Yes, 100%.\", \"return_search_results\": false }' ) # Extract key information is_hallucination = $( echo \" $response \" | jq -r '.is_hallucination' ) explanation = $( echo \" $response \" | jq -r '.explanation' ) # Print whether reliability issue was detected if [[ \" $is_hallucination \" == \"true\" ]] ; then echo -e \"\\nüö® RELIABILITY ISSUE DETECTED\" else echo -e \"\\n‚úÖ NO RELIABILITY ISSUE DETECTED\" fi # Print the explanation echo -e \"\\nüß† Explanation: $explanation \" # Print full response echo -e \"\\nüîó API Response: $response \" Context validation mode ÔºÉ When providing the context parameter, the service will not perform external verification. Instead, it focuses on whether the answer complies with the provided context. RAG applications Ensure the LLM's responses are accurate by using Verify in your Retrieval Augmented Generation (RAG) workflows. This example checks whether an answer is correct based on the provided context. Python CLI from os import environ import requests from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a reliability check request with context to kluster.ai... \\n \" ) # Set up request data url = \"https://api.kluster.ai/v1/verify/reliability\" headers = { \"Authorization\" : f \"Bearer { api_key } \" , \"Content-Type\" : \"application/json\" } payload = { \"prompt\" : \"What's the invoice date?\" , \"output\" : \"The Invoice date is: May 22, 2025 \" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"return_search_results\" : False } # Send the request to the reliability verification endpoint response = requests . post ( url , headers = headers , json = payload ) # Convert the response to JSON result = response . json () # Extract key information is_hallucination = result . get ( \"is_hallucination\" ) explanation = result . get ( \"explanation\" ) # Print whether reliability issue was detected print ( f \" { 'üö®RELIABILITY ISSUE DETECTED' if is_hallucination else '‚úÖNO RELIABILITY ISSUE DETECTED' } \" ) # Print the explanation print ( f \" \\n üß†Explanation: { explanation } \" ) # Print full response print ( f \" \\n üîóAPI Response: { result } \" ) #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a reliability check request with context to kluster.ai...\\n\" # Submit reliability verification request response = $( curl --location 'https://api.kluster.ai/v1/verify/reliability' \\ --header \"Authorization: Bearer $API_KEY \" \\ --header \"Content-Type: application/json\" \\ --data '{ \"prompt\": \"What is the invoice date?\", \"output\": \"The Invoice date is: May 22, 2025 \", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun2 Terms:N30 Ref:PO451C\", \"return_search_results\": true }' ) # Extract key information is_hallucination = $( echo \" $response \" | jq -r '.is_hallucination' ) explanation = $( echo \" $response \" | jq -r '.explanation' ) # Print whether reliability issue was detected if [[ \" $is_hallucination \" == \"true\" ]] ; then echo -e \"\\nüö® RELIABILITY ISSUE DETECTED\" else echo -e \"\\n‚úÖ NO RELIABILITY ISSUE DETECTED\" fi # Print the explanation echo -e \"\\nüß† Explanation: $explanation \" # Print full response echo -e \"\\nüîó API Response: $response \" Best practices ÔºÉ Include relevant context : When validating against specific information, provide comprehensive context. Use domain-specific context : Include authoritative references for specialized knowledge domains. Consider general verification : For widely known information, the service can verify against general knowledge sources. Review explanations : The detailed explanations provide valuable insights into the reasoning process. Next steps ÔºÉ Learn how to use Chat completion reliability verification for evaluating entire conversation histories Review the complete API documentation for detailed endpoint specifications",
      "scraped_at": 1749147202.967036
    },
    "https://docs.kluster.ai/api-reference/reference/#the-request-output-object": {
      "url": "https://docs.kluster.ai/api-reference/reference/#the-request-output-object",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147202.9272182
    },
    "https://docs.kluster.ai/api-reference/reference#create-chat-completion": {
      "url": "https://docs.kluster.ai/api-reference/reference#create-chat-completion",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147204.2877467
    },
    "https://docs.kluster.ai/tutorials/klusterai-api/reliability-check": {
      "url": "https://docs.kluster.ai/tutorials/klusterai-api/reliability-check",
      "title": "Reliability check | kluster.ai Docs",
      "content": "Reliability check with the kluster.ai API ¬∂ Introduction ¬∂ Reliability issues in AI occur when models generate information that appears plausible but is unreliable or unsupported by the provided context. This poses significant risks in production applications, particularly in domains where accuracy is critical. This tutorial demonstrates how to use Verify to identify and prevent reliability issues in your applications. We'll explore available methods: a dedicated API endpoint and via the OpenAI compatible chat completions endpoint. The service can evaluate AI responses based on provided context (perfect for RAG applications) or perform real-time verification against general knowledge. By following this tutorial, you'll learn how to: Verify reliability in individual Q&A pairs. Compare general knowledge verification vs. context validation modes. Validate responses in full conversation histories. Prerequisites ¬∂ Before getting started, ensure you have the following: A kluster.ai account : sign up on the kluster.ai platform if you don't have one A kluster.ai API key : after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide Setup ¬∂ In this notebook, we'll use Python's getpass module to input the key safely. After execution, please provide your unique kluster.ai API key (ensure no spaces). In [1]: Copied! from getpass import getpass api_key = getpass ( \"Enter your kluster.ai API key: \" ) from getpass import getpass api_key = getpass(\"Enter your kluster.ai API key: \") Next, ensure you've installed OpenAI Python and other required libraries: In [2]: Copied! % pip install - q openai requests %pip install -q openai requests Note: you may need to restart the kernel to use updated packages. With the OpenAI Python library installed, we import the necessary dependencies for the tutorial: In [3]: Copied! from openai import OpenAI import time import json import requests from openai import OpenAI import time import json import requests And then, initialize the client by pointing it to the kluster.ai endpoint, and passing your API key. In [4]: Copied! # Define the base URL for both methods base_url_endpoint = \"https://api.kluster.ai/v1/verify/reliability\" #To test with HTTP requests base_url = \"https://api.kluster.ai/v1\" # To test with OpenAI client # Set up the client client = OpenAI ( base_url = base_url_endpoint , api_key = api_key , ) # Define the base URL for both methods base_url_endpoint = \"https://api.kluster.ai/v1/verify/reliability\" #To test with HTTP requests base_url= \"https://api.kluster.ai/v1\" # To test with OpenAI client # Set up the client client = OpenAI( base_url=base_url_endpoint, api_key=api_key, ) Dedicated reliability endpoint ¬∂ The reliability check dedicated endpoint validates whether an answer to a specific question contains unreliable or incorrect information. It operates in two modes: General knowledge verification : when no context is provided, the service verifies answers by comparing it to other sources. Context validation mode : when context is provided, the service only validates answers against that context. For our example, we'll create diverse test cases to demonstrate the reliability check capabilities: General knowledge verification examples : questions where the service verifies against external sources. Context validation examples : scenarios where responses must align with provided context. Search results demonstration : see how enabling return_search_results provides sources used for verification, helping you understand and trust the service's decisions. Invoice extraction example : a practical use case for document processing. Chat completions example : use the convenient OpenAI SDK to check for reliability issues. To call the endpoint, we'll use the following function: In [5]: Copied! # Function that runs the reliability check for general knowledge examples def check_reliability_qa ( prompt , output , context = None , return_search_results = False ): \"\"\"Check reliability using the dedicated endpoint\"\"\" url = base_url_endpoint headers = { \"Authorization\" : f \"Bearer { api_key } \" , \"Content-Type\" : \"application/json\" } # Prepare the payload payload = { \"prompt\" : prompt , \"output\" : output , \"return_search_results\" : return_search_results } # Add context if provided if context : payload [ \"context\" ] = context # Make the POST request to the API response = requests . post ( url , headers = headers , json = payload ) return response . json () # Function that runs the reliability check for general knowledge examples def check_reliability_qa(prompt, output, context=None, return_search_results=False): \"\"\"Check reliability using the dedicated endpoint\"\"\" url = base_url_endpoint headers = { \"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\" } # Prepare the payload payload = { \"prompt\": prompt, \"output\": output, \"return_search_results\": return_search_results } # Add context if provided if context: payload[\"context\"] = context # Make the POST request to the API response = requests.post(url, headers=headers, json=payload) return response.json() Prepare the data ¬∂ In all scenarios, a prompt and output most be provided. The prompt is the message/question from the user, and the output is the answer from the Model. In addition, we are also providing the ground truth in regards to hallucination. In [6]: Copied! # Create test datasets general_knowledge_examples = [ { \"prompt\" : \"What is the capital of France?\" , \"output\" : \"The capital of France is London.\" , \"expected_hallucination\" : True }, { \"prompt\" : \"When was the Eiffel Tower built?\" , \"output\" : \"The Eiffel Tower was built in 1889 for the Paris Exposition.\" , \"expected_hallucination\" : False }, { \"prompt\" : \"Are ghosts real?\" , \"output\" : \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\" , \"expected_hallucination\" : True } ] # Create test datasets general_knowledge_examples = [ { \"prompt\": \"What is the capital of France?\", \"output\": \"The capital of France is London.\", \"expected_hallucination\": True }, { \"prompt\": \"When was the Eiffel Tower built?\", \"output\": \"The Eiffel Tower was built in 1889 for the Paris Exposition.\", \"expected_hallucination\": False }, { \"prompt\": \"Are ghosts real?\", \"output\": \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\", \"expected_hallucination\": True } ] For context validation, the necessary data must be provided via the context field. In [7]: Copied! context_validation_examples = [ { \"prompt\" : \"What's the invoice date?\" , \"output\" : \"The invoice date is May 22, 2025.\" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"expected_hallucination\" : False }, { \"prompt\" : \"What's the total amount on the invoice?\" , \"output\" : \"The total amount is 8500 USD.\" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"expected_hallucination\" : True }, { \"prompt\" : \"Who is the client mentioned in the document?\" , \"output\" : \"The client is Acme.\" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"expected_hallucination\" : False } ] context_validation_examples = [ { \"prompt\": \"What's the invoice date?\", \"output\": \"The invoice date is May 22, 2025.\", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\", \"expected_hallucination\": False }, { \"prompt\": \"What's the total amount on the invoice?\", \"output\": \"The total amount is 8500 USD.\", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\", \"expected_hallucination\": True }, { \"prompt\": \"Who is the client mentioned in the document?\", \"output\": \"The client is Acme.\", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\", \"expected_hallucination\": False } ] General knowledge verification ¬∂ Let's test general knowledge verification mode with our examples: In [8]: Copied! # Test general knowledge verification mode verification_results = [] for i , example in enumerate ( general_knowledge_examples ): print ( f \"=== General Knowledge Verification Example { i + 1 } ===\" ) print ( f \"Question: { example [ 'prompt' ] } \" ) print ( f \"Answer: { example [ 'output' ] } \" ) print ( f \"Expected Unreliable: { example [ 'expected_hallucination' ] } \" ) print () result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], return_search_results = False ) verification_results . append ({ 'example' : example , 'result' : result }) print ( \"Check Result:\" ) print ( f \"Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) print ( f \"Explanation: { result . get ( 'explanation' , 'N/A' ) } \" ) print ( f \"Tokens Used: { result . get ( 'usage' , {}) } \" ) print ( \" \\n \" + \"=\" * 80 + \" \\n \" ) # Test general knowledge verification mode verification_results = [] for i, example in enumerate(general_knowledge_examples): print(f\"=== General Knowledge Verification Example {i+1} ===\") print(f\"Question: {example['prompt']}\") print(f\"Answer: {example['output']}\") print(f\"Expected Unreliable: {example['expected_hallucination']}\") print() result = check_reliability_qa( prompt=example['prompt'], output=example['output'], return_search_results=False ) verification_results.append({ 'example': example, 'result': result }) print(\"Check Result:\") print(f\"Is Unreliable: {result.get('is_hallucination', 'N/A')}\") print(f\"Explanation: {result.get('explanation', 'N/A')}\") print(f\"Tokens Used: {result.get('usage', {})}\") print(\"\\n\" + \"=\"*80 + \"\\n\") === General Knowledge Verification Example 1 === Question: What is the capital of France? Answer: The capital of France is London. Expected Unreliable: True Check Result: Is Unreliable: True Explanation: The user asked for the capital of France. The correct capital of France is Paris, not London. London is the capital of England, not France, making the response factually incorrect. Tokens Used: {'completion_tokens': 118, 'prompt_tokens': 937, 'total_tokens': 1055} ================================================================================ === General Knowledge Verification Example 2 === Question: When was the Eiffel Tower built? Answer: The Eiffel Tower was built in 1889 for the Paris Exposition. Expected Unreliable: False Check Result: Is Unreliable: False Explanation: The response correctly states that the Eiffel Tower was built in 1889. The Eiffel Tower was indeed constructed for the 1889 World's Fair in Paris, making the additional context accurate. The information provided is verifiable and aligns with historical facts about the Eiffel Tower. Tokens Used: {'completion_tokens': 418, 'prompt_tokens': 957, 'total_tokens': 1375} ================================================================================ === General Knowledge Verification Example 3 === Question: Are ghosts real? Answer: Yes, there is a recent scientific study from Harvard that confirms ghosts exist. Expected Unreliable: True Check Result: Is Unreliable: True Explanation: The original user request asks if ghosts are real. The response from the other LLM claims that a recent scientific study from Harvard confirms the existence of ghosts. The search results provide several links related to Harvard and the study of ghosts or supernatural phenomena, but none of them directly confirm the existence of ghosts. The snippets from the search results indicate that Harvard has conducted studies and courses on the topic of ghosts and supernatural phenomena, but these are primarily focused on folklore, mythology, and the cultural or psychological aspects of belief in ghosts. There is no clear evidence in the search results of a scientific study from Harvard that confirms the existence of ghosts. The response from the other LLM is an example of hallucination because it presents a factual claim (a recent scientific study from Harvard confirming ghosts exist) that is not supported by the search results. Tokens Used: {'completion_tokens': 282, 'prompt_tokens': 1744, 'total_tokens': 2026} ================================================================================ Enable search results ¬∂ When enabling the property return_search_results=true , the reliability check feature will return the sources used for the verification. In [9]: Copied! # Test general knowledge verification with search results enabled print ( \"=== General Knowledge Verification with Search Results ===\" ) example = { \"prompt\" : \"Are ghosts real?\" , \"output\" : \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\" } # Let's run the check with search results enabled result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], return_search_results = True # Enable search results ) # Display the result print ( f \"Question: { example [ 'prompt' ] } \" ) print ( f \"Answer: { example [ 'output' ] } \" ) print ( f \" \\n Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) # Display search results if 'search_results' in result and result [ 'search_results' ]: print ( f \" \\n üìö Search Results Used ( { len ( result [ 'search_results' ]) } sources):\" ) for idx , source in enumerate ( result [ 'search_results' ][: 5 ], 1 ): # Show first 5 print ( f \" \\n { idx } . { source . get ( 'title' , 'No title' ) } \" ) print ( f \" üìÑ { source . get ( 'snippet' , 'No snippet' )[: 150 ] } ...\" ) print ( f \" üîó { source . get ( 'link' , 'No link' ) } \" ) else : print ( \" \\n No search results returned\" ) print ( f \" \\n Tokens Used: { result . get ( 'usage' , {}) } \" ) # Test general knowledge verification with search results enabled print(\"=== General Knowledge Verification with Search Results ===\") example = { \"prompt\": \"Are ghosts real?\", \"output\": \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\" } # Let's run the check with search results enabled result = check_reliability_qa( prompt=example['prompt'], output=example['output'], return_search_results=True # Enable search results ) # Display the result print(f\"Question: {example['prompt']}\") print(f\"Answer: {example['output']}\") print(f\"\\nIs Unreliable: {result.get('is_hallucination', 'N/A')}\") # Display search results if 'search_results' in result and result['search_results']: print(f\"\\nüìö Search Results Used ({len(result['search_results'])} sources):\") for idx, source in enumerate(result['search_results'][:5], 1): # Show first 5 print(f\"\\n{idx}. {source.get('title', 'No title')}\") print(f\" üìÑ {source.get('snippet', 'No snippet')[:150]}...\") print(f\" üîó {source.get('link', 'No link')}\") else: print(\"\\nNo search results returned\") print(f\"\\nTokens Used: {result.get('usage', {})}\") === General Knowledge Verification with Search Results === Question: Are ghosts real? Answer: Yes, there is a recent scientific study from Harvard that confirms ghosts exist. Is Unreliable: True üìö Search Results Used (10 sources): 1. The Allure of the Supernatural | Harvard Independent üìÑ Focusing in on ghosts and other such spirits, the study revealed a greater proportion of belief in the mystical than the national averages ...... üîó https://harvardindependent.com/the-allure-of-the-supernatural/ 2. Harvard class studies supernatural stories üìÑ Folklore & Mythology course examines how tales of spirits and ghosts from the past affect the present and the future.... üîó https://news.harvard.edu/gazette/story/2021/10/harvard-class-studies-supernatural-stories/ 3. The Ghost Studies: New Perspectives on the Origins of Paranormal ... üìÑ New and exciting scientific theories that explain apparitions, hauntings, and communications from the dead.... üîó https://www.harvard.com/book/9781632651211 4. Did Scientists Just Discover the Cause of Ghost Sightings? | Unveiled üìÑ Ghosts & the Afterlife: Science Unveils the Mystery of Spirits ¬∑ Ghosts Aren't Real: 4 Scientific Explanations for Paranormal Activity ¬∑ Harvard ...... üîó https://www.youtube.com/watch?v=fuFOGYxb6bI 5. The Ivy and the Occult | Harvard Independent üìÑ While ghost stories and psychical research seem to have largely disappeared from Harvard over the years, there is still an eclectic mix of ...... üîó https://harvardindependent.com/the-ivy-and-the-occult/ Tokens Used: {'completion_tokens': 308, 'prompt_tokens': 1778, 'total_tokens': 2086} Context validation mode ¬∂ The context validation mode uses the context property as the ground truth. When enabled, the service does not verify the answer using external knowledge; instead, it focuses on identifying reliability issues based solely on the information within the provided context . In [10]: Copied! # Test context validation mode context_results = [] # for i , example in enumerate ( context_validation_examples ): print ( f \"=== Context Validation Example { i + 1 } ===\" ) print ( f \"Context: { example [ 'context' ] } \" ) print ( f \"Question: { example [ 'prompt' ] } \" ) print ( f \"Answer: { example [ 'output' ] } \" ) print ( f \"Expected Unreliable: { example [ 'expected_hallucination' ] } \" ) print () # Run the reliability check with context result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], context = example [ 'context' ], return_search_results = False ) context_results . append ({ 'example' : example , 'result' : result }) # Display the results print ( \"Check Result:\" ) print ( f \"Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) print ( f \"Explanation: { result . get ( 'explanation' , 'N/A' ) } \" ) print ( f \"Tokens Used: { result . get ( 'usage' , {}) } \" ) print ( \" \\n \" + \"=\" * 80 + \" \\n \" ) # Test context validation mode context_results = [] # for i, example in enumerate(context_validation_examples): print(f\"=== Context Validation Example {i+1} ===\") print(f\"Context: {example['context']}\") print(f\"Question: {example['prompt']}\") print(f\"Answer: {example['output']}\") print(f\"Expected Unreliable: {example['expected_hallucination']}\") print() # Run the reliability check with context result = check_reliability_qa( prompt=example['prompt'], output=example['output'], context=example['context'], return_search_results=False ) context_results.append({ 'example': example, 'result': result }) # Display the results print(\"Check Result:\") print(f\"Is Unreliable: {result.get('is_hallucination', 'N/A')}\") print(f\"Explanation: {result.get('explanation', 'N/A')}\") print(f\"Tokens Used: {result.get('usage', {})}\") print(\"\\n\" + \"=\"*80 + \"\\n\") === Context Validation Example 1 === Context: InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C Question: What's the invoice date? Answer: The invoice date is May 22, 2025. Expected Unreliable: False Check Result: Is Unreliable: False Explanation: The answer accurately reflects the information given in the document regarding the invoice date, making a reasonable assumption about the abbreviated year. Tokens Used: {'completion_tokens': 438, 'prompt_tokens': 267, 'total_tokens': 705} ================================================================================ === Context Validation Example 2 === Context: InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C Question: What's the total amount on the invoice? Answer: The total amount is 8500 USD. Expected Unreliable: True Check Result: Is Unreliable: True Explanation: The answer contradicts the document by stating a different amount and currency. Tokens Used: {'completion_tokens': 426, 'prompt_tokens': 267, 'total_tokens': 693} ================================================================================ === Context Validation Example 3 === Context: InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C Question: Who is the client mentioned in the document? Answer: The client is Acme. Expected Unreliable: False Check Result: Is Unreliable: False Explanation: To determine whether the answer is faithful to the contents of the document, we need to analyze the provided information. The document contains a specific entry: \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\". Within this entry, it is explicitly stated that the \"Client:Acme\". The question asks, \"Who is the client mentioned in the document?\" The answer provided is \"The client is Acme.\" To assess the faithfulness of the answer to the document: 1. The document directly states that the client is \"Acme\". 2. The answer directly corresponds to this information by stating \"The client is Acme\". 3. There is no additional information introduced in the answer that is not present in the document. 4. The answer does not contradict any information provided in the document. Given these observations, the answer accurately reflects the information contained within the document. Therefore, the verdict is: {\"REASONING\": \"The answer directly corresponds to the information provided in the document without introducing new information or contradicting existing information.\", \"HALLUCINATION\": 0} Tokens Used: {'completion_tokens': 246, 'prompt_tokens': 265, 'total_tokens': 511} ================================================================================ Extended context ¬∂ A very common use case is document extraction. Let's see how a lengthy invoice used as context helps us to check if our model is producing reliable output. In [11]: Copied! # Invoice Example invoice = ''' { \"invoiceId\": \"INV-20250523-XG-74920B\", \"orderReference\": \"ORD-PROC-Q2-2025-ALPHA-99374-DELTA\", \"customerIdentification\": \"CUST-EAGLECORP-GLOBAL-007\", \"dateIssued\": \"2025-05-23\", \"dueDate\": \"2025-06-22\", \"paymentTerms\": \"Net 30 Days\", \"currency\": \"USD\", \"issuerDetails\": { \"companyName\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd.\", \"taxId\": \"VAT-GB-293847261\", \"registrationNumber\": \"REG-LND-09876543X\", \"address\": { \"street\": \"121B, Innovation Drive, Silicon Roundabout, Tech City East\", \"city\": \"London\", \"postalCode\": \"EC1Y 8XZ\", \"country\": \"United Kingdom\", \"planet\": \"Earth\", \"dimension\": \"Sigma-7\" }, \"contact\": { \"primaryPhone\": \"+44-20-7946-0001 ext. 777\", \"secondaryPhone\": \"+44-20-7946-0002\", \"fax\": \"+44-20-7946-0003\", \"email\": \"billing@quantumsynergistics-ans.co.uk\", \"website\": \"www.quantumsynergistics-ans.co.uk\" }, \"bankDetails\": { \"bankName\": \"Universal Interstellar Bank PLC\", \"accountName\": \"Quantum Synergistics & ANS Ltd.\", \"accountNumber\": \"9876543210123456\", \"swiftBic\": \"UNIVGB2LXXX\", \"iban\": \"GB29 UNIV 9876 5432 1012 3456 78\", \"reference\": \"INV-20250523-XG-74920B\" } }, \"billingInformation\": { \"companyName\": \"EagleCorp Global Holdings Inc. & Subsidiaries\", \"department\": \"Strategic Procurement & Interstellar Logistics Division\", \"attentionTo\": \"Ms. Evelyn Reed, Chief Procurement Officer (CPO)\", \"taxId\": \"EIN-US-98-7654321X\", \"clientReferenceId\": \"EGL-PROC-REF-Q2-2025-7734-GAMMA\", \"address\": { \"street\": \"Suite 9870, Eagle Tower One, 1500 Constitution Avenue NW\", \"city\": \"Washington D.C.\", \"state\": \"District of Columbia\", \"postalCode\": \"20001-1500\", \"country\": \"United States of America\" }, \"contact\": { \"phone\": \"+1-202-555-0189 ext. 1234\", \"email\": \"e.reed.procurement@eaglecorpglobal.com\" } }, \"shippingInformation\": [ { \"shipmentId\": \"SHIP-ALPHA-001-XG74920B\", \"recipientName\": \"Dr. Aris Thorne, Head of R&D\", \"facilityName\": \"EagleCorp Advanced Research Facility - Sector Gamma-7\", \"address\": { \"street\": \"Docking Bay 7, 47 Industrial Park Road\", \"city\": \"New Chicago\", \"state\": \"Illinois\", \"postalCode\": \"60699-0047\", \"country\": \"United States of America\", \"deliveryZone\": \"Restricted Access - Level 3 Clearance Required\" }, \"shippingMethod\": \"Cryo-Stasis Freight - Priority Overnight\", \"trackingNumber\": \"TRK-CSFPON-9988776655-A01\", \"notes\": \"Deliver between 08:00 - 10:00 Local Time. Handle with Extreme Care. Temperature Sensitive Materials.\" }, { \"shipmentId\": \"SHIP-BETA-002-XG74920B\", \"recipientName\": \"Mr. Jian Li, Operations Manager\", \"facilityName\": \"EagleCorp Manufacturing Plant - Unit 42\", \"address\": { \"street\": \"88 Manufacturing Drive, Innovation Valley Industrial Estate\", \"city\": \"Shenzhen\", \"province\": \"Guangdong\", \"postalCode\": \"518000\", \"country\": \"China\", \"deliveryZone\": \"Loading Dock B - Heavy Goods\" }, \"shippingMethod\": \"Secure Air Cargo - Expedited\", \"trackingNumber\": \"TRK-SACEXP-CN7766554433-B02\", \"notes\": \"Requires Forklift. Confirm delivery appointment 24hrs prior.\" } ], \"lineItems\": [ { \"itemId\": \"QN-CORE-X9000-PRO\", \"productCode\": \"PQC-SYS-001A-REV4\", \"description\": \"Quantum Entanglement Core Processor - Model X9000 Professional Edition. Includes integrated cryo-cooler and temporal displacement shielding. Firmware v7.8.2-alpha.\", \"servicePeriod\": \"N/A\", \"quantity\": 2, \"unit\": \"Unit(s)\", \"unitPrice\": 750000.00, \"discountPercentage\": 5.0, \"discountAmount\": 75000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 285000.00, \"subtotal\": 1425000.00, \"totalLineAmount\": 1710000.00, \"serialNumbers\": [\"SN-QECX9P-0000A1F8\", \"SN-QECX9P-0000A2C4\"], \"warrantyId\": \"WARR-QECX9P-5YR-PREM-001\" }, { \"itemId\": \"NANO-FAB-M7-ULTRA\", \"productCode\": \"NFM-DEV-007B-REV2\", \"description\": \"Advanced Nanite Fabricator - Model M7 Ultra. High precision, multi-material capability. Includes 12-month software subscription (Tier 1).\", \"servicePeriod\": \"N/A\", \"quantity\": 1, \"unit\": \"System\", \"unitPrice\": 1250000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 250000.00, \"subtotal\": 1250000.00, \"totalLineAmount\": 1500000.00, \"serialNumbers\": [\"SN-NFM7U-XYZ001B\"], \"warrantyId\": \"WARR-NFM7U-3YR-STD-002\" }, { \"itemId\": \"SVC-CONSULT-QIP-PH1\", \"productCode\": \"CS-QIP-001-PHASE1\", \"description\": \"Quantum Implementation Project - Phase 1 Consultation Services. On-site engineering support, system integration planning, and initial staff training (400 hours block).\", \"servicePeriod\": \"2025-06-01 to 2025-08-31\", \"quantity\": 400, \"unit\": \"Hour(s)\", \"unitPrice\": 850.00, \"discountPercentage\": 10.0, \"discountAmount\": 34000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 306000.00, \"totalLineAmount\": 306000.00, \"projectCode\": \"PROJ-EAGLE-QIP-2025\", \"consultantId\": [\"CONS-DR-EVA-ROSTOVA\", \"CONS-RAJ-SINGH-ENG\"] }, { \"itemId\": \"MAT-CRYOFLUID-XF100\", \"productCode\": \"CHEM-CRYO-003C\", \"description\": \"Cryogenic Cooling Fluid - Type XF-100. Ultra-low temperature stability. Non-conductive. (Sold in 200L insulated containers)\", \"servicePeriod\": \"N/A\", \"quantity\": 10, \"unit\": \"Container(s)\", \"unitPrice\": 15000.00, \"discountPercentage\": 2.5, \"discountAmount\": 3750.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 29250.00, \"subtotal\": 146250.00, \"totalLineAmount\": 175500.00, \"batchNumbers\": [\"BATCH-XF100-2501A01\", \"BATCH-XF100-2501A02\", \"BATCH-XF100-2501A03\", \"BATCH-XF100-2501A04\", \"BATCH-XF100-2501A05\", \"BATCH-XF100-2501A06\", \"BATCH-XF100-2501A07\", \"BATCH-XF100-2501A08\", \"BATCH-XF100-2501A09\", \"BATCH-XF100-2501A10\"], \"shelfLife\": \"24 Months from DOM\" }, { \"itemId\": \"SOFT-LICENSE-QAI-ENT\", \"productCode\": \"SL-QAI-ENT-001-5YR\", \"description\": \"Quantum AI Algorithmic Suite - Enterprise License. 5-Year Subscription. Unlimited User Access. Includes Premium Support Package (PSP-GOLD-001).\", \"servicePeriod\": \"2025-06-01 to 2030-05-31\", \"quantity\": 1, \"unit\": \"License\", \"unitPrice\": 450000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 450000.00, \"totalLineAmount\": 450000.00, \"licenseKey\": \"LIC-QAIENT-XG74920B-ABC123XYZ789-EAGLECORP\", \"supportContractId\": \"SUP-PSP-GOLD-001-XG74920B\" }, { \"itemId\": \"COMP-SENSOR-ARRAY-SIGMA\", \"productCode\": \"SNS-ARR-SGM-004D\", \"description\": \"Multi-Dimensional Sensor Array - Sigma Series. High-sensitivity, wide spectrum coverage. Includes calibration certificate traceable to NIST/NPL.\", \"servicePeriod\": \"N/A\", \"quantity\": 8, \"unit\": \"Unit(s)\", \"unitPrice\": 22000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 35200.00, \"subtotal\": 176000.00, \"totalLineAmount\": 211200.00, \"serialNumbers\": [\"SN-MDSA-SGM-0101\", \"SN-MDSA-SGM-0102\", \"SN-MDSA-SGM-0103\", \"SN-MDSA-SGM-0104\", \"SN-MDSA-SGM-0105\", \"SN-MDSA-SGM-0106\", \"SN-MDSA-SGM-0107\", \"SN-MDSA-SGM-0108\"], \"calibrationDate\": \"2025-05-15\" }, { \"itemId\": \"MAINT-KIT-ADV-ROBOTICS\", \"productCode\": \"MNT-KIT-ROBO-002A\", \"description\": \"Advanced Robotics Maintenance Toolkit. Includes specialized diagnostic tools and Class-5 cleanroom consumables. For AR-700 and AR-800 series.\", \"servicePeriod\": \"N/A\", \"quantity\": 5, \"unit\": \"Kit(s)\", \"unitPrice\": 7500.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 7500.00, \"subtotal\": 37500.00, \"totalLineAmount\": 45000.00, \"componentListId\": \"CL-MNTROBO-002A-V3\" }, { \"itemId\": \"DATA-STORAGE-CRYSTAL-1PB\", \"productCode\": \"DSC-1PB-HG-009\", \"description\": \"Holographic Data Storage Crystal - 1 Petabyte Capacity. Archival Grade. Read/Write Speed: 50 GB/s. Phase-change matrix type.\", \"servicePeriod\": \"N/A\", \"quantity\": 20, \"unit\": \"Crystal(s)\", \"unitPrice\": 18000.00, \"discountPercentage\": 10.0, \"discountAmount\": 36000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 64800.00, \"subtotal\": 324000.00, \"totalLineAmount\": 388800.00, \"serialNumbers\": [\"SN-DSC1PB-HG-A001F to SN-DSC1PB-HG-A001P\", \"SN-DSC1PB-HG-B002A to SN-DSC1PB-HG-B002D\"], \"dataIntegrityCert\": \"DIC-HG9-20250520-BATCH01\" } ], \"summary\": { \"subtotalBeforeDiscounts\": 4128500.00, \"totalDiscountAmount\": 148750.00, \"subtotalAfterDiscounts\": 3979750.00, \"totalTaxAmount\": 671750.00, \"shippingAndHandling\": [ { \"description\": \"Cryo-Stasis Freight - Priority Overnight (SHIP-ALPHA-001)\", \"chargeCode\": \"SHP-CRYO-PRIO-INTL\", \"amount\": 12500.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Secure Air Cargo - Expedited (SHIP-BETA-002)\", \"chargeCode\": \"SHP-SAC-EXP-CN\", \"amount\": 8800.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Special Handling - Temperature Sensitive & High Value Goods\", \"chargeCode\": \"HDL-SPECREQ-HVTS\", \"amount\": 5500.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 1100.00 }, { \"description\": \"Customs Clearance & Documentation Fee - International\", \"chargeCode\": \"FEE-CUSTOMS-INTL-001\", \"amount\": 2750.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Transit Insurance - Full Value Coverage\", \"chargeCode\": \"INS-TRANSIT-FULL-XG74920B\", \"amount\": 25000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 } ], \"totalShippingAndHandling\": 54550.00, \"totalShippingAndHandlingTax\": 1100.00, \"grandTotal\": 4707150.00, \"amountPaid\": 0.00, \"amountDue\": 4707150.00 }, \"paymentInstructions\": { \"preferredMethod\": \"Wire Transfer\", \"paymentReference\": \"INV-20250523-XG-74920B / CUST-EAGLECORP-GLOBAL-007\", \"latePaymentPenalty\": \"1.5% per month on outstanding balance after due date.\", \"earlyPaymentDiscount\": \"1 % d iscount if paid within 10 days (Amount: $47071.50, New Total: $4660078.50). Reference EPD-XG74920B if claiming.\", \"alternativePayments\": [ { \"method\": \"Secured Crypto Transfer (USDC or ETH)\", \"details\": \"Wallet Address: 0x1234ABCD5678EFGH9012IJKL3456MNOP7890QRST. Memo: XG74920B. Confirmation required via secure_payments@quantumsynergistics-ans.co.uk\" }, { \"method\": \"Irrevocable Letter of Credit (ILOC)\", \"details\": \"To be issued by a Prime Bank, acceptable to Quantum Synergistics. Contact accounts_receivable@quantumsynergistics-ans.co.uk for ILOC requirements.\" } ] }, \"notesAndRemarks\": [ \"All hardware components are subject to export control regulations (EAR/ITAR where applicable). Compliance documentation attached separately (DOC-REF: EXPCOMPL-XG74920B).\", \"Software licenses are non-transferable and subject to the End User License Agreement (EULA-QSANS-V4.2).\", \"On-site consultation hours are estimates. Additional hours will be billed separately under addendum A1 of contract CS-QIP-001.\", \"Warranty claims must be submitted via the online portal at support.quantumsynergistics-ans.co.uk using the provided Warranty IDs.\", \"Return Material Authorization (RMA) required for all returns. Contact customer support for RMA number. Restocking fees may apply (15-25% based on product type and condition). See detailed Return Policy (POL-RET-QSANS-2025-V2).\", \"Projected delivery dates for back-ordered sub-components (Ref: SUBCOMP-BO-LIST-XG74920B-01) will be communicated by your account manager within 7 business days.\" ], \"attachments\": [ {\"documentName\": \"QSANS_Product_Specification_Sheets_Q2_2025.pdf\", \"fileId\": \"DOC-SPECS-QSANS-Q22025-V1.3\"}, {\"documentName\": \"EULA_QSANS_Software_V4.2.pdf\", \"fileId\": \"DOC-EULA-QSANS-V4.2\"}, {\"documentName\": \"Warranty_Terms_and_Conditions_Premium_Standard.pdf\", \"fileId\": \"DOC-WARR-QSANS-PREMSTD-V3.1\"}, {\"documentName\": \"Export_Compliance_Declaration_XG74920B.pdf\", \"fileId\": \"DOC-EXPCOMPL-XG74920B\"}, {\"documentName\": \"Return_Policy_QSANS_2025_V2.pdf\", \"fileId\": \"DOC-POL-RET-QSANS-2025-V2\"}, {\"documentName\": \"Consultation_Services_SOW_PROJ-EAGLE-QIP-2025.pdf\", \"fileId\": \"DOC-SOW-EAGLE-QIP-2025-PH1\"} ], \"approvalWorkflow\": { \"issuerApproval\": { \"approverName\": \"Mr. Alistair Finch\", \"approverTitle\": \"Head of Commercial Operations\", \"approvalDate\": \"2025-05-23\", \"signatureId\": \"SIG-AFINCH-QSANS-20250523-001A\" }, \"clientAcknowledgmentRequired\": true, \"clientAcknowledgmentInstructions\": \"Please sign and return a copy of this invoice or confirm receipt and acceptance via email to billing@quantumsynergistics-ans.co.uk within 5 business days.\" }, \"versionHistory\": [ {\"version\": 1.0, \"date\": \"2025-05-23\", \"reason\": \"Initial Draft\", \"editorId\": \"SYS-AUTOINV-GEN\"}, {\"version\": 1.1, \"date\": \"2025-05-23\", \"reason\": \"Added shipping details and corrected tax calculation for item QN-CORE-X9000-PRO.\", \"editorId\": \"USER-CFO-REVIEW-BOT\"} ], \"footerMessage\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd. - Pioneering the Future, Today. Thank you for your business. For support, please visit our dedicated portal or contact your account representative. All transactions are governed by the laws of England and Wales. Registered Office: 121B, Innovation Drive, London, EC1Y 8XZ, UK. Company Reg No: REG-LND-09876543X. VAT No: VAT-GB-293847261.\" } ''' # Invoice Example invoice=''' { \"invoiceId\": \"INV-20250523-XG-74920B\", \"orderReference\": \"ORD-PROC-Q2-2025-ALPHA-99374-DELTA\", \"customerIdentification\": \"CUST-EAGLECORP-GLOBAL-007\", \"dateIssued\": \"2025-05-23\", \"dueDate\": \"2025-06-22\", \"paymentTerms\": \"Net 30 Days\", \"currency\": \"USD\", \"issuerDetails\": { \"companyName\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd.\", \"taxId\": \"VAT-GB-293847261\", \"registrationNumber\": \"REG-LND-09876543X\", \"address\": { \"street\": \"121B, Innovation Drive, Silicon Roundabout, Tech City East\", \"city\": \"London\", \"postalCode\": \"EC1Y 8XZ\", \"country\": \"United Kingdom\", \"planet\": \"Earth\", \"dimension\": \"Sigma-7\" }, \"contact\": { \"primaryPhone\": \"+44-20-7946-0001 ext. 777\", \"secondaryPhone\": \"+44-20-7946-0002\", \"fax\": \"+44-20-7946-0003\", \"email\": \"billing@quantumsynergistics-ans.co.uk\", \"website\": \"www.quantumsynergistics-ans.co.uk\" }, \"bankDetails\": { \"bankName\": \"Universal Interstellar Bank PLC\", \"accountName\": \"Quantum Synergistics & ANS Ltd.\", \"accountNumber\": \"9876543210123456\", \"swiftBic\": \"UNIVGB2LXXX\", \"iban\": \"GB29 UNIV 9876 5432 1012 3456 78\", \"reference\": \"INV-20250523-XG-74920B\" } }, \"billingInformation\": { \"companyName\": \"EagleCorp Global Holdings Inc. & Subsidiaries\", \"department\": \"Strategic Procurement & Interstellar Logistics Division\", \"attentionTo\": \"Ms. Evelyn Reed, Chief Procurement Officer (CPO)\", \"taxId\": \"EIN-US-98-7654321X\", \"clientReferenceId\": \"EGL-PROC-REF-Q2-2025-7734-GAMMA\", \"address\": { \"street\": \"Suite 9870, Eagle Tower One, 1500 Constitution Avenue NW\", \"city\": \"Washington D.C.\", \"state\": \"District of Columbia\", \"postalCode\": \"20001-1500\", \"country\": \"United States of America\" }, \"contact\": { \"phone\": \"+1-202-555-0189 ext. 1234\", \"email\": \"e.reed.procurement@eaglecorpglobal.com\" } }, \"shippingInformation\": [ { \"shipmentId\": \"SHIP-ALPHA-001-XG74920B\", \"recipientName\": \"Dr. Aris Thorne, Head of R&D\", \"facilityName\": \"EagleCorp Advanced Research Facility - Sector Gamma-7\", \"address\": { \"street\": \"Docking Bay 7, 47 Industrial Park Road\", \"city\": \"New Chicago\", \"state\": \"Illinois\", \"postalCode\": \"60699-0047\", \"country\": \"United States of America\", \"deliveryZone\": \"Restricted Access - Level 3 Clearance Required\" }, \"shippingMethod\": \"Cryo-Stasis Freight - Priority Overnight\", \"trackingNumber\": \"TRK-CSFPON-9988776655-A01\", \"notes\": \"Deliver between 08:00 - 10:00 Local Time. Handle with Extreme Care. Temperature Sensitive Materials.\" }, { \"shipmentId\": \"SHIP-BETA-002-XG74920B\", \"recipientName\": \"Mr. Jian Li, Operations Manager\", \"facilityName\": \"EagleCorp Manufacturing Plant - Unit 42\", \"address\": { \"street\": \"88 Manufacturing Drive, Innovation Valley Industrial Estate\", \"city\": \"Shenzhen\", \"province\": \"Guangdong\", \"postalCode\": \"518000\", \"country\": \"China\", \"deliveryZone\": \"Loading Dock B - Heavy Goods\" }, \"shippingMethod\": \"Secure Air Cargo - Expedited\", \"trackingNumber\": \"TRK-SACEXP-CN7766554433-B02\", \"notes\": \"Requires Forklift. Confirm delivery appointment 24hrs prior.\" } ], \"lineItems\": [ { \"itemId\": \"QN-CORE-X9000-PRO\", \"productCode\": \"PQC-SYS-001A-REV4\", \"description\": \"Quantum Entanglement Core Processor - Model X9000 Professional Edition. Includes integrated cryo-cooler and temporal displacement shielding. Firmware v7.8.2-alpha.\", \"servicePeriod\": \"N/A\", \"quantity\": 2, \"unit\": \"Unit(s)\", \"unitPrice\": 750000.00, \"discountPercentage\": 5.0, \"discountAmount\": 75000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 285000.00, \"subtotal\": 1425000.00, \"totalLineAmount\": 1710000.00, \"serialNumbers\": [\"SN-QECX9P-0000A1F8\", \"SN-QECX9P-0000A2C4\"], \"warrantyId\": \"WARR-QECX9P-5YR-PREM-001\" }, { \"itemId\": \"NANO-FAB-M7-ULTRA\", \"productCode\": \"NFM-DEV-007B-REV2\", \"description\": \"Advanced Nanite Fabricator - Model M7 Ultra. High precision, multi-material capability. Includes 12-month software subscription (Tier 1).\", \"servicePeriod\": \"N/A\", \"quantity\": 1, \"unit\": \"System\", \"unitPrice\": 1250000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 250000.00, \"subtotal\": 1250000.00, \"totalLineAmount\": 1500000.00, \"serialNumbers\": [\"SN-NFM7U-XYZ001B\"], \"warrantyId\": \"WARR-NFM7U-3YR-STD-002\" }, { \"itemId\": \"SVC-CONSULT-QIP-PH1\", \"productCode\": \"CS-QIP-001-PHASE1\", \"description\": \"Quantum Implementation Project - Phase 1 Consultation Services. On-site engineering support, system integration planning, and initial staff training (400 hours block).\", \"servicePeriod\": \"2025-06-01 to 2025-08-31\", \"quantity\": 400, \"unit\": \"Hour(s)\", \"unitPrice\": 850.00, \"discountPercentage\": 10.0, \"discountAmount\": 34000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 306000.00, \"totalLineAmount\": 306000.00, \"projectCode\": \"PROJ-EAGLE-QIP-2025\", \"consultantId\": [\"CONS-DR-EVA-ROSTOVA\", \"CONS-RAJ-SINGH-ENG\"] }, { \"itemId\": \"MAT-CRYOFLUID-XF100\", \"productCode\": \"CHEM-CRYO-003C\", \"description\": \"Cryogenic Cooling Fluid - Type XF-100. Ultra-low temperature stability. Non-conductive. (Sold in 200L insulated containers)\", \"servicePeriod\": \"N/A\", \"quantity\": 10, \"unit\": \"Container(s)\", \"unitPrice\": 15000.00, \"discountPercentage\": 2.5, \"discountAmount\": 3750.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 29250.00, \"subtotal\": 146250.00, \"totalLineAmount\": 175500.00, \"batchNumbers\": [\"BATCH-XF100-2501A01\", \"BATCH-XF100-2501A02\", \"BATCH-XF100-2501A03\", \"BATCH-XF100-2501A04\", \"BATCH-XF100-2501A05\", \"BATCH-XF100-2501A06\", \"BATCH-XF100-2501A07\", \"BATCH-XF100-2501A08\", \"BATCH-XF100-2501A09\", \"BATCH-XF100-2501A10\"], \"shelfLife\": \"24 Months from DOM\" }, { \"itemId\": \"SOFT-LICENSE-QAI-ENT\", \"productCode\": \"SL-QAI-ENT-001-5YR\", \"description\": \"Quantum AI Algorithmic Suite - Enterprise License. 5-Year Subscription. Unlimited User Access. Includes Premium Support Package (PSP-GOLD-001).\", \"servicePeriod\": \"2025-06-01 to 2030-05-31\", \"quantity\": 1, \"unit\": \"License\", \"unitPrice\": 450000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 450000.00, \"totalLineAmount\": 450000.00, \"licenseKey\": \"LIC-QAIENT-XG74920B-ABC123XYZ789-EAGLECORP\", \"supportContractId\": \"SUP-PSP-GOLD-001-XG74920B\" }, { \"itemId\": \"COMP-SENSOR-ARRAY-SIGMA\", \"productCode\": \"SNS-ARR-SGM-004D\", \"description\": \"Multi-Dimensional Sensor Array - Sigma Series. High-sensitivity, wide spectrum coverage. Includes calibration certificate traceable to NIST/NPL.\", \"servicePeriod\": \"N/A\", \"quantity\": 8, \"unit\": \"Unit(s)\", \"unitPrice\": 22000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 35200.00, \"subtotal\": 176000.00, \"totalLineAmount\": 211200.00, \"serialNumbers\": [\"SN-MDSA-SGM-0101\", \"SN-MDSA-SGM-0102\", \"SN-MDSA-SGM-0103\", \"SN-MDSA-SGM-0104\", \"SN-MDSA-SGM-0105\", \"SN-MDSA-SGM-0106\", \"SN-MDSA-SGM-0107\", \"SN-MDSA-SGM-0108\"], \"calibrationDate\": \"2025-05-15\" }, { \"itemId\": \"MAINT-KIT-ADV-ROBOTICS\", \"productCode\": \"MNT-KIT-ROBO-002A\", \"description\": \"Advanced Robotics Maintenance Toolkit. Includes specialized diagnostic tools and Class-5 cleanroom consumables. For AR-700 and AR-800 series.\", \"servicePeriod\": \"N/A\", \"quantity\": 5, \"unit\": \"Kit(s)\", \"unitPrice\": 7500.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 7500.00, \"subtotal\": 37500.00, \"totalLineAmount\": 45000.00, \"componentListId\": \"CL-MNTROBO-002A-V3\" }, { \"itemId\": \"DATA-STORAGE-CRYSTAL-1PB\", \"productCode\": \"DSC-1PB-HG-009\", \"description\": \"Holographic Data Storage Crystal - 1 Petabyte Capacity. Archival Grade. Read/Write Speed: 50 GB/s. Phase-change matrix type.\", \"servicePeriod\": \"N/A\", \"quantity\": 20, \"unit\": \"Crystal(s)\", \"unitPrice\": 18000.00, \"discountPercentage\": 10.0, \"discountAmount\": 36000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 64800.00, \"subtotal\": 324000.00, \"totalLineAmount\": 388800.00, \"serialNumbers\": [\"SN-DSC1PB-HG-A001F to SN-DSC1PB-HG-A001P\", \"SN-DSC1PB-HG-B002A to SN-DSC1PB-HG-B002D\"], \"dataIntegrityCert\": \"DIC-HG9-20250520-BATCH01\" } ], \"summary\": { \"subtotalBeforeDiscounts\": 4128500.00, \"totalDiscountAmount\": 148750.00, \"subtotalAfterDiscounts\": 3979750.00, \"totalTaxAmount\": 671750.00, \"shippingAndHandling\": [ { \"description\": \"Cryo-Stasis Freight - Priority Overnight (SHIP-ALPHA-001)\", \"chargeCode\": \"SHP-CRYO-PRIO-INTL\", \"amount\": 12500.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Secure Air Cargo - Expedited (SHIP-BETA-002)\", \"chargeCode\": \"SHP-SAC-EXP-CN\", \"amount\": 8800.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Special Handling - Temperature Sensitive & High Value Goods\", \"chargeCode\": \"HDL-SPECREQ-HVTS\", \"amount\": 5500.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 1100.00 }, { \"description\": \"Customs Clearance & Documentation Fee - International\", \"chargeCode\": \"FEE-CUSTOMS-INTL-001\", \"amount\": 2750.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Transit Insurance - Full Value Coverage\", \"chargeCode\": \"INS-TRANSIT-FULL-XG74920B\", \"amount\": 25000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 } ], \"totalShippingAndHandling\": 54550.00, \"totalShippingAndHandlingTax\": 1100.00, \"grandTotal\": 4707150.00, \"amountPaid\": 0.00, \"amountDue\": 4707150.00 }, \"paymentInstructions\": { \"preferredMethod\": \"Wire Transfer\", \"paymentReference\": \"INV-20250523-XG-74920B / CUST-EAGLECORP-GLOBAL-007\", \"latePaymentPenalty\": \"1.5% per month on outstanding balance after due date.\", \"earlyPaymentDiscount\": \"1% discount if paid within 10 days (Amount: $47071.50, New Total: $4660078.50). Reference EPD-XG74920B if claiming.\", \"alternativePayments\": [ { \"method\": \"Secured Crypto Transfer (USDC or ETH)\", \"details\": \"Wallet Address: 0x1234ABCD5678EFGH9012IJKL3456MNOP7890QRST. Memo: XG74920B. Confirmation required via secure_payments@quantumsynergistics-ans.co.uk\" }, { \"method\": \"Irrevocable Letter of Credit (ILOC)\", \"details\": \"To be issued by a Prime Bank, acceptable to Quantum Synergistics. Contact accounts_receivable@quantumsynergistics-ans.co.uk for ILOC requirements.\" } ] }, \"notesAndRemarks\": [ \"All hardware components are subject to export control regulations (EAR/ITAR where applicable). Compliance documentation attached separately (DOC-REF: EXPCOMPL-XG74920B).\", \"Software licenses are non-transferable and subject to the End User License Agreement (EULA-QSANS-V4.2).\", \"On-site consultation hours are estimates. Additional hours will be billed separately under addendum A1 of contract CS-QIP-001.\", \"Warranty claims must be submitted via the online portal at support.quantumsynergistics-ans.co.uk using the provided Warranty IDs.\", \"Return Material Authorization (RMA) required for all returns. Contact customer support for RMA number. Restocking fees may apply (15-25% based on product type and condition). See detailed Return Policy (POL-RET-QSANS-2025-V2).\", \"Projected delivery dates for back-ordered sub-components (Ref: SUBCOMP-BO-LIST-XG74920B-01) will be communicated by your account manager within 7 business days.\" ], \"attachments\": [ {\"documentName\": \"QSANS_Product_Specification_Sheets_Q2_2025.pdf\", \"fileId\": \"DOC-SPECS-QSANS-Q22025-V1.3\"}, {\"documentName\": \"EULA_QSANS_Software_V4.2.pdf\", \"fileId\": \"DOC-EULA-QSANS-V4.2\"}, {\"documentName\": \"Warranty_Terms_and_Conditions_Premium_Standard.pdf\", \"fileId\": \"DOC-WARR-QSANS-PREMSTD-V3.1\"}, {\"documentName\": \"Export_Compliance_Declaration_XG74920B.pdf\", \"fileId\": \"DOC-EXPCOMPL-XG74920B\"}, {\"documentName\": \"Return_Policy_QSANS_2025_V2.pdf\", \"fileId\": \"DOC-POL-RET-QSANS-2025-V2\"}, {\"documentName\": \"Consultation_Services_SOW_PROJ-EAGLE-QIP-2025.pdf\", \"fileId\": \"DOC-SOW-EAGLE-QIP-2025-PH1\"} ], \"approvalWorkflow\": { \"issuerApproval\": { \"approverName\": \"Mr. Alistair Finch\", \"approverTitle\": \"Head of Commercial Operations\", \"approvalDate\": \"2025-05-23\", \"signatureId\": \"SIG-AFINCH-QSANS-20250523-001A\" }, \"clientAcknowledgmentRequired\": true, \"clientAcknowledgmentInstructions\": \"Please sign and return a copy of this invoice or confirm receipt and acceptance via email to billing@quantumsynergistics-ans.co.uk within 5 business days.\" }, \"versionHistory\": [ {\"version\": 1.0, \"date\": \"2025-05-23\", \"reason\": \"Initial Draft\", \"editorId\": \"SYS-AUTOINV-GEN\"}, {\"version\": 1.1, \"date\": \"2025-05-23\", \"reason\": \"Added shipping details and corrected tax calculation for item QN-CORE-X9000-PRO.\", \"editorId\": \"USER-CFO-REVIEW-BOT\"} ], \"footerMessage\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd. - Pioneering the Future, Today. Thank you for your business. For support, please visit our dedicated portal or contact your account representative. All transactions are governed by the laws of England and Wales. Registered Office: 121B, Innovation Drive, London, EC1Y 8XZ, UK. Company Reg No: REG-LND-09876543X. VAT No: VAT-GB-293847261.\" } ''' With the context above, let's create two examples, one where the answer from the model is the correct ID and the other is missing just one character. In [12]: Copied! # Long context examples to test reliability check in invoice processing invoice_examples = [ { \"prompt\" : \"What is the Secure Air Cargo code?\" , \"output\" : \"The Secure Air Cargo code is SHP-SAC-EXP-CN.\" , \"context\" : invoice , \"expected_unreliable\" : False }, { \"prompt\" : \"What is the Secure Air Cargo code?\" , \"output\" : \"The Secure Air Cargo code is HP-SAC-EXP-CN.\" , \"context\" : invoice , \"expected_unreliable\" : True } ] # Long context examples to test reliability check in invoice processing invoice_examples = [ { \"prompt\": \"What is the Secure Air Cargo code?\", \"output\": \"The Secure Air Cargo code is SHP-SAC-EXP-CN.\", \"context\": invoice, \"expected_unreliable\": False }, { \"prompt\": \"What is the Secure Air Cargo code?\", \"output\": \"The Secure Air Cargo code is HP-SAC-EXP-CN.\", \"context\": invoice, \"expected_unreliable\": True } ] Now, by comparing these two answers, we can test the Verify reliability check response: In [15]: Copied! # Test long context validation with invoice print ( \"=== Long Context - Invoice Processing Examples === \\n \" ) print ( f \"Question: { invoice_examples [ 0 ][ 'prompt' ] } \\n \" ) # Run the first example for i , example in enumerate ( invoice_examples ): # Print the question and expected answer type answer_type = \"Correct Answer\" if not example [ 'expected_unreliable' ] else \"Wrong Answer\" print ( f \"#Run { i + 1 } - { answer_type } \" ) # Run the reliability check on the invoice example result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], context = example [ 'context' ], return_search_results = False ) # Print the results print ( f \"a- Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) print ( f \"b- Expected value: { example [ 'expected_unreliable' ] } \" ) explanation = result . get ( 'explanation' , 'N/A' ) # Limit explanation to max 400 characters max_chars = 400 short_explanation = explanation [: max_chars ] + \"...\" if len ( explanation ) > max_chars else explanation print ( f \"c- Short summary of explanation: { short_explanation } \" ) print ( \"--\" ) print () # Test long context validation with invoice print(\"=== Long Context - Invoice Processing Examples ===\\n\") print(f\"Question: {invoice_examples[0]['prompt']}\\n\") # Run the first example for i, example in enumerate(invoice_examples): # Print the question and expected answer type answer_type = \"Correct Answer\" if not example['expected_unreliable'] else \"Wrong Answer\" print(f\"#Run {i+1} - {answer_type}\") # Run the reliability check on the invoice example result = check_reliability_qa( prompt=example['prompt'], output=example['output'], context=example['context'], return_search_results=False ) # Print the results print(f\"a- Is Unreliable: {result.get('is_hallucination', 'N/A')}\") print(f\"b- Expected value: {example['expected_unreliable']}\") explanation = result.get('explanation', 'N/A') # Limit explanation to max 400 characters max_chars = 400 short_explanation = explanation[:max_chars] + \"...\" if len(explanation) > max_chars else explanation print(f\"c- Short summary of explanation: {short_explanation}\") print(\"--\") print() === Long Context - Invoice Processing Examples === Question: What is the Secure Air Cargo code? #Run 1 - Correct Answer a- Is Unreliable: False b- Expected value: False c- Short summary of explanation: The DOCUMENT contains a JSON object representing an invoice with various details including shipping information. Under the 'shippingAndHandling' section within 'summary', there is a list of charges, one of which is described as 'Secure Air Cargo - Expedited (SHIP-BETA-002)' with the charge code 'SHP-SAC-EXP-CN'. This matches the information given in the ANSWER. -- #Run 2 - Wrong Answer a- Is Unreliable: True b- Expected value: True c- Short summary of explanation: To determine whether the provided answer is faithful to the contents of the DOCUMENT, we need to examine the information given in the DOCUMENT and compare it with the ANSWER. The QUESTION asks for the Secure Air Cargo code. Upon reviewing the DOCUMENT, we find that it contains detailed information about an invoice, including shipping information for various items. Specifically, under \"shippingInf... -- OpenAI chat completion endpoint ¬∂ The reliability check conducted via the OpenAI chat completion endpoint method validates multi-turn conversations for reliability issues. This is ideal for conversational AI systems and chatbots. Prepare the data ¬∂ For this scenario, we need to provide the promp via the user role, and the answer from the LLM via the assistant role. We set expected_unreliable as the ground truth for comparison. In [18]: Copied! # Chat conversation examples for Chat Completion checks chat_examples = [ { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant that provides accurate medical information.\" }, { \"role\" : \"user\" , \"content\" : \"Does vitamin C cure the common cold?\" }, { \"role\" : \"assistant\" , \"content\" : \"Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours.\" } ], \"expected_unreliable\" : True }, { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a knowledgeable financial advisor.\" }, { \"role\" : \"user\" , \"content\" : \"What is compound interest?\" }, { \"role\" : \"assistant\" , \"content\" : \"Compound interest is when you earn interest on both your original investment and the interest that has already been earned. It causes your money to grow exponentially over time.\" } ], \"expected_unreliable\" : False } ] # Chat conversation examples for Chat Completion checks chat_examples = [ { \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate medical information.\"}, {\"role\": \"user\", \"content\": \"Does vitamin C cure the common cold?\"}, {\"role\": \"assistant\", \"content\": \"Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours.\"} ], \"expected_unreliable\": True }, { \"messages\": [ {\"role\": \"system\", \"content\": \"You are a knowledgeable financial advisor.\"}, {\"role\": \"user\", \"content\": \"What is compound interest?\"}, {\"role\": \"assistant\", \"content\": \"Compound interest is when you earn interest on both your original investment and the interest that has already been earned. It causes your money to grow exponentially over time.\"} ], \"expected_unreliable\": False } ] Reliability check via the OpenAI client ¬∂ In [19]: Copied! # Function for Chat Completion reliability check def check_reliability_chat ( messages ): \"\"\"Check reliability in chat conversations using OpenAI library\"\"\" # Create a separate client for chat completions with the correct base URL client = OpenAI ( base_url = base_url , api_key = api_key , ) # Make the request using OpenAI client - pass parameters directly response = client . chat . completions . create ( model = \"klusterai/verify-reliability\" , #Reliability model messages = messages ) # Parse the response - kluster.ai returns check results in a specific format return response . choices [ 0 ] . message . content # Function for Chat Completion reliability check def check_reliability_chat(messages): \"\"\"Check reliability in chat conversations using OpenAI library\"\"\" # Create a separate client for chat completions with the correct base URL client = OpenAI( base_url=base_url, api_key=api_key, ) # Make the request using OpenAI client - pass parameters directly response = client.chat.completions.create( model=\"klusterai/verify-reliability\", #Reliability model messages=messages ) # Parse the response - kluster.ai returns check results in a specific format return response.choices[0].message.content In [20]: Copied! # Test Chat Completion checks with our examples chat_results = [] for i , example in enumerate ( chat_examples ): print ( f \"=== Chat Example { i + 1 } ===\" ) print ( f \"System: { example [ 'messages' ][ 0 ][ 'content' ] } \" ) print ( f \"User: { example [ 'messages' ][ 1 ][ 'content' ] } \" ) print ( f \"Assistant: { example [ 'messages' ][ 2 ][ 'content' ] } \" ) print ( f \"Expected Unreliable: { example [ 'expected_unreliable' ] } \" ) print () try : result = check_reliability_chat ( messages = example [ 'messages' ], ) chat_results . append ({ 'example' : example , 'result' : result }) print ( \"Check Result:\" ) print ( f \"Result: { result } \" ) except Exception as e : print ( f \"Error processing chat example: { e } \" ) print ( \" \\n \" + \"=\" * 80 + \" \\n \" ) # Summary of chat checks print ( \"### Chat Check Summary\" ) print ( f \"Processed { len ( chat_results ) } chat conversations\" ) for i , result in enumerate ( chat_results ): expected = result [ 'example' ][ 'expected_unreliable' ] print ( f \"Chat { i + 1 } : Expected unreliable = { expected } \" ) # Test Chat Completion checks with our examples chat_results = [] for i, example in enumerate(chat_examples): print(f\"=== Chat Example {i+1} ===\") print(f\"System: {example['messages'][0]['content']}\") print(f\"User: {example['messages'][1]['content']}\") print(f\"Assistant: {example['messages'][2]['content']}\") print(f\"Expected Unreliable: {example['expected_unreliable']}\") print() try: result = check_reliability_chat( messages=example['messages'], ) chat_results.append({ 'example': example, 'result': result }) print(\"Check Result:\") print(f\"Result: {result}\") except Exception as e: print(f\"Error processing chat example: {e}\") print(\"\\n\" + \"=\"*80 + \"\\n\") # Summary of chat checks print(\"### Chat Check Summary\") print(f\"Processed {len(chat_results)} chat conversations\") for i, result in enumerate(chat_results): expected = result['example']['expected_unreliable'] print(f\"Chat {i+1}: Expected unreliable = {expected}\") === Chat Example 1 === System: You are a helpful assistant that provides accurate medical information. User: Does vitamin C cure the common cold? Assistant: Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours. Expected Unreliable: True Error processing chat example: Error code: 500 - {'error': {'message': 'Unexpected error occurred', 'errorCode': 4000, 'type': 'invalid_request_error'}} ================================================================================ === Chat Example 2 === System: You are a knowledgeable financial advisor. User: What is compound interest? Assistant: Compound interest is when you earn interest on both your original investment and the interest that has already been earned. It causes your money to grow exponentially over time. Expected Unreliable: False Error processing chat example: Error code: 500 - {'error': {'message': 'Unexpected error occurred', 'errorCode': 4000, 'type': 'invalid_request_error'}} ================================================================================ ### Chat Check Summary Processed 0 chat conversations Summary ¬∂ This tutorial demonstrated how to use the reliability check feature of the Verify service to identify and prevent reliability issues in AI outputs. In this particular example, we used the dedicated reliability endpoint and the OpenAI-compatible method via the chat completions endpoint. Some key takeaways: Two reliability check methods : A dedicated endpoint for Q/A verifications, and the OpenAI-compatible chat completion endpoint for conversations. Two operation modes : General knowledge verification and context-based validation. Detailed explanations : The service provides clear reasoning for its determinations. Transparent verification : With return_search_results enabled, the service provides a list of sources used for verification. This helps users understand the basis for each reliability decision thereby increasing trust in the results.",
      "scraped_at": 1749147203.568625
    },
    "https://docs.kluster.ai/api-reference/reference#chat-completion-object": {
      "url": "https://docs.kluster.ai/api-reference/reference#chat-completion-object",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147203.9080036
    },
    "https://docs.kluster.ai/api-reference/reference/": {
      "url": "https://docs.kluster.ai/api-reference/reference/",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147202.693743
    },
    "https://docs.kluster.ai/get-started/fine-tuning/api/": {
      "url": "https://docs.kluster.ai/get-started/fine-tuning/api/",
      "title": "Fine-tuning with the kluster.ai API | kluster.ai Docs",
      "content": "Fine-tuning with the kluster.ai API ÔºÉ The kluster.ai API lets you automate and integrate fine-tuning into your development workflows. You can create, manage, and monitor fine-tuning jobs directly from your code, making it easy to customize models for your specific needs. This guide provides a practical overview of the fine-tuning process using the API. It covers the required data format, how to upload your dataset, and how to launch and monitor a fine-tuning job. For a step-by-step walkthrough, see the linked tutorial in the tips below. Prerequisites ÔºÉ Before getting started with fine-tuning, ensure you have the following: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. Prepared dataset : You need data formatted according to kluster.ai's requirements for fine-tuning (detailed below). Supported models ÔºÉ kluster.ai currently supports fine-tuning for the following models: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Note You can query the models endpoint in the API and filter for the tag \"fine-tunable.\" Fine-tuning workflow ÔºÉ Fine‚Äëtuning a model with the kluster.ai API follows a straightforward five‚Äëstep workflow: Prepare your data : Collect and structure high‚Äëquality JSONL training examples that reflect the task you want the model to learn. Upload your training file : Send the JSONL file to kluster.ai and note the returned file_id . Create the fine‚Äëtuning job : Launch a fine‚Äëtuning job specifying the base model and training file_id (plus any optional hyperparameters). Monitor job progress : Poll the job endpoint (or subscribe to webhooks) until the job reaches the succeeded state. Use your fine‚Äëtuned model : Invoke the model name returned by the job for inference in your application or the kluster.ai playground. The following sections will provide a closer look at each step. Prepare your data ÔºÉ High-quality, well-formatted data is crucial for successful fine-tuning: Format : Data must be in JSONL format, where each line is a valid JSON object representing a training example. Structure : Each JSON object should contain a messages array with system, user, and assistant messages. Example format : { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a JSON Generation Specialist. Convert user requests into properly formatted JSON.\" }, { \"role\" : \"user\" , \"content\" : \"Create a configuration for a web application with name 'TaskMaster', version 1.2.0, and environment set to development.\" }, { \"role\" : \"assistant\" , \"content\" : \"{\\n \\\"application\\\": {\\n \\\"name\\\": \\\"TaskMaster\\\",\\n \\\"version\\\": \\\"1.2.0\\\",\\n \\\"environment\\\": \\\"development\\\"\\n }\\n}\" } ] } Quantity : The minimum requirement is 10 examples, but more diverse and high-quality examples yield better results. Quality : Ensure your data accurately represents the task you want the model to perform. Data preparation For a detailed walkthrough of data preparation, see the Fine-tuning sentiment analysis tutorial . Find Llama datasets on Hugging Face There is a wide range of datasets suitable for Llama model fine-tuning on Hugging Face Datasets . Browse trending and community-curated datasets to accelerate your data preparation. Set up the client ÔºÉ First, install the OpenAI Python library: pip install openai Then initialize the client with the kluster.ai base URL: from openai import OpenAI api_key = getpass ( \"Enter your kluster.ai API key: \" ) # Set up the client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key ) Upload your training file ÔºÉ Once your data is prepared, upload it to the kluster.ai platform: # Upload fine-tuning file (for files under 100MB) with open ( 'training_data.jsonl' , 'rb' ) as file : upload_response = client . files . create ( file = file , purpose = \"fine-tune\" # Important: specify \"fine-tune\" as the purpose ) # Get the file ID file_id = upload_response . id print ( f \"File uploaded successfully. File ID: { file_id } \" ) File size & upload limits Each fine-tuning file must be ‚â§ 100 MB on both the free and standard tiers (the standard tier simply allows more total examples). When your dataset approaches this limit, use the chunked upload method for reliable multi-part uploads. Create a fine-tuning job ÔºÉ After uploading your data, initiate the fine-tuning job: # Model model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" # Create fine-tune job fine_tuning_job = client . fine_tuning . jobs . create ( training_file = file_id , model = model , # Optional hyperparameters # hyperparameters={ # \"batch_size\": 3, # \"n_epochs\": 2, # \"learning_rate_multiplier\": 0.08 # } ) Monitor job progress ÔºÉ Track the status of your fine-tuning job: # Retrieve job status job_status = client . fine_tuning . jobs . retrieve ( fine_tuning_job . id ) print ( f \"Job status: { job_status . status } \" ) Use your fine-tuned model ÔºÉ Once your fine-tuning job completes successfully, you will receive a unique fine-tuned model name that you can use for inference: # Get the fine-tuned model name finished_job = client . fine_tuning . jobs . retrieve ( fine_tuning_job . id ) fine_tuned_model = finished_job . fine_tuned_model # Use the fine-tuned model for inference response = client . chat . completions . create ( model = fine_tuned_model , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a JSON Generation Specialist. Convert user requests into properly formatted JSON.\" }, { \"role\" : \"user\" , \"content\" : \"Create a configuration for a web application with name 'TaskMaster', version 1.2.0, and environment set to development.\" } ] ) You can view the end-to-end python script below: fine-tune.py from getpass import getpass from openai import OpenAI api_key = getpass ( \"Enter your kluster.ai API key: \" ) # Set up the client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key ) # Upload fine-tuning file (for files under 100MB) with open ( 'training_data.jsonl' , 'rb' ) as file : upload_response = client . files . create ( file = file , purpose = \"fine-tune\" # Important: specify \"fine-tune\" as the purpose ) # Get the file ID file_id = upload_response . id print ( f \"File uploaded successfully. File ID: { file_id } \" ) # Model model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" # Create fine-tune job fine_tuning_job = client . fine_tuning . jobs . create ( training_file = file_id , model = model , # Optional hyperparameters # hyperparameters={ # \"batch_size\": 3, # \"n_epochs\": 2, # \"learning_rate_multiplier\": 0.08 # } ) # Retrieve job status job_status = client . fine_tuning . jobs . retrieve ( fine_tuning_job . id ) print ( f \"Job status: { job_status . status } \" ) # Get the fine-tuned model name finished_job = client . fine_tuning . jobs . retrieve ( fine_tuning_job . id ) fine_tuned_model = finished_job . fine_tuned_model # Use the fine-tuned model for inference response = client . chat . completions . create ( model = fine_tuned_model , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a JSON Generation Specialist. Convert user requests into properly formatted JSON.\" }, { \"role\" : \"user\" , \"content\" : \"Create a configuration for a web application with name 'TaskMaster', version 1.2.0, and environment set to development.\" } ] ) Use your fine-tuned model in the playground (optional) ÔºÉ After your fine-tuned model is created, you can also test it in the kluster.ai playground: Go to the kluster.ai playground Select your fine-tuned model from the model dropdown menu Start chatting with your model to evaluate its performance on your specific task Benefits of fine-tuning ÔºÉ Fine-tuning offers several advantages over using general-purpose models: Improved performance : Fine-tuned models often outperform base models on specific tasks. Cost efficiency : Smaller fine-tuned models can outperform larger models at a lower cost. Reduced latency : Fine-tuned models can deliver faster responses for your applications. Consistency : More reliable outputs tailored to your specific task or domain. Next steps ÔºÉ Detailed tutorial : Follow the Fine-tuning sentiment analysis tutorial . API reference : Review the API reference documentation for all fine-tuning related endpoints. Explore models : See the Models page to check which foundation models support fine-tuning. Platform approach : Try the user-friendly platform interface for fine-tuning without writing code.",
      "scraped_at": 1749147203.2893348
    },
    "https://docs.kluster.ai/get-started/integrations/": {
      "url": "https://docs.kluster.ai/get-started/integrations/",
      "title": "Integrate CrewAI with kluster.ai API | kluster.ai Docs",
      "content": "Integrate CrewAI with kluster.ai ÔºÉ CrewAI is a multi-agent platform that organizes specialized AI agents‚Äîeach with defined roles, tools, and goals‚Äîwithin a structured process to tackle complex tasks efficiently. CrewAI agents streamline workflows and deliver reliable, scalable solutions by coordinating tasks and ensuring smooth collaboration. This guide walks you through integrating kluster.ai with CrewAI to create and run a simple AI agent chatbot that leverages the kluster.ai API. Prerequisites ÔºÉ Before starting, ensure you have the following prerequisites: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. CrewAI installed - the Installation Guide on the CrewAI website will walk you through installing CrewAI, setting up a virtual Python environment, and creating a new project. Note that CrewAI requires a Python version >= 3.10 and < 3.13 Create a project with the CLI ÔºÉ Open your Python virtual environment, and then follow these steps to use the CrewAI CLI to create a new project: Create a project - following the installation guide, create your first project with the following command: crewai create crew INSERT_PROJECT_NAME Select model and provider - during setup, the CLI will ask you to choose a provider and a model. Select openai as the provider and then choose any available model. Because you'll configure kluster.ai as a custom model, your initial model choice won't affect the final integration. The CLI will prompt you for an OpenAI API key, but this isn‚Äôt required. Simply press enter to skip Build a simple AI agent ÔºÉ After finishing the CLI setup, you will see a src directory with files crew.py and main.py . This guide won't use these sample files because they include extra features outside the scope. Follow these steps to continue: Create your first file - create a hello_crew.py file in src/YOUR_PROJECT_NAME to correspond to a simple AI agent chatbot Import modules and select model - open hello_crew.py to add imports and define a custom LLM for kluster.ai by setting the following parameters: provider - you can specify openai_compatible model - choose one of kluster.ai's available models based on your use case. Regardless of which model you choose, prepend its name with openai/ to ensure CrewAI, which relies on LiteLLM, processes your requests correctly base_url - use https://api.kluster.ai/v1 to send requests to the kluster.ai endpoint api_key - replace INSERT_API_KEY in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide hello_crew.py import random from crewai import LLM , Agent , Crew , Process , Task from crewai.project import CrewBase , agent , crew , task @CrewBase class HelloWorldCrew : # Override any default YAML references agents_config = {} tasks_config = {} def __init__ ( self ): \"\"\" When this crew is instantiated, create a custom LLM with your base_url. \"\"\" self . custom_llm = LLM ( provider = \"openai_compatible\" , model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_KLUSTER_API_KEY\" ) This example overrides agents_config and tasks_config with empty dictionaries to tell CrewAI to ignore all YAML files and rely solely on your code, keeping this guide as streamlined as possible. Define your agent - set the agent's role, goal, and backstory, and assign the custom LLM (via the kluster.ai API) for generating creative greetings: hello_crew.py @agent def hello_agent ( self ) -> Agent : \"\"\" A super simple agent with a single purpose: greet the user in a friendly, varied way. \"\"\" return Agent ( role = \"HelloWorldAgent\" , goal = \"Greet the user in a fun and creative way.\" , backstory = \"I'm a friendly agent who greets everyone in a slightly different manner!\" , llm = self . custom_llm , verbose = True ) Give the agent a task - define a task that prompts the agent for a unique, creative greeting using randomness to avoid repetition. Passing this prompt to hello_agent() ensures varied responses. CrewAI requires an expected_output field, defined here as a short greeting: hello_crew.py @task def hello_task ( self ) -> Task : \"\"\" A task that asks the agent to produce a dynamic greeting. \"\"\" random_factor = random . randint ( 100000 , 999999 ) prompt = f \"\"\" You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: { random_factor } Example: \"Hey there, how's your day going?\" \"\"\" return Task ( description = prompt , expected_output = \"A short, creative greeting\" , agent = self . hello_agent () ) Tie it all together with a @crew method - add the following method to return the assembled Crew object with a single agent and task. This method enables CrewAI to coordinate the agent and task you defined: hello_crew.py @crew def hello_crew ( self ) -> Crew : \"\"\" Our entire 'Hello World' crew‚Äîonly 1 agent + 1 task in sequence. \"\"\" return Crew ( agents = self . agents , tasks = self . tasks , process = Process . sequential , verbose = True ) Set up the entry point for the agent - create a new file named hello_main.py . In hello_main.py , import and initialize the HelloWorldCrew class, call its hello_crew() method, and then kickoff() to launch the task sequence: hello_main.py #!/usr/bin/env python from hello_crew import HelloWorldCrew def run (): \"\"\" Kick off the HelloWorld crew with no inputs. \"\"\" HelloWorldCrew () . hello_crew () . kickoff ( inputs = {}) if __name__ == \"__main__\" : run () View complete script hello_crew.py import random from crewai import LLM , Agent , Crew , Process , Task from crewai.project import CrewBase , agent , crew , task @CrewBase class HelloWorldCrew : # Override any default YAML references agents_config = {} tasks_config = {} def __init__ ( self ): \"\"\" When this crew is instantiated, create a custom LLM with your base_url. \"\"\" self . custom_llm = LLM ( provider = \"openai_compatible\" , model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_KLUSTER_API_KEY\" ) @agent def hello_agent ( self ) -> Agent : \"\"\" A super simple agent with a single purpose: greet the user in a friendly, varied way. \"\"\" return Agent ( role = \"HelloWorldAgent\" , goal = \"Greet the user in a fun and creative way.\" , backstory = \"I'm a friendly agent who greets everyone in a slightly different manner!\" , llm = self . custom_llm , verbose = True ) @task def hello_task ( self ) -> Task : \"\"\" A task that asks the agent to produce a dynamic greeting. \"\"\" random_factor = random . randint ( 100000 , 999999 ) prompt = f \"\"\" You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: { random_factor } Example: \"Hey there, how's your day going?\" \"\"\" return Task ( description = prompt , expected_output = \"A short, creative greeting\" , agent = self . hello_agent () ) @crew def hello_crew ( self ) -> Crew : \"\"\" Our entire 'Hello World' crew‚Äîonly 1 agent + 1 task in sequence. \"\"\" return Crew ( agents = self . agents , tasks = self . tasks , process = Process . sequential , verbose = True ) Put it all together ÔºÉ To run your agent, ensure you are in the same directory as your hello_main.py file, then use the following command: python hello_main.py Upon running the script, you'll see output that looks like the following: # Agent: HelloWorldAgent ## Task: You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: 896380 Example: \"Hey there, how's your day going?\" # Agent: HelloWorldAgent ## Final Answer: Hello, it's a beautiful day to shine, how's your sparkle today? And that's it! You've now successfully configured your AI agent harnessing CrewAI and the power of the kluster.ai API!",
      "scraped_at": 1749147203.60358
    },
    "https://docs.kluster.ai/tutorials/klusterai-api/reliability-check/": {
      "url": "https://docs.kluster.ai/tutorials/klusterai-api/reliability-check/",
      "title": "Reliability check | kluster.ai Docs",
      "content": "Reliability check with the kluster.ai API ¬∂ Introduction ¬∂ Reliability issues in AI occur when models generate information that appears plausible but is unreliable or unsupported by the provided context. This poses significant risks in production applications, particularly in domains where accuracy is critical. This tutorial demonstrates how to use Verify to identify and prevent reliability issues in your applications. We'll explore available methods: a dedicated API endpoint and via the OpenAI compatible chat completions endpoint. The service can evaluate AI responses based on provided context (perfect for RAG applications) or perform real-time verification against general knowledge. By following this tutorial, you'll learn how to: Verify reliability in individual Q&A pairs. Compare general knowledge verification vs. context validation modes. Validate responses in full conversation histories. Prerequisites ¬∂ Before getting started, ensure you have the following: A kluster.ai account : sign up on the kluster.ai platform if you don't have one A kluster.ai API key : after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide Setup ¬∂ In this notebook, we'll use Python's getpass module to input the key safely. After execution, please provide your unique kluster.ai API key (ensure no spaces). In [1]: Copied! from getpass import getpass api_key = getpass ( \"Enter your kluster.ai API key: \" ) from getpass import getpass api_key = getpass(\"Enter your kluster.ai API key: \") Next, ensure you've installed OpenAI Python and other required libraries: In [2]: Copied! % pip install - q openai requests %pip install -q openai requests Note: you may need to restart the kernel to use updated packages. With the OpenAI Python library installed, we import the necessary dependencies for the tutorial: In [3]: Copied! from openai import OpenAI import time import json import requests from openai import OpenAI import time import json import requests And then, initialize the client by pointing it to the kluster.ai endpoint, and passing your API key. In [4]: Copied! # Define the base URL for both methods base_url_endpoint = \"https://api.kluster.ai/v1/verify/reliability\" #To test with HTTP requests base_url = \"https://api.kluster.ai/v1\" # To test with OpenAI client # Set up the client client = OpenAI ( base_url = base_url_endpoint , api_key = api_key , ) # Define the base URL for both methods base_url_endpoint = \"https://api.kluster.ai/v1/verify/reliability\" #To test with HTTP requests base_url= \"https://api.kluster.ai/v1\" # To test with OpenAI client # Set up the client client = OpenAI( base_url=base_url_endpoint, api_key=api_key, ) Dedicated reliability endpoint ¬∂ The reliability check dedicated endpoint validates whether an answer to a specific question contains unreliable or incorrect information. It operates in two modes: General knowledge verification : when no context is provided, the service verifies answers by comparing it to other sources. Context validation mode : when context is provided, the service only validates answers against that context. For our example, we'll create diverse test cases to demonstrate the reliability check capabilities: General knowledge verification examples : questions where the service verifies against external sources. Context validation examples : scenarios where responses must align with provided context. Search results demonstration : see how enabling return_search_results provides sources used for verification, helping you understand and trust the service's decisions. Invoice extraction example : a practical use case for document processing. Chat completions example : use the convenient OpenAI SDK to check for reliability issues. To call the endpoint, we'll use the following function: In [5]: Copied! # Function that runs the reliability check for general knowledge examples def check_reliability_qa ( prompt , output , context = None , return_search_results = False ): \"\"\"Check reliability using the dedicated endpoint\"\"\" url = base_url_endpoint headers = { \"Authorization\" : f \"Bearer { api_key } \" , \"Content-Type\" : \"application/json\" } # Prepare the payload payload = { \"prompt\" : prompt , \"output\" : output , \"return_search_results\" : return_search_results } # Add context if provided if context : payload [ \"context\" ] = context # Make the POST request to the API response = requests . post ( url , headers = headers , json = payload ) return response . json () # Function that runs the reliability check for general knowledge examples def check_reliability_qa(prompt, output, context=None, return_search_results=False): \"\"\"Check reliability using the dedicated endpoint\"\"\" url = base_url_endpoint headers = { \"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\" } # Prepare the payload payload = { \"prompt\": prompt, \"output\": output, \"return_search_results\": return_search_results } # Add context if provided if context: payload[\"context\"] = context # Make the POST request to the API response = requests.post(url, headers=headers, json=payload) return response.json() Prepare the data ¬∂ In all scenarios, a prompt and output most be provided. The prompt is the message/question from the user, and the output is the answer from the Model. In addition, we are also providing the ground truth in regards to hallucination. In [6]: Copied! # Create test datasets general_knowledge_examples = [ { \"prompt\" : \"What is the capital of France?\" , \"output\" : \"The capital of France is London.\" , \"expected_hallucination\" : True }, { \"prompt\" : \"When was the Eiffel Tower built?\" , \"output\" : \"The Eiffel Tower was built in 1889 for the Paris Exposition.\" , \"expected_hallucination\" : False }, { \"prompt\" : \"Are ghosts real?\" , \"output\" : \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\" , \"expected_hallucination\" : True } ] # Create test datasets general_knowledge_examples = [ { \"prompt\": \"What is the capital of France?\", \"output\": \"The capital of France is London.\", \"expected_hallucination\": True }, { \"prompt\": \"When was the Eiffel Tower built?\", \"output\": \"The Eiffel Tower was built in 1889 for the Paris Exposition.\", \"expected_hallucination\": False }, { \"prompt\": \"Are ghosts real?\", \"output\": \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\", \"expected_hallucination\": True } ] For context validation, the necessary data must be provided via the context field. In [7]: Copied! context_validation_examples = [ { \"prompt\" : \"What's the invoice date?\" , \"output\" : \"The invoice date is May 22, 2025.\" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"expected_hallucination\" : False }, { \"prompt\" : \"What's the total amount on the invoice?\" , \"output\" : \"The total amount is 8500 USD.\" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"expected_hallucination\" : True }, { \"prompt\" : \"Who is the client mentioned in the document?\" , \"output\" : \"The client is Acme.\" , \"context\" : \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\" , \"expected_hallucination\" : False } ] context_validation_examples = [ { \"prompt\": \"What's the invoice date?\", \"output\": \"The invoice date is May 22, 2025.\", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\", \"expected_hallucination\": False }, { \"prompt\": \"What's the total amount on the invoice?\", \"output\": \"The total amount is 8500 USD.\", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\", \"expected_hallucination\": True }, { \"prompt\": \"Who is the client mentioned in the document?\", \"output\": \"The client is Acme.\", \"context\": \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\", \"expected_hallucination\": False } ] General knowledge verification ¬∂ Let's test general knowledge verification mode with our examples: In [8]: Copied! # Test general knowledge verification mode verification_results = [] for i , example in enumerate ( general_knowledge_examples ): print ( f \"=== General Knowledge Verification Example { i + 1 } ===\" ) print ( f \"Question: { example [ 'prompt' ] } \" ) print ( f \"Answer: { example [ 'output' ] } \" ) print ( f \"Expected Unreliable: { example [ 'expected_hallucination' ] } \" ) print () result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], return_search_results = False ) verification_results . append ({ 'example' : example , 'result' : result }) print ( \"Check Result:\" ) print ( f \"Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) print ( f \"Explanation: { result . get ( 'explanation' , 'N/A' ) } \" ) print ( f \"Tokens Used: { result . get ( 'usage' , {}) } \" ) print ( \" \\n \" + \"=\" * 80 + \" \\n \" ) # Test general knowledge verification mode verification_results = [] for i, example in enumerate(general_knowledge_examples): print(f\"=== General Knowledge Verification Example {i+1} ===\") print(f\"Question: {example['prompt']}\") print(f\"Answer: {example['output']}\") print(f\"Expected Unreliable: {example['expected_hallucination']}\") print() result = check_reliability_qa( prompt=example['prompt'], output=example['output'], return_search_results=False ) verification_results.append({ 'example': example, 'result': result }) print(\"Check Result:\") print(f\"Is Unreliable: {result.get('is_hallucination', 'N/A')}\") print(f\"Explanation: {result.get('explanation', 'N/A')}\") print(f\"Tokens Used: {result.get('usage', {})}\") print(\"\\n\" + \"=\"*80 + \"\\n\") === General Knowledge Verification Example 1 === Question: What is the capital of France? Answer: The capital of France is London. Expected Unreliable: True Check Result: Is Unreliable: True Explanation: The user asked for the capital of France. The correct capital of France is Paris, not London. London is the capital of England, not France, making the response factually incorrect. Tokens Used: {'completion_tokens': 118, 'prompt_tokens': 937, 'total_tokens': 1055} ================================================================================ === General Knowledge Verification Example 2 === Question: When was the Eiffel Tower built? Answer: The Eiffel Tower was built in 1889 for the Paris Exposition. Expected Unreliable: False Check Result: Is Unreliable: False Explanation: The response correctly states that the Eiffel Tower was built in 1889. The Eiffel Tower was indeed constructed for the 1889 World's Fair in Paris, making the additional context accurate. The information provided is verifiable and aligns with historical facts about the Eiffel Tower. Tokens Used: {'completion_tokens': 418, 'prompt_tokens': 957, 'total_tokens': 1375} ================================================================================ === General Knowledge Verification Example 3 === Question: Are ghosts real? Answer: Yes, there is a recent scientific study from Harvard that confirms ghosts exist. Expected Unreliable: True Check Result: Is Unreliable: True Explanation: The original user request asks if ghosts are real. The response from the other LLM claims that a recent scientific study from Harvard confirms the existence of ghosts. The search results provide several links related to Harvard and the study of ghosts or supernatural phenomena, but none of them directly confirm the existence of ghosts. The snippets from the search results indicate that Harvard has conducted studies and courses on the topic of ghosts and supernatural phenomena, but these are primarily focused on folklore, mythology, and the cultural or psychological aspects of belief in ghosts. There is no clear evidence in the search results of a scientific study from Harvard that confirms the existence of ghosts. The response from the other LLM is an example of hallucination because it presents a factual claim (a recent scientific study from Harvard confirming ghosts exist) that is not supported by the search results. Tokens Used: {'completion_tokens': 282, 'prompt_tokens': 1744, 'total_tokens': 2026} ================================================================================ Enable search results ¬∂ When enabling the property return_search_results=true , the reliability check feature will return the sources used for the verification. In [9]: Copied! # Test general knowledge verification with search results enabled print ( \"=== General Knowledge Verification with Search Results ===\" ) example = { \"prompt\" : \"Are ghosts real?\" , \"output\" : \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\" } # Let's run the check with search results enabled result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], return_search_results = True # Enable search results ) # Display the result print ( f \"Question: { example [ 'prompt' ] } \" ) print ( f \"Answer: { example [ 'output' ] } \" ) print ( f \" \\n Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) # Display search results if 'search_results' in result and result [ 'search_results' ]: print ( f \" \\n üìö Search Results Used ( { len ( result [ 'search_results' ]) } sources):\" ) for idx , source in enumerate ( result [ 'search_results' ][: 5 ], 1 ): # Show first 5 print ( f \" \\n { idx } . { source . get ( 'title' , 'No title' ) } \" ) print ( f \" üìÑ { source . get ( 'snippet' , 'No snippet' )[: 150 ] } ...\" ) print ( f \" üîó { source . get ( 'link' , 'No link' ) } \" ) else : print ( \" \\n No search results returned\" ) print ( f \" \\n Tokens Used: { result . get ( 'usage' , {}) } \" ) # Test general knowledge verification with search results enabled print(\"=== General Knowledge Verification with Search Results ===\") example = { \"prompt\": \"Are ghosts real?\", \"output\": \"Yes, there is a recent scientific study from Harvard that confirms ghosts exist.\" } # Let's run the check with search results enabled result = check_reliability_qa( prompt=example['prompt'], output=example['output'], return_search_results=True # Enable search results ) # Display the result print(f\"Question: {example['prompt']}\") print(f\"Answer: {example['output']}\") print(f\"\\nIs Unreliable: {result.get('is_hallucination', 'N/A')}\") # Display search results if 'search_results' in result and result['search_results']: print(f\"\\nüìö Search Results Used ({len(result['search_results'])} sources):\") for idx, source in enumerate(result['search_results'][:5], 1): # Show first 5 print(f\"\\n{idx}. {source.get('title', 'No title')}\") print(f\" üìÑ {source.get('snippet', 'No snippet')[:150]}...\") print(f\" üîó {source.get('link', 'No link')}\") else: print(\"\\nNo search results returned\") print(f\"\\nTokens Used: {result.get('usage', {})}\") === General Knowledge Verification with Search Results === Question: Are ghosts real? Answer: Yes, there is a recent scientific study from Harvard that confirms ghosts exist. Is Unreliable: True üìö Search Results Used (10 sources): 1. The Allure of the Supernatural | Harvard Independent üìÑ Focusing in on ghosts and other such spirits, the study revealed a greater proportion of belief in the mystical than the national averages ...... üîó https://harvardindependent.com/the-allure-of-the-supernatural/ 2. Harvard class studies supernatural stories üìÑ Folklore & Mythology course examines how tales of spirits and ghosts from the past affect the present and the future.... üîó https://news.harvard.edu/gazette/story/2021/10/harvard-class-studies-supernatural-stories/ 3. The Ghost Studies: New Perspectives on the Origins of Paranormal ... üìÑ New and exciting scientific theories that explain apparitions, hauntings, and communications from the dead.... üîó https://www.harvard.com/book/9781632651211 4. Did Scientists Just Discover the Cause of Ghost Sightings? | Unveiled üìÑ Ghosts & the Afterlife: Science Unveils the Mystery of Spirits ¬∑ Ghosts Aren't Real: 4 Scientific Explanations for Paranormal Activity ¬∑ Harvard ...... üîó https://www.youtube.com/watch?v=fuFOGYxb6bI 5. The Ivy and the Occult | Harvard Independent üìÑ While ghost stories and psychical research seem to have largely disappeared from Harvard over the years, there is still an eclectic mix of ...... üîó https://harvardindependent.com/the-ivy-and-the-occult/ Tokens Used: {'completion_tokens': 308, 'prompt_tokens': 1778, 'total_tokens': 2086} Context validation mode ¬∂ The context validation mode uses the context property as the ground truth. When enabled, the service does not verify the answer using external knowledge; instead, it focuses on identifying reliability issues based solely on the information within the provided context . In [10]: Copied! # Test context validation mode context_results = [] # for i , example in enumerate ( context_validation_examples ): print ( f \"=== Context Validation Example { i + 1 } ===\" ) print ( f \"Context: { example [ 'context' ] } \" ) print ( f \"Question: { example [ 'prompt' ] } \" ) print ( f \"Answer: { example [ 'output' ] } \" ) print ( f \"Expected Unreliable: { example [ 'expected_hallucination' ] } \" ) print () # Run the reliability check with context result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], context = example [ 'context' ], return_search_results = False ) context_results . append ({ 'example' : example , 'result' : result }) # Display the results print ( \"Check Result:\" ) print ( f \"Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) print ( f \"Explanation: { result . get ( 'explanation' , 'N/A' ) } \" ) print ( f \"Tokens Used: { result . get ( 'usage' , {}) } \" ) print ( \" \\n \" + \"=\" * 80 + \" \\n \" ) # Test context validation mode context_results = [] # for i, example in enumerate(context_validation_examples): print(f\"=== Context Validation Example {i+1} ===\") print(f\"Context: {example['context']}\") print(f\"Question: {example['prompt']}\") print(f\"Answer: {example['output']}\") print(f\"Expected Unreliable: {example['expected_hallucination']}\") print() # Run the reliability check with context result = check_reliability_qa( prompt=example['prompt'], output=example['output'], context=example['context'], return_search_results=False ) context_results.append({ 'example': example, 'result': result }) # Display the results print(\"Check Result:\") print(f\"Is Unreliable: {result.get('is_hallucination', 'N/A')}\") print(f\"Explanation: {result.get('explanation', 'N/A')}\") print(f\"Tokens Used: {result.get('usage', {})}\") print(\"\\n\" + \"=\"*80 + \"\\n\") === Context Validation Example 1 === Context: InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C Question: What's the invoice date? Answer: The invoice date is May 22, 2025. Expected Unreliable: False Check Result: Is Unreliable: False Explanation: The answer accurately reflects the information given in the document regarding the invoice date, making a reasonable assumption about the abbreviated year. Tokens Used: {'completion_tokens': 438, 'prompt_tokens': 267, 'total_tokens': 705} ================================================================================ === Context Validation Example 2 === Context: InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C Question: What's the total amount on the invoice? Answer: The total amount is 8500 USD. Expected Unreliable: True Check Result: Is Unreliable: True Explanation: The answer contradicts the document by stating a different amount and currency. Tokens Used: {'completion_tokens': 426, 'prompt_tokens': 267, 'total_tokens': 693} ================================================================================ === Context Validation Example 3 === Context: InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C Question: Who is the client mentioned in the document? Answer: The client is Acme. Expected Unreliable: False Check Result: Is Unreliable: False Explanation: To determine whether the answer is faithful to the contents of the document, we need to analyze the provided information. The document contains a specific entry: \"InvID:INV7701B Co:OptiTech Client:Acme Amt:7116GBP Date:22May25 Due:21Jun25 Terms:N30 Ref:PO451C\". Within this entry, it is explicitly stated that the \"Client:Acme\". The question asks, \"Who is the client mentioned in the document?\" The answer provided is \"The client is Acme.\" To assess the faithfulness of the answer to the document: 1. The document directly states that the client is \"Acme\". 2. The answer directly corresponds to this information by stating \"The client is Acme\". 3. There is no additional information introduced in the answer that is not present in the document. 4. The answer does not contradict any information provided in the document. Given these observations, the answer accurately reflects the information contained within the document. Therefore, the verdict is: {\"REASONING\": \"The answer directly corresponds to the information provided in the document without introducing new information or contradicting existing information.\", \"HALLUCINATION\": 0} Tokens Used: {'completion_tokens': 246, 'prompt_tokens': 265, 'total_tokens': 511} ================================================================================ Extended context ¬∂ A very common use case is document extraction. Let's see how a lengthy invoice used as context helps us to check if our model is producing reliable output. In [11]: Copied! # Invoice Example invoice = ''' { \"invoiceId\": \"INV-20250523-XG-74920B\", \"orderReference\": \"ORD-PROC-Q2-2025-ALPHA-99374-DELTA\", \"customerIdentification\": \"CUST-EAGLECORP-GLOBAL-007\", \"dateIssued\": \"2025-05-23\", \"dueDate\": \"2025-06-22\", \"paymentTerms\": \"Net 30 Days\", \"currency\": \"USD\", \"issuerDetails\": { \"companyName\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd.\", \"taxId\": \"VAT-GB-293847261\", \"registrationNumber\": \"REG-LND-09876543X\", \"address\": { \"street\": \"121B, Innovation Drive, Silicon Roundabout, Tech City East\", \"city\": \"London\", \"postalCode\": \"EC1Y 8XZ\", \"country\": \"United Kingdom\", \"planet\": \"Earth\", \"dimension\": \"Sigma-7\" }, \"contact\": { \"primaryPhone\": \"+44-20-7946-0001 ext. 777\", \"secondaryPhone\": \"+44-20-7946-0002\", \"fax\": \"+44-20-7946-0003\", \"email\": \"billing@quantumsynergistics-ans.co.uk\", \"website\": \"www.quantumsynergistics-ans.co.uk\" }, \"bankDetails\": { \"bankName\": \"Universal Interstellar Bank PLC\", \"accountName\": \"Quantum Synergistics & ANS Ltd.\", \"accountNumber\": \"9876543210123456\", \"swiftBic\": \"UNIVGB2LXXX\", \"iban\": \"GB29 UNIV 9876 5432 1012 3456 78\", \"reference\": \"INV-20250523-XG-74920B\" } }, \"billingInformation\": { \"companyName\": \"EagleCorp Global Holdings Inc. & Subsidiaries\", \"department\": \"Strategic Procurement & Interstellar Logistics Division\", \"attentionTo\": \"Ms. Evelyn Reed, Chief Procurement Officer (CPO)\", \"taxId\": \"EIN-US-98-7654321X\", \"clientReferenceId\": \"EGL-PROC-REF-Q2-2025-7734-GAMMA\", \"address\": { \"street\": \"Suite 9870, Eagle Tower One, 1500 Constitution Avenue NW\", \"city\": \"Washington D.C.\", \"state\": \"District of Columbia\", \"postalCode\": \"20001-1500\", \"country\": \"United States of America\" }, \"contact\": { \"phone\": \"+1-202-555-0189 ext. 1234\", \"email\": \"e.reed.procurement@eaglecorpglobal.com\" } }, \"shippingInformation\": [ { \"shipmentId\": \"SHIP-ALPHA-001-XG74920B\", \"recipientName\": \"Dr. Aris Thorne, Head of R&D\", \"facilityName\": \"EagleCorp Advanced Research Facility - Sector Gamma-7\", \"address\": { \"street\": \"Docking Bay 7, 47 Industrial Park Road\", \"city\": \"New Chicago\", \"state\": \"Illinois\", \"postalCode\": \"60699-0047\", \"country\": \"United States of America\", \"deliveryZone\": \"Restricted Access - Level 3 Clearance Required\" }, \"shippingMethod\": \"Cryo-Stasis Freight - Priority Overnight\", \"trackingNumber\": \"TRK-CSFPON-9988776655-A01\", \"notes\": \"Deliver between 08:00 - 10:00 Local Time. Handle with Extreme Care. Temperature Sensitive Materials.\" }, { \"shipmentId\": \"SHIP-BETA-002-XG74920B\", \"recipientName\": \"Mr. Jian Li, Operations Manager\", \"facilityName\": \"EagleCorp Manufacturing Plant - Unit 42\", \"address\": { \"street\": \"88 Manufacturing Drive, Innovation Valley Industrial Estate\", \"city\": \"Shenzhen\", \"province\": \"Guangdong\", \"postalCode\": \"518000\", \"country\": \"China\", \"deliveryZone\": \"Loading Dock B - Heavy Goods\" }, \"shippingMethod\": \"Secure Air Cargo - Expedited\", \"trackingNumber\": \"TRK-SACEXP-CN7766554433-B02\", \"notes\": \"Requires Forklift. Confirm delivery appointment 24hrs prior.\" } ], \"lineItems\": [ { \"itemId\": \"QN-CORE-X9000-PRO\", \"productCode\": \"PQC-SYS-001A-REV4\", \"description\": \"Quantum Entanglement Core Processor - Model X9000 Professional Edition. Includes integrated cryo-cooler and temporal displacement shielding. Firmware v7.8.2-alpha.\", \"servicePeriod\": \"N/A\", \"quantity\": 2, \"unit\": \"Unit(s)\", \"unitPrice\": 750000.00, \"discountPercentage\": 5.0, \"discountAmount\": 75000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 285000.00, \"subtotal\": 1425000.00, \"totalLineAmount\": 1710000.00, \"serialNumbers\": [\"SN-QECX9P-0000A1F8\", \"SN-QECX9P-0000A2C4\"], \"warrantyId\": \"WARR-QECX9P-5YR-PREM-001\" }, { \"itemId\": \"NANO-FAB-M7-ULTRA\", \"productCode\": \"NFM-DEV-007B-REV2\", \"description\": \"Advanced Nanite Fabricator - Model M7 Ultra. High precision, multi-material capability. Includes 12-month software subscription (Tier 1).\", \"servicePeriod\": \"N/A\", \"quantity\": 1, \"unit\": \"System\", \"unitPrice\": 1250000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 250000.00, \"subtotal\": 1250000.00, \"totalLineAmount\": 1500000.00, \"serialNumbers\": [\"SN-NFM7U-XYZ001B\"], \"warrantyId\": \"WARR-NFM7U-3YR-STD-002\" }, { \"itemId\": \"SVC-CONSULT-QIP-PH1\", \"productCode\": \"CS-QIP-001-PHASE1\", \"description\": \"Quantum Implementation Project - Phase 1 Consultation Services. On-site engineering support, system integration planning, and initial staff training (400 hours block).\", \"servicePeriod\": \"2025-06-01 to 2025-08-31\", \"quantity\": 400, \"unit\": \"Hour(s)\", \"unitPrice\": 850.00, \"discountPercentage\": 10.0, \"discountAmount\": 34000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 306000.00, \"totalLineAmount\": 306000.00, \"projectCode\": \"PROJ-EAGLE-QIP-2025\", \"consultantId\": [\"CONS-DR-EVA-ROSTOVA\", \"CONS-RAJ-SINGH-ENG\"] }, { \"itemId\": \"MAT-CRYOFLUID-XF100\", \"productCode\": \"CHEM-CRYO-003C\", \"description\": \"Cryogenic Cooling Fluid - Type XF-100. Ultra-low temperature stability. Non-conductive. (Sold in 200L insulated containers)\", \"servicePeriod\": \"N/A\", \"quantity\": 10, \"unit\": \"Container(s)\", \"unitPrice\": 15000.00, \"discountPercentage\": 2.5, \"discountAmount\": 3750.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 29250.00, \"subtotal\": 146250.00, \"totalLineAmount\": 175500.00, \"batchNumbers\": [\"BATCH-XF100-2501A01\", \"BATCH-XF100-2501A02\", \"BATCH-XF100-2501A03\", \"BATCH-XF100-2501A04\", \"BATCH-XF100-2501A05\", \"BATCH-XF100-2501A06\", \"BATCH-XF100-2501A07\", \"BATCH-XF100-2501A08\", \"BATCH-XF100-2501A09\", \"BATCH-XF100-2501A10\"], \"shelfLife\": \"24 Months from DOM\" }, { \"itemId\": \"SOFT-LICENSE-QAI-ENT\", \"productCode\": \"SL-QAI-ENT-001-5YR\", \"description\": \"Quantum AI Algorithmic Suite - Enterprise License. 5-Year Subscription. Unlimited User Access. Includes Premium Support Package (PSP-GOLD-001).\", \"servicePeriod\": \"2025-06-01 to 2030-05-31\", \"quantity\": 1, \"unit\": \"License\", \"unitPrice\": 450000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 450000.00, \"totalLineAmount\": 450000.00, \"licenseKey\": \"LIC-QAIENT-XG74920B-ABC123XYZ789-EAGLECORP\", \"supportContractId\": \"SUP-PSP-GOLD-001-XG74920B\" }, { \"itemId\": \"COMP-SENSOR-ARRAY-SIGMA\", \"productCode\": \"SNS-ARR-SGM-004D\", \"description\": \"Multi-Dimensional Sensor Array - Sigma Series. High-sensitivity, wide spectrum coverage. Includes calibration certificate traceable to NIST/NPL.\", \"servicePeriod\": \"N/A\", \"quantity\": 8, \"unit\": \"Unit(s)\", \"unitPrice\": 22000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 35200.00, \"subtotal\": 176000.00, \"totalLineAmount\": 211200.00, \"serialNumbers\": [\"SN-MDSA-SGM-0101\", \"SN-MDSA-SGM-0102\", \"SN-MDSA-SGM-0103\", \"SN-MDSA-SGM-0104\", \"SN-MDSA-SGM-0105\", \"SN-MDSA-SGM-0106\", \"SN-MDSA-SGM-0107\", \"SN-MDSA-SGM-0108\"], \"calibrationDate\": \"2025-05-15\" }, { \"itemId\": \"MAINT-KIT-ADV-ROBOTICS\", \"productCode\": \"MNT-KIT-ROBO-002A\", \"description\": \"Advanced Robotics Maintenance Toolkit. Includes specialized diagnostic tools and Class-5 cleanroom consumables. For AR-700 and AR-800 series.\", \"servicePeriod\": \"N/A\", \"quantity\": 5, \"unit\": \"Kit(s)\", \"unitPrice\": 7500.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 7500.00, \"subtotal\": 37500.00, \"totalLineAmount\": 45000.00, \"componentListId\": \"CL-MNTROBO-002A-V3\" }, { \"itemId\": \"DATA-STORAGE-CRYSTAL-1PB\", \"productCode\": \"DSC-1PB-HG-009\", \"description\": \"Holographic Data Storage Crystal - 1 Petabyte Capacity. Archival Grade. Read/Write Speed: 50 GB/s. Phase-change matrix type.\", \"servicePeriod\": \"N/A\", \"quantity\": 20, \"unit\": \"Crystal(s)\", \"unitPrice\": 18000.00, \"discountPercentage\": 10.0, \"discountAmount\": 36000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 64800.00, \"subtotal\": 324000.00, \"totalLineAmount\": 388800.00, \"serialNumbers\": [\"SN-DSC1PB-HG-A001F to SN-DSC1PB-HG-A001P\", \"SN-DSC1PB-HG-B002A to SN-DSC1PB-HG-B002D\"], \"dataIntegrityCert\": \"DIC-HG9-20250520-BATCH01\" } ], \"summary\": { \"subtotalBeforeDiscounts\": 4128500.00, \"totalDiscountAmount\": 148750.00, \"subtotalAfterDiscounts\": 3979750.00, \"totalTaxAmount\": 671750.00, \"shippingAndHandling\": [ { \"description\": \"Cryo-Stasis Freight - Priority Overnight (SHIP-ALPHA-001)\", \"chargeCode\": \"SHP-CRYO-PRIO-INTL\", \"amount\": 12500.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Secure Air Cargo - Expedited (SHIP-BETA-002)\", \"chargeCode\": \"SHP-SAC-EXP-CN\", \"amount\": 8800.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Special Handling - Temperature Sensitive & High Value Goods\", \"chargeCode\": \"HDL-SPECREQ-HVTS\", \"amount\": 5500.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 1100.00 }, { \"description\": \"Customs Clearance & Documentation Fee - International\", \"chargeCode\": \"FEE-CUSTOMS-INTL-001\", \"amount\": 2750.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Transit Insurance - Full Value Coverage\", \"chargeCode\": \"INS-TRANSIT-FULL-XG74920B\", \"amount\": 25000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 } ], \"totalShippingAndHandling\": 54550.00, \"totalShippingAndHandlingTax\": 1100.00, \"grandTotal\": 4707150.00, \"amountPaid\": 0.00, \"amountDue\": 4707150.00 }, \"paymentInstructions\": { \"preferredMethod\": \"Wire Transfer\", \"paymentReference\": \"INV-20250523-XG-74920B / CUST-EAGLECORP-GLOBAL-007\", \"latePaymentPenalty\": \"1.5% per month on outstanding balance after due date.\", \"earlyPaymentDiscount\": \"1 % d iscount if paid within 10 days (Amount: $47071.50, New Total: $4660078.50). Reference EPD-XG74920B if claiming.\", \"alternativePayments\": [ { \"method\": \"Secured Crypto Transfer (USDC or ETH)\", \"details\": \"Wallet Address: 0x1234ABCD5678EFGH9012IJKL3456MNOP7890QRST. Memo: XG74920B. Confirmation required via secure_payments@quantumsynergistics-ans.co.uk\" }, { \"method\": \"Irrevocable Letter of Credit (ILOC)\", \"details\": \"To be issued by a Prime Bank, acceptable to Quantum Synergistics. Contact accounts_receivable@quantumsynergistics-ans.co.uk for ILOC requirements.\" } ] }, \"notesAndRemarks\": [ \"All hardware components are subject to export control regulations (EAR/ITAR where applicable). Compliance documentation attached separately (DOC-REF: EXPCOMPL-XG74920B).\", \"Software licenses are non-transferable and subject to the End User License Agreement (EULA-QSANS-V4.2).\", \"On-site consultation hours are estimates. Additional hours will be billed separately under addendum A1 of contract CS-QIP-001.\", \"Warranty claims must be submitted via the online portal at support.quantumsynergistics-ans.co.uk using the provided Warranty IDs.\", \"Return Material Authorization (RMA) required for all returns. Contact customer support for RMA number. Restocking fees may apply (15-25% based on product type and condition). See detailed Return Policy (POL-RET-QSANS-2025-V2).\", \"Projected delivery dates for back-ordered sub-components (Ref: SUBCOMP-BO-LIST-XG74920B-01) will be communicated by your account manager within 7 business days.\" ], \"attachments\": [ {\"documentName\": \"QSANS_Product_Specification_Sheets_Q2_2025.pdf\", \"fileId\": \"DOC-SPECS-QSANS-Q22025-V1.3\"}, {\"documentName\": \"EULA_QSANS_Software_V4.2.pdf\", \"fileId\": \"DOC-EULA-QSANS-V4.2\"}, {\"documentName\": \"Warranty_Terms_and_Conditions_Premium_Standard.pdf\", \"fileId\": \"DOC-WARR-QSANS-PREMSTD-V3.1\"}, {\"documentName\": \"Export_Compliance_Declaration_XG74920B.pdf\", \"fileId\": \"DOC-EXPCOMPL-XG74920B\"}, {\"documentName\": \"Return_Policy_QSANS_2025_V2.pdf\", \"fileId\": \"DOC-POL-RET-QSANS-2025-V2\"}, {\"documentName\": \"Consultation_Services_SOW_PROJ-EAGLE-QIP-2025.pdf\", \"fileId\": \"DOC-SOW-EAGLE-QIP-2025-PH1\"} ], \"approvalWorkflow\": { \"issuerApproval\": { \"approverName\": \"Mr. Alistair Finch\", \"approverTitle\": \"Head of Commercial Operations\", \"approvalDate\": \"2025-05-23\", \"signatureId\": \"SIG-AFINCH-QSANS-20250523-001A\" }, \"clientAcknowledgmentRequired\": true, \"clientAcknowledgmentInstructions\": \"Please sign and return a copy of this invoice or confirm receipt and acceptance via email to billing@quantumsynergistics-ans.co.uk within 5 business days.\" }, \"versionHistory\": [ {\"version\": 1.0, \"date\": \"2025-05-23\", \"reason\": \"Initial Draft\", \"editorId\": \"SYS-AUTOINV-GEN\"}, {\"version\": 1.1, \"date\": \"2025-05-23\", \"reason\": \"Added shipping details and corrected tax calculation for item QN-CORE-X9000-PRO.\", \"editorId\": \"USER-CFO-REVIEW-BOT\"} ], \"footerMessage\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd. - Pioneering the Future, Today. Thank you for your business. For support, please visit our dedicated portal or contact your account representative. All transactions are governed by the laws of England and Wales. Registered Office: 121B, Innovation Drive, London, EC1Y 8XZ, UK. Company Reg No: REG-LND-09876543X. VAT No: VAT-GB-293847261.\" } ''' # Invoice Example invoice=''' { \"invoiceId\": \"INV-20250523-XG-74920B\", \"orderReference\": \"ORD-PROC-Q2-2025-ALPHA-99374-DELTA\", \"customerIdentification\": \"CUST-EAGLECORP-GLOBAL-007\", \"dateIssued\": \"2025-05-23\", \"dueDate\": \"2025-06-22\", \"paymentTerms\": \"Net 30 Days\", \"currency\": \"USD\", \"issuerDetails\": { \"companyName\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd.\", \"taxId\": \"VAT-GB-293847261\", \"registrationNumber\": \"REG-LND-09876543X\", \"address\": { \"street\": \"121B, Innovation Drive, Silicon Roundabout, Tech City East\", \"city\": \"London\", \"postalCode\": \"EC1Y 8XZ\", \"country\": \"United Kingdom\", \"planet\": \"Earth\", \"dimension\": \"Sigma-7\" }, \"contact\": { \"primaryPhone\": \"+44-20-7946-0001 ext. 777\", \"secondaryPhone\": \"+44-20-7946-0002\", \"fax\": \"+44-20-7946-0003\", \"email\": \"billing@quantumsynergistics-ans.co.uk\", \"website\": \"www.quantumsynergistics-ans.co.uk\" }, \"bankDetails\": { \"bankName\": \"Universal Interstellar Bank PLC\", \"accountName\": \"Quantum Synergistics & ANS Ltd.\", \"accountNumber\": \"9876543210123456\", \"swiftBic\": \"UNIVGB2LXXX\", \"iban\": \"GB29 UNIV 9876 5432 1012 3456 78\", \"reference\": \"INV-20250523-XG-74920B\" } }, \"billingInformation\": { \"companyName\": \"EagleCorp Global Holdings Inc. & Subsidiaries\", \"department\": \"Strategic Procurement & Interstellar Logistics Division\", \"attentionTo\": \"Ms. Evelyn Reed, Chief Procurement Officer (CPO)\", \"taxId\": \"EIN-US-98-7654321X\", \"clientReferenceId\": \"EGL-PROC-REF-Q2-2025-7734-GAMMA\", \"address\": { \"street\": \"Suite 9870, Eagle Tower One, 1500 Constitution Avenue NW\", \"city\": \"Washington D.C.\", \"state\": \"District of Columbia\", \"postalCode\": \"20001-1500\", \"country\": \"United States of America\" }, \"contact\": { \"phone\": \"+1-202-555-0189 ext. 1234\", \"email\": \"e.reed.procurement@eaglecorpglobal.com\" } }, \"shippingInformation\": [ { \"shipmentId\": \"SHIP-ALPHA-001-XG74920B\", \"recipientName\": \"Dr. Aris Thorne, Head of R&D\", \"facilityName\": \"EagleCorp Advanced Research Facility - Sector Gamma-7\", \"address\": { \"street\": \"Docking Bay 7, 47 Industrial Park Road\", \"city\": \"New Chicago\", \"state\": \"Illinois\", \"postalCode\": \"60699-0047\", \"country\": \"United States of America\", \"deliveryZone\": \"Restricted Access - Level 3 Clearance Required\" }, \"shippingMethod\": \"Cryo-Stasis Freight - Priority Overnight\", \"trackingNumber\": \"TRK-CSFPON-9988776655-A01\", \"notes\": \"Deliver between 08:00 - 10:00 Local Time. Handle with Extreme Care. Temperature Sensitive Materials.\" }, { \"shipmentId\": \"SHIP-BETA-002-XG74920B\", \"recipientName\": \"Mr. Jian Li, Operations Manager\", \"facilityName\": \"EagleCorp Manufacturing Plant - Unit 42\", \"address\": { \"street\": \"88 Manufacturing Drive, Innovation Valley Industrial Estate\", \"city\": \"Shenzhen\", \"province\": \"Guangdong\", \"postalCode\": \"518000\", \"country\": \"China\", \"deliveryZone\": \"Loading Dock B - Heavy Goods\" }, \"shippingMethod\": \"Secure Air Cargo - Expedited\", \"trackingNumber\": \"TRK-SACEXP-CN7766554433-B02\", \"notes\": \"Requires Forklift. Confirm delivery appointment 24hrs prior.\" } ], \"lineItems\": [ { \"itemId\": \"QN-CORE-X9000-PRO\", \"productCode\": \"PQC-SYS-001A-REV4\", \"description\": \"Quantum Entanglement Core Processor - Model X9000 Professional Edition. Includes integrated cryo-cooler and temporal displacement shielding. Firmware v7.8.2-alpha.\", \"servicePeriod\": \"N/A\", \"quantity\": 2, \"unit\": \"Unit(s)\", \"unitPrice\": 750000.00, \"discountPercentage\": 5.0, \"discountAmount\": 75000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 285000.00, \"subtotal\": 1425000.00, \"totalLineAmount\": 1710000.00, \"serialNumbers\": [\"SN-QECX9P-0000A1F8\", \"SN-QECX9P-0000A2C4\"], \"warrantyId\": \"WARR-QECX9P-5YR-PREM-001\" }, { \"itemId\": \"NANO-FAB-M7-ULTRA\", \"productCode\": \"NFM-DEV-007B-REV2\", \"description\": \"Advanced Nanite Fabricator - Model M7 Ultra. High precision, multi-material capability. Includes 12-month software subscription (Tier 1).\", \"servicePeriod\": \"N/A\", \"quantity\": 1, \"unit\": \"System\", \"unitPrice\": 1250000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 250000.00, \"subtotal\": 1250000.00, \"totalLineAmount\": 1500000.00, \"serialNumbers\": [\"SN-NFM7U-XYZ001B\"], \"warrantyId\": \"WARR-NFM7U-3YR-STD-002\" }, { \"itemId\": \"SVC-CONSULT-QIP-PH1\", \"productCode\": \"CS-QIP-001-PHASE1\", \"description\": \"Quantum Implementation Project - Phase 1 Consultation Services. On-site engineering support, system integration planning, and initial staff training (400 hours block).\", \"servicePeriod\": \"2025-06-01 to 2025-08-31\", \"quantity\": 400, \"unit\": \"Hour(s)\", \"unitPrice\": 850.00, \"discountPercentage\": 10.0, \"discountAmount\": 34000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 306000.00, \"totalLineAmount\": 306000.00, \"projectCode\": \"PROJ-EAGLE-QIP-2025\", \"consultantId\": [\"CONS-DR-EVA-ROSTOVA\", \"CONS-RAJ-SINGH-ENG\"] }, { \"itemId\": \"MAT-CRYOFLUID-XF100\", \"productCode\": \"CHEM-CRYO-003C\", \"description\": \"Cryogenic Cooling Fluid - Type XF-100. Ultra-low temperature stability. Non-conductive. (Sold in 200L insulated containers)\", \"servicePeriod\": \"N/A\", \"quantity\": 10, \"unit\": \"Container(s)\", \"unitPrice\": 15000.00, \"discountPercentage\": 2.5, \"discountAmount\": 3750.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 29250.00, \"subtotal\": 146250.00, \"totalLineAmount\": 175500.00, \"batchNumbers\": [\"BATCH-XF100-2501A01\", \"BATCH-XF100-2501A02\", \"BATCH-XF100-2501A03\", \"BATCH-XF100-2501A04\", \"BATCH-XF100-2501A05\", \"BATCH-XF100-2501A06\", \"BATCH-XF100-2501A07\", \"BATCH-XF100-2501A08\", \"BATCH-XF100-2501A09\", \"BATCH-XF100-2501A10\"], \"shelfLife\": \"24 Months from DOM\" }, { \"itemId\": \"SOFT-LICENSE-QAI-ENT\", \"productCode\": \"SL-QAI-ENT-001-5YR\", \"description\": \"Quantum AI Algorithmic Suite - Enterprise License. 5-Year Subscription. Unlimited User Access. Includes Premium Support Package (PSP-GOLD-001).\", \"servicePeriod\": \"2025-06-01 to 2030-05-31\", \"quantity\": 1, \"unit\": \"License\", \"unitPrice\": 450000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00, \"subtotal\": 450000.00, \"totalLineAmount\": 450000.00, \"licenseKey\": \"LIC-QAIENT-XG74920B-ABC123XYZ789-EAGLECORP\", \"supportContractId\": \"SUP-PSP-GOLD-001-XG74920B\" }, { \"itemId\": \"COMP-SENSOR-ARRAY-SIGMA\", \"productCode\": \"SNS-ARR-SGM-004D\", \"description\": \"Multi-Dimensional Sensor Array - Sigma Series. High-sensitivity, wide spectrum coverage. Includes calibration certificate traceable to NIST/NPL.\", \"servicePeriod\": \"N/A\", \"quantity\": 8, \"unit\": \"Unit(s)\", \"unitPrice\": 22000.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 35200.00, \"subtotal\": 176000.00, \"totalLineAmount\": 211200.00, \"serialNumbers\": [\"SN-MDSA-SGM-0101\", \"SN-MDSA-SGM-0102\", \"SN-MDSA-SGM-0103\", \"SN-MDSA-SGM-0104\", \"SN-MDSA-SGM-0105\", \"SN-MDSA-SGM-0106\", \"SN-MDSA-SGM-0107\", \"SN-MDSA-SGM-0108\"], \"calibrationDate\": \"2025-05-15\" }, { \"itemId\": \"MAINT-KIT-ADV-ROBOTICS\", \"productCode\": \"MNT-KIT-ROBO-002A\", \"description\": \"Advanced Robotics Maintenance Toolkit. Includes specialized diagnostic tools and Class-5 cleanroom consumables. For AR-700 and AR-800 series.\", \"servicePeriod\": \"N/A\", \"quantity\": 5, \"unit\": \"Kit(s)\", \"unitPrice\": 7500.00, \"discountPercentage\": 0.0, \"discountAmount\": 0.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 7500.00, \"subtotal\": 37500.00, \"totalLineAmount\": 45000.00, \"componentListId\": \"CL-MNTROBO-002A-V3\" }, { \"itemId\": \"DATA-STORAGE-CRYSTAL-1PB\", \"productCode\": \"DSC-1PB-HG-009\", \"description\": \"Holographic Data Storage Crystal - 1 Petabyte Capacity. Archival Grade. Read/Write Speed: 50 GB/s. Phase-change matrix type.\", \"servicePeriod\": \"N/A\", \"quantity\": 20, \"unit\": \"Crystal(s)\", \"unitPrice\": 18000.00, \"discountPercentage\": 10.0, \"discountAmount\": 36000.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 64800.00, \"subtotal\": 324000.00, \"totalLineAmount\": 388800.00, \"serialNumbers\": [\"SN-DSC1PB-HG-A001F to SN-DSC1PB-HG-A001P\", \"SN-DSC1PB-HG-B002A to SN-DSC1PB-HG-B002D\"], \"dataIntegrityCert\": \"DIC-HG9-20250520-BATCH01\" } ], \"summary\": { \"subtotalBeforeDiscounts\": 4128500.00, \"totalDiscountAmount\": 148750.00, \"subtotalAfterDiscounts\": 3979750.00, \"totalTaxAmount\": 671750.00, \"shippingAndHandling\": [ { \"description\": \"Cryo-Stasis Freight - Priority Overnight (SHIP-ALPHA-001)\", \"chargeCode\": \"SHP-CRYO-PRIO-INTL\", \"amount\": 12500.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Secure Air Cargo - Expedited (SHIP-BETA-002)\", \"chargeCode\": \"SHP-SAC-EXP-CN\", \"amount\": 8800.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Special Handling - Temperature Sensitive & High Value Goods\", \"chargeCode\": \"HDL-SPECREQ-HVTS\", \"amount\": 5500.00, \"taxRatePercentage\": 20.0, \"taxAmount\": 1100.00 }, { \"description\": \"Customs Clearance & Documentation Fee - International\", \"chargeCode\": \"FEE-CUSTOMS-INTL-001\", \"amount\": 2750.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 }, { \"description\": \"Transit Insurance - Full Value Coverage\", \"chargeCode\": \"INS-TRANSIT-FULL-XG74920B\", \"amount\": 25000.00, \"taxRatePercentage\": 0.0, \"taxAmount\": 0.00 } ], \"totalShippingAndHandling\": 54550.00, \"totalShippingAndHandlingTax\": 1100.00, \"grandTotal\": 4707150.00, \"amountPaid\": 0.00, \"amountDue\": 4707150.00 }, \"paymentInstructions\": { \"preferredMethod\": \"Wire Transfer\", \"paymentReference\": \"INV-20250523-XG-74920B / CUST-EAGLECORP-GLOBAL-007\", \"latePaymentPenalty\": \"1.5% per month on outstanding balance after due date.\", \"earlyPaymentDiscount\": \"1% discount if paid within 10 days (Amount: $47071.50, New Total: $4660078.50). Reference EPD-XG74920B if claiming.\", \"alternativePayments\": [ { \"method\": \"Secured Crypto Transfer (USDC or ETH)\", \"details\": \"Wallet Address: 0x1234ABCD5678EFGH9012IJKL3456MNOP7890QRST. Memo: XG74920B. Confirmation required via secure_payments@quantumsynergistics-ans.co.uk\" }, { \"method\": \"Irrevocable Letter of Credit (ILOC)\", \"details\": \"To be issued by a Prime Bank, acceptable to Quantum Synergistics. Contact accounts_receivable@quantumsynergistics-ans.co.uk for ILOC requirements.\" } ] }, \"notesAndRemarks\": [ \"All hardware components are subject to export control regulations (EAR/ITAR where applicable). Compliance documentation attached separately (DOC-REF: EXPCOMPL-XG74920B).\", \"Software licenses are non-transferable and subject to the End User License Agreement (EULA-QSANS-V4.2).\", \"On-site consultation hours are estimates. Additional hours will be billed separately under addendum A1 of contract CS-QIP-001.\", \"Warranty claims must be submitted via the online portal at support.quantumsynergistics-ans.co.uk using the provided Warranty IDs.\", \"Return Material Authorization (RMA) required for all returns. Contact customer support for RMA number. Restocking fees may apply (15-25% based on product type and condition). See detailed Return Policy (POL-RET-QSANS-2025-V2).\", \"Projected delivery dates for back-ordered sub-components (Ref: SUBCOMP-BO-LIST-XG74920B-01) will be communicated by your account manager within 7 business days.\" ], \"attachments\": [ {\"documentName\": \"QSANS_Product_Specification_Sheets_Q2_2025.pdf\", \"fileId\": \"DOC-SPECS-QSANS-Q22025-V1.3\"}, {\"documentName\": \"EULA_QSANS_Software_V4.2.pdf\", \"fileId\": \"DOC-EULA-QSANS-V4.2\"}, {\"documentName\": \"Warranty_Terms_and_Conditions_Premium_Standard.pdf\", \"fileId\": \"DOC-WARR-QSANS-PREMSTD-V3.1\"}, {\"documentName\": \"Export_Compliance_Declaration_XG74920B.pdf\", \"fileId\": \"DOC-EXPCOMPL-XG74920B\"}, {\"documentName\": \"Return_Policy_QSANS_2025_V2.pdf\", \"fileId\": \"DOC-POL-RET-QSANS-2025-V2\"}, {\"documentName\": \"Consultation_Services_SOW_PROJ-EAGLE-QIP-2025.pdf\", \"fileId\": \"DOC-SOW-EAGLE-QIP-2025-PH1\"} ], \"approvalWorkflow\": { \"issuerApproval\": { \"approverName\": \"Mr. Alistair Finch\", \"approverTitle\": \"Head of Commercial Operations\", \"approvalDate\": \"2025-05-23\", \"signatureId\": \"SIG-AFINCH-QSANS-20250523-001A\" }, \"clientAcknowledgmentRequired\": true, \"clientAcknowledgmentInstructions\": \"Please sign and return a copy of this invoice or confirm receipt and acceptance via email to billing@quantumsynergistics-ans.co.uk within 5 business days.\" }, \"versionHistory\": [ {\"version\": 1.0, \"date\": \"2025-05-23\", \"reason\": \"Initial Draft\", \"editorId\": \"SYS-AUTOINV-GEN\"}, {\"version\": 1.1, \"date\": \"2025-05-23\", \"reason\": \"Added shipping details and corrected tax calculation for item QN-CORE-X9000-PRO.\", \"editorId\": \"USER-CFO-REVIEW-BOT\"} ], \"footerMessage\": \"Quantum Synergistics & Advanced Nanotech Solutions Ltd. - Pioneering the Future, Today. Thank you for your business. For support, please visit our dedicated portal or contact your account representative. All transactions are governed by the laws of England and Wales. Registered Office: 121B, Innovation Drive, London, EC1Y 8XZ, UK. Company Reg No: REG-LND-09876543X. VAT No: VAT-GB-293847261.\" } ''' With the context above, let's create two examples, one where the answer from the model is the correct ID and the other is missing just one character. In [12]: Copied! # Long context examples to test reliability check in invoice processing invoice_examples = [ { \"prompt\" : \"What is the Secure Air Cargo code?\" , \"output\" : \"The Secure Air Cargo code is SHP-SAC-EXP-CN.\" , \"context\" : invoice , \"expected_unreliable\" : False }, { \"prompt\" : \"What is the Secure Air Cargo code?\" , \"output\" : \"The Secure Air Cargo code is HP-SAC-EXP-CN.\" , \"context\" : invoice , \"expected_unreliable\" : True } ] # Long context examples to test reliability check in invoice processing invoice_examples = [ { \"prompt\": \"What is the Secure Air Cargo code?\", \"output\": \"The Secure Air Cargo code is SHP-SAC-EXP-CN.\", \"context\": invoice, \"expected_unreliable\": False }, { \"prompt\": \"What is the Secure Air Cargo code?\", \"output\": \"The Secure Air Cargo code is HP-SAC-EXP-CN.\", \"context\": invoice, \"expected_unreliable\": True } ] Now, by comparing these two answers, we can test the Verify reliability check response: In [15]: Copied! # Test long context validation with invoice print ( \"=== Long Context - Invoice Processing Examples === \\n \" ) print ( f \"Question: { invoice_examples [ 0 ][ 'prompt' ] } \\n \" ) # Run the first example for i , example in enumerate ( invoice_examples ): # Print the question and expected answer type answer_type = \"Correct Answer\" if not example [ 'expected_unreliable' ] else \"Wrong Answer\" print ( f \"#Run { i + 1 } - { answer_type } \" ) # Run the reliability check on the invoice example result = check_reliability_qa ( prompt = example [ 'prompt' ], output = example [ 'output' ], context = example [ 'context' ], return_search_results = False ) # Print the results print ( f \"a- Is Unreliable: { result . get ( 'is_hallucination' , 'N/A' ) } \" ) print ( f \"b- Expected value: { example [ 'expected_unreliable' ] } \" ) explanation = result . get ( 'explanation' , 'N/A' ) # Limit explanation to max 400 characters max_chars = 400 short_explanation = explanation [: max_chars ] + \"...\" if len ( explanation ) > max_chars else explanation print ( f \"c- Short summary of explanation: { short_explanation } \" ) print ( \"--\" ) print () # Test long context validation with invoice print(\"=== Long Context - Invoice Processing Examples ===\\n\") print(f\"Question: {invoice_examples[0]['prompt']}\\n\") # Run the first example for i, example in enumerate(invoice_examples): # Print the question and expected answer type answer_type = \"Correct Answer\" if not example['expected_unreliable'] else \"Wrong Answer\" print(f\"#Run {i+1} - {answer_type}\") # Run the reliability check on the invoice example result = check_reliability_qa( prompt=example['prompt'], output=example['output'], context=example['context'], return_search_results=False ) # Print the results print(f\"a- Is Unreliable: {result.get('is_hallucination', 'N/A')}\") print(f\"b- Expected value: {example['expected_unreliable']}\") explanation = result.get('explanation', 'N/A') # Limit explanation to max 400 characters max_chars = 400 short_explanation = explanation[:max_chars] + \"...\" if len(explanation) > max_chars else explanation print(f\"c- Short summary of explanation: {short_explanation}\") print(\"--\") print() === Long Context - Invoice Processing Examples === Question: What is the Secure Air Cargo code? #Run 1 - Correct Answer a- Is Unreliable: False b- Expected value: False c- Short summary of explanation: The DOCUMENT contains a JSON object representing an invoice with various details including shipping information. Under the 'shippingAndHandling' section within 'summary', there is a list of charges, one of which is described as 'Secure Air Cargo - Expedited (SHIP-BETA-002)' with the charge code 'SHP-SAC-EXP-CN'. This matches the information given in the ANSWER. -- #Run 2 - Wrong Answer a- Is Unreliable: True b- Expected value: True c- Short summary of explanation: To determine whether the provided answer is faithful to the contents of the DOCUMENT, we need to examine the information given in the DOCUMENT and compare it with the ANSWER. The QUESTION asks for the Secure Air Cargo code. Upon reviewing the DOCUMENT, we find that it contains detailed information about an invoice, including shipping information for various items. Specifically, under \"shippingInf... -- OpenAI chat completion endpoint ¬∂ The reliability check conducted via the OpenAI chat completion endpoint method validates multi-turn conversations for reliability issues. This is ideal for conversational AI systems and chatbots. Prepare the data ¬∂ For this scenario, we need to provide the promp via the user role, and the answer from the LLM via the assistant role. We set expected_unreliable as the ground truth for comparison. In [18]: Copied! # Chat conversation examples for Chat Completion checks chat_examples = [ { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant that provides accurate medical information.\" }, { \"role\" : \"user\" , \"content\" : \"Does vitamin C cure the common cold?\" }, { \"role\" : \"assistant\" , \"content\" : \"Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours.\" } ], \"expected_unreliable\" : True }, { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a knowledgeable financial advisor.\" }, { \"role\" : \"user\" , \"content\" : \"What is compound interest?\" }, { \"role\" : \"assistant\" , \"content\" : \"Compound interest is when you earn interest on both your original investment and the interest that has already been earned. It causes your money to grow exponentially over time.\" } ], \"expected_unreliable\" : False } ] # Chat conversation examples for Chat Completion checks chat_examples = [ { \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate medical information.\"}, {\"role\": \"user\", \"content\": \"Does vitamin C cure the common cold?\"}, {\"role\": \"assistant\", \"content\": \"Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours.\"} ], \"expected_unreliable\": True }, { \"messages\": [ {\"role\": \"system\", \"content\": \"You are a knowledgeable financial advisor.\"}, {\"role\": \"user\", \"content\": \"What is compound interest?\"}, {\"role\": \"assistant\", \"content\": \"Compound interest is when you earn interest on both your original investment and the interest that has already been earned. It causes your money to grow exponentially over time.\"} ], \"expected_unreliable\": False } ] Reliability check via the OpenAI client ¬∂ In [19]: Copied! # Function for Chat Completion reliability check def check_reliability_chat ( messages ): \"\"\"Check reliability in chat conversations using OpenAI library\"\"\" # Create a separate client for chat completions with the correct base URL client = OpenAI ( base_url = base_url , api_key = api_key , ) # Make the request using OpenAI client - pass parameters directly response = client . chat . completions . create ( model = \"klusterai/verify-reliability\" , #Reliability model messages = messages ) # Parse the response - kluster.ai returns check results in a specific format return response . choices [ 0 ] . message . content # Function for Chat Completion reliability check def check_reliability_chat(messages): \"\"\"Check reliability in chat conversations using OpenAI library\"\"\" # Create a separate client for chat completions with the correct base URL client = OpenAI( base_url=base_url, api_key=api_key, ) # Make the request using OpenAI client - pass parameters directly response = client.chat.completions.create( model=\"klusterai/verify-reliability\", #Reliability model messages=messages ) # Parse the response - kluster.ai returns check results in a specific format return response.choices[0].message.content In [20]: Copied! # Test Chat Completion checks with our examples chat_results = [] for i , example in enumerate ( chat_examples ): print ( f \"=== Chat Example { i + 1 } ===\" ) print ( f \"System: { example [ 'messages' ][ 0 ][ 'content' ] } \" ) print ( f \"User: { example [ 'messages' ][ 1 ][ 'content' ] } \" ) print ( f \"Assistant: { example [ 'messages' ][ 2 ][ 'content' ] } \" ) print ( f \"Expected Unreliable: { example [ 'expected_unreliable' ] } \" ) print () try : result = check_reliability_chat ( messages = example [ 'messages' ], ) chat_results . append ({ 'example' : example , 'result' : result }) print ( \"Check Result:\" ) print ( f \"Result: { result } \" ) except Exception as e : print ( f \"Error processing chat example: { e } \" ) print ( \" \\n \" + \"=\" * 80 + \" \\n \" ) # Summary of chat checks print ( \"### Chat Check Summary\" ) print ( f \"Processed { len ( chat_results ) } chat conversations\" ) for i , result in enumerate ( chat_results ): expected = result [ 'example' ][ 'expected_unreliable' ] print ( f \"Chat { i + 1 } : Expected unreliable = { expected } \" ) # Test Chat Completion checks with our examples chat_results = [] for i, example in enumerate(chat_examples): print(f\"=== Chat Example {i+1} ===\") print(f\"System: {example['messages'][0]['content']}\") print(f\"User: {example['messages'][1]['content']}\") print(f\"Assistant: {example['messages'][2]['content']}\") print(f\"Expected Unreliable: {example['expected_unreliable']}\") print() try: result = check_reliability_chat( messages=example['messages'], ) chat_results.append({ 'example': example, 'result': result }) print(\"Check Result:\") print(f\"Result: {result}\") except Exception as e: print(f\"Error processing chat example: {e}\") print(\"\\n\" + \"=\"*80 + \"\\n\") # Summary of chat checks print(\"### Chat Check Summary\") print(f\"Processed {len(chat_results)} chat conversations\") for i, result in enumerate(chat_results): expected = result['example']['expected_unreliable'] print(f\"Chat {i+1}: Expected unreliable = {expected}\") === Chat Example 1 === System: You are a helpful assistant that provides accurate medical information. User: Does vitamin C cure the common cold? Assistant: Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours. Expected Unreliable: True Error processing chat example: Error code: 500 - {'error': {'message': 'Unexpected error occurred', 'errorCode': 4000, 'type': 'invalid_request_error'}} ================================================================================ === Chat Example 2 === System: You are a knowledgeable financial advisor. User: What is compound interest? Assistant: Compound interest is when you earn interest on both your original investment and the interest that has already been earned. It causes your money to grow exponentially over time. Expected Unreliable: False Error processing chat example: Error code: 500 - {'error': {'message': 'Unexpected error occurred', 'errorCode': 4000, 'type': 'invalid_request_error'}} ================================================================================ ### Chat Check Summary Processed 0 chat conversations Summary ¬∂ This tutorial demonstrated how to use the reliability check feature of the Verify service to identify and prevent reliability issues in AI outputs. In this particular example, we used the dedicated reliability endpoint and the OpenAI-compatible method via the chat completions endpoint. Some key takeaways: Two reliability check methods : A dedicated endpoint for Q/A verifications, and the OpenAI-compatible chat completion endpoint for conversations. Two operation modes : General knowledge verification and context-based validation. Detailed explanations : The service provides clear reasoning for its determinations. Transparent verification : With return_search_results enabled, the service provides a list of sources used for verification. This helps users understand the basis for each reliability decision thereby increasing trust in the results.",
      "scraped_at": 1749147203.7004805
    },
    "https://docs.kluster.ai/get-started/start-building/batch/": {
      "url": "https://docs.kluster.ai/get-started/start-building/batch/",
      "title": "Perform batch inference jobs | kluster.ai Docs",
      "content": "Perform batch inference jobs ÔºÉ Overview ÔºÉ This guide provides examples and instructions on how to create, submit, retrieve, and manage batch inference jobs using the kluster.ai API. You will find guidance about preparing your data, selecting a model, submitting your batch job, and retrieving your results. Please make sure you check the API request limits . Prerequisites ÔºÉ This guide assumes familiarity with Large Language Model (LLM) development and OpenAI libraries. Before getting started, make sure you have: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. A virtual Python environment - (optional) recommended for developers using Python. It helps isolate Python installations in a virtual environment to reduce the risk of environment or package conflicts between your projects Required Python libraries - install the following Python libraries: OpenAI Python API library - to access the openai module getpass - to handle API keys safely A basic understanding of JSON Lines (JSONL) - JSONL is the required text input format for performing batch inferences with the kluster.ai API If you plan to use cURL via the CLI, you can export your kluster.ai API key as a variable: export API_KEY = INSERT_API_KEY Supported models ÔºÉ Please visit the Models page to learn more about all the models supported by the kluster.ai batch API. In addition, you can see the complete list of available models programmatically using the list supported models endpoint. Batch job workflow overview ÔºÉ Working with batch jobs in the kluster.ai API involves the following steps: Create batch job file - prepare a JSON Lines file containing one or more chat completion requests to execute in the batch Upload batch job file - upload the file to kluster.ai to receive a unique file ID Start the batch job - initiate a new batch job using the file ID Monitor job progress - track the status of your batch job to ensure successful completion Retrieve results - once the job finishes, access and process the results as needed In addition to these core steps, this guide will give you hands-on experience to: Cancel a batch job - cancel an ongoing batch job before it completes List all batch jobs - review all of your batch jobs Warning For the free tier, the maximum number of batch requests (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. Quickstart snippets ÔºÉ The following code snippets provide a full end-to-end batch inference example for different models supported by kluster.ai. You can simply copy and paste the snippet into your local environment. Python ÔºÉ To use these snippets, run the Python script and enter your kluster.ai API key when prompted. DeepSeek-R1 # Batch completions with the DeepSeek-R1 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) DeepSeek-R1-0528 # Batch completions with the DeepSeek-R1-0528 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1-0528\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1-0528\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-R1-0528\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) DeepSeek-V3-0324 # Batch completions with the DeepSeek-V3-0324 model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Gemma 3 27B # Batch completions with the Gemma 3 27B model on kluster.ai from os import environ import json import time from getpass import getpass from openai import OpenAI # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"google/gemma-3-27b-it\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is this?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image1_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"google/gemma-3-27b-it\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Extract the text, find typos if any.\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image2_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"google/gemma-3-27b-it\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image3_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again print ( f \" \\n Image1 URL: { image1_url } \" ) print ( f \" \\n Image2 URL: { image2_url } \" ) print ( f \" \\n Image3 URL: { image3_url } \" ) # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) print ( batch_status ) Meta Llama 3.1 8B # Batch completions with the Meta Llama 3.1 8B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Meta Llama 3.3 70B # Batch completions with the Meta Llama 3.3 70B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Meta Llama 4 Maverick # Batch completions with the Meta Llama 4 Maverick model on kluster.ai from os import environ import json import time from getpass import getpass from openai import OpenAI # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is this?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image1_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Extract the text, find typos if any.\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image2_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image3_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again print ( f \" \\n Image1 URL: { image1_url } \" ) print ( f \" \\n Image2 URL: { image2_url } \" ) print ( f \" \\n Image3 URL: { image3_url } \" ) # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) print ( batch_status ) Meta Llama 4 Scout # Batch completions with the Meta Llama 4 Scout model on kluster.ai from os import environ import json import time from getpass import getpass from openai import OpenAI # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is this?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image1_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Extract the text, find typos if any.\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image2_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image3_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again print ( f \" \\n Image1 URL: { image1_url } \" ) print ( f \" \\n Image2 URL: { image2_url } \" ) print ( f \" \\n Image3 URL: { image3_url } \" ) # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) print ( batch_status ) Mistral NeMo # Batch completions with the Mistral NeMo model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"mistralai/Mistral-Nemo-Instruct-2407\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"mistralai/Mistral-Nemo-Instruct-2407\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"mistralai/Mistral-Nemo-Instruct-2407\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Qwen2.5-VL 7B # Batch completions with the Qwen2.5-VL 7B model on kluster.ai from os import environ import json import time from getpass import getpass from openai import OpenAI # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What is this?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image1_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Extract the text, find typos if any.\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image2_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image3_url }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again print ( f \" \\n Image1 URL: { image1_url } \" ) print ( f \" \\n Image2 URL: { image2_url } \" ) print ( f \" \\n Image3 URL: { image3_url } \" ) # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) print ( batch_status ) Qwen3-235B-A22B # Batch completions with the Qwen3-235B-A22B model on kluster.ai from os import environ from openai import OpenAI from getpass import getpass import json import time # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen3-235B-A22B-FP8\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen3-235B-A22B-FP8\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen3-235B-A22B-FP8\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a multilingual, experienced maths tutor.\" , }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem in Spanish\" , }, ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results and log result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Print response to console print ( f \" \\n üîç AI batch response:\" ) print ( results ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) CLI ÔºÉ Similarly, the following curl commands showcase how to easily send a chat completion request to kluster.ai for the different supported models. This example assumes you've exported your kluster.ai API key as the variable API_KEY . DeepSeek-R1 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" DeepSeek-R1-0528 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1-0528\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1-0528\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-R1-0528\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" DeepSeek-V3-0324 #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Gemma 3 27B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai...\\n\" # Define image URLs # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"google/gemma-3-27b-it\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is this?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image1_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"google/gemma-3-27b-it\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Extract the text, find typos if any.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image2_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"google/gemma-3-27b-it\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who can park in the area?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image3_url\"}}]}],\"max_completion_tokens\": 1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nImage1 URL: $image1_url \" echo -e \"\\nImage2 URL: $image2_url \" echo -e \"\\nImage3 URL: $image3_url \" echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Meta Llama 3.1 8B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Meta Llama 3.3 70B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Meta Llama 4 Maverick #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai...\\n\" # Define image URLs # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is this?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image1_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Extract the text, find typos if any.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image2_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who can park in the area?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image3_url\"}}]}],\"max_completion_tokens\": 1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nImage1 URL: $image1_url \" echo -e \"\\nImage2 URL: $image2_url \" echo -e \"\\nImage3 URL: $image3_url \" echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Meta Llama 4 Scout #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai...\\n\" # Define image URLs # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is this?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image1_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Extract the text, find typos if any.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image2_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who can park in the area?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image3_url\"}}]}],\"max_completion_tokens\": 1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nImage1 URL: $image1_url \" echo -e \"\\nImage2 URL: $image2_url \" echo -e \"\\nImage3 URL: $image3_url \" echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Mistral NeMo #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"mistralai/Mistral-Nemo-Instruct-2407\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"mistralai/Mistral-Nemo-Instruct-2407\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"mistralai/Mistral-Nemo-Instruct-2407\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Qwen2.5-VL 7B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai...\\n\" # Define image URLs # Newton's cradle image1_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\" # Text with typos image2_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\" # Parking sign image3_url = \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is this?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image1_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Extract the text, find typos if any.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image2_url\"}}]}],\"max_completion_tokens\": 1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who can park in the area?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"$image3_url\"}}]}],\"max_completion_tokens\": 1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nImage1 URL: $image1_url \" echo -e \"\\nImage2 URL: $image2_url \" echo -e \"\\nImage3 URL: $image3_url \" echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Qwen3-235B-A22B #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo \"Error: API_KEY environment variable is not set.\" > & 2 exit 1 fi echo -e \"üì§ Sending batch request to kluster.ai... \" # Create request with specified structure cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen3-235B-A22B-FP8\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen3-235B-A22B-FP8\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-4\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen3-235B-A22B-FP8\", \"messages\":[{\"role\": \"system\", \"content\": \"You are a multilingual, experienced maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem in Spanish\"}],\"max_completion_tokens\":1000}} EOF # Upload batch job file FILE_ID = $( curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" | jq -r '.id' ) echo \"File uploaded, file ID: $FILE_ID \" # Submit batch job BATCH_ID = $( curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"' $FILE_ID '\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' | jq -r '.id' ) echo \"Batch job submitted, job ID: $BATCH_ID \" # Poll the batch status until it's completed STATUS = \"in_progress\" while [[ \" $STATUS \" ! = \"completed\" ]] ; do echo \"Waiting for batch job to complete... Status: $STATUS \" sleep 10 # Wait for 10 seconds before checking again STATUS = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.status' ) done # Retrieve the batch output file kluster_OUTPUT_FILE = $( curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" | jq -r '.output_file_id' ) # Retrieve the results OUTPUT_CONTENT = $( curl -s https://api.kluster.ai/v1/files/ $kluster_OUTPUT_FILE /content \\ -H \"Authorization: Bearer $API_KEY \" ) # Log results echo -e \"\\nüîç AI batch response:\" echo \" $OUTPUT_CONTENT \" Batch inference flow ÔºÉ This section details the batch inference process using the kluster.ai API and DeepSeek R1 model, but you can adapt it to any of the supported models . Create batch jobs as JSON files ÔºÉ To begin the batch job workflow, you'll need to assemble your batch requests and add them to a JSON Lines file ( .jsonl ). Each request must include the following arguments: custom_id string - a unique request ID to match outputs to inputs method string - the HTTP method to use for the request. Currently, only POST is supported url string - the /v1/chat/completions endpoint body object - a request body containing: model string required - name of one of the supported models messages array required - a list of chat messages ( system , user , or assistant roles, and also image_url for images) Any optional chat completion parameters , such as temperature , max_completion_tokens , etc. Tip You can use a different model for each request you submit. The following examples generate requests and save them in a JSONL file, which is ready to be uploaded for processing. Python CLI import time import os from getpass import getpass from openai import OpenAI # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen3-235B-A22B-FP8\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) cat << EOF > my_batch_request.jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an experienced cook.\"}, {\"role\": \"user\", \"content\": \"What is the ultimate breakfast sandwich?\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"Qwen/Qwen3-235B-A22B-FP8\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a maths tutor.\"}, {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}],\"max_completion_tokens\":1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who can park in the area?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\"}}]}],\"max_completion_tokens\":1000}} EOF Warning For the free tier, the maximum number of batch requests (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. Upload batch job files ÔºÉ After you've created the JSON Lines file, you need to upload it using the files endpoint along with the intended purpose. Consequently, you need to set the purpose value to \"batch\" for batch jobs. The response will contain an id field; save this value as you'll need it in the next step, where it's referred to as input_file_id . You can view your uploaded files in the Files tab of the kluster.ai platform. Use the following command examples to upload your batch job files: Python curl # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Warning Remember that the maximum file size permitted is 100 MB. Submit a batch job ÔºÉ Next, submit a batch job by calling the batches endpoint and providing the id of the uploaded batch job file (from the previous section) as the input_file_id , and additional parameters to specify the job's configuration. The response includes an id that can be used to monitor the job's progress, as demonstrated in the next section. You can use the following snippets to submit your batch job: Python curl # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Monitor job progress ÔºÉ You can make periodic requests to the batches endpoint to monitor your batch job's progress. Use the id of the batch request from the preceding section as the batch_id to check its status. The job is complete when the status field returns \"completed\" . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. To see a complete list of the supported statuses, refer to the Retrieve a batch API reference page. You can use the following snippets to monitor your batch job: Python curl # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Retrieve results ÔºÉ To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id , which is returned from querying the batch's status (from the previous section). The output file will be a JSONL file, where each line contains the custom_id from your input file request and the corresponding response. You can use the following snippets to retrieve the results from your batch job: Python curl # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"üíæ Response saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_results.jsonl View the complete script Python import json import time import os from getpass import getpass from openai import OpenAI # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) print ( f \"üì§ Sending batch request to kluster.ai... \\n \" ) # Create request with specified structure requests = [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are an experienced cook.\" }, { \"role\" : \"user\" , \"content\" : \"What is the ultimate breakfast sandwich?\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-2\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"Qwen/Qwen3-235B-A22B-FP8\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a maths tutor.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the Pythagorean theorem.\" }, ], \"max_completion_tokens\" : 1000 , }, }, { \"custom_id\" : \"request-3\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Who can park in the area?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\" }, }, ], } ], \"max_completion_tokens\" : 1000 , }, }, # Additional tasks can be added here ] # Save tasks to a JSONL file (newline-delimited JSON) file_name = \"my_batch_request.jsonl\" with open ( file_name , \"w\" ) as file : for request in requests : file . write ( json . dumps ( request ) + \" \\n \" ) # Upload batch job file batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) # Submit batch job batch_request = client . batches . create ( input_file_id = batch_input_file . id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) # Poll the batch status until it's complete while True : batch_status = client . batches . retrieve ( batch_request . id ) print ( f \"Batch status: { batch_status . status } \" ) print ( f \"Completed tasks: { batch_status . request_counts . completed } / { batch_status . request_counts . total } \" ) if batch_status . status . lower () in [ \"completed\" , \"failed\" , \"cancelled\" ]: break time . sleep ( 10 ) # Wait for 10 seconds before checking again # Check if the Batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"üíæ Response saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) List all batch jobs ÔºÉ To list all of your batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. You can use the following snippets to list all of your batch jobs: Python curl import os from openai import OpenAI from getpass import getpass # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) # Log all batch jobs (limit to 3) print ( client . batches . list ( limit = 3 ) . to_dict ()) curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Cancel a batch job ÔºÉ To cancel a batch job currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, and the status will show as canceling. Once complete, the status will show as cancelled . You can use the following snippets to cancel a batch job: Python curl Example import os from openai import OpenAI from getpass import getpass # Get API key from user input api_key = os . environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) # Cancel batch job with specified ID client . batches . cancel ( \"mybatch-123\" ) Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } Summary ÔºÉ You have now experienced the complete batch inference job lifecycle using kluster.ai's batch API. In this guide, you've learned how to: Prepare and submit batch jobs with structured request inputs Track your job's progress in real-time Retrieve and handle job results View and manage your batch jobs Cancel jobs when needed The kluster.ai batch API is designed to efficiently and reliably handle your large-scale LLM workloads. If you have questions or suggestions, the support team would love to hear from you.",
      "scraped_at": 1749147205.0904174
    },
    "https://docs.kluster.ai/get-started/verify/reliability/workflow-integrations/": {
      "url": "https://docs.kluster.ai/get-started/verify/reliability/workflow-integrations/",
      "title": "Workflow Integrations | kluster.ai Docs",
      "content": "Workflow integrations ÔºÉ You can integrate the Verify reliability check feature into your favorite automation platforms with ready-to-use workflow templates. These pre-configured workflows connect directly to the kluster.ai API, allowing you to add AI verification capabilities to your existing processes in minutes. Prerequisites ÔºÉ Before getting started with the workflow integrations, ensure the following requirements are met: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. Workflow platform : Set up Dify , n8n , or your preferred automation tool Available Workflows ÔºÉ Dify ÔºÉ By using Dify , you can build AI applications with built-in reliability verification. This workflow seamlessly integrates Verify into your Dify chatbots and agents, ensuring every response is validated for accuracy and trustworthiness before reaching your users. Configure kluster.ai as a Model Provider Navigate to Settings and select Model Provider Click on Add Provider and choose OpenAI-API-compatible Enter these settings: Base URL : https://api.kluster.ai/v1 API Key : Your kluster.ai API key Model : Select from available models Save and test the connection to ensure it works properly. Set up the kluster verify node: Select the HTTP Request node kluster verify Add your API key to the Authorization header Import and Configure the Workflow Download the workflow template below and import it into your Dify workspace. The workflow comes pre-configured to verify AI responses in real-time. Download Dify Workflow n8n ÔºÉ Add verification checkpoints to your n8n automation pipelines. This workflow validates AI-generated content against your source documents, tools, or real-time data, perfect for ensuring accuracy in automated content generation and data processing workflows. Set Up API Credentials Select the OpenAI and choose Credentials . Then click Create New Base URL : https://api.kluster.ai/v1 API Key : Your kluster.ai API key Model : Select from available models Set up the kluster verify node API key: Open the kluster verify node and modify the headers as follow: Header Name : Authorization Header Value : Bearer YOUR_API_KEY Import and Configure the Workflow Download the workflow template below and import it via the n8n interface. The workflow includes pre-configured HTTP nodes that connect to the /v1/verify/reliability endpoint, handle request/response formatting, and parse verification results. Connect your data sources and configure output routing as needed. Download n8n Workflow Next Steps ÔºÉ Ready to build more reliable AI applications? Explore the API : Check the complete API reference for advanced configuration options. Learn verification methods : Dive into the dedicated reliability endpoint for detailed implementation patterns. Try the tutorial : Follow the hands-on reliability check tutorial with code examples.",
      "scraped_at": 1749147205.3298163
    },
    "https://docs.kluster.ai/api-reference/reference/#chat-completion-object": {
      "url": "https://docs.kluster.ai/api-reference/reference/#chat-completion-object",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147205.6473424
    },
    "https://docs.kluster.ai/api-reference/reference/#retrieve-a-batch": {
      "url": "https://docs.kluster.ai/api-reference/reference/#retrieve-a-batch",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147206.0017529
    },
    "https://docs.kluster.ai/get-started/get-api-key": {
      "url": "https://docs.kluster.ai/get-started/get-api-key",
      "title": "Get a kluster.ai API key | kluster.ai Docs",
      "content": "Generate your kluster.ai API key ÔºÉ The API key is a unique identifier that authenticates requests associated with your account. You must have at least one API key to access kluster.ai 's services. This guide will help you obtain an API key, the first step to leveraging kluster.ai's powerful and cost-effective AI capabilities. Create an account ÔºÉ If you haven't already created an account with kluster.ai, visit the registration page and take the following steps: Enter your full name Provide a valid email address Create a secure password Click the Sign up button Generate a new API key ÔºÉ After you've signed up or logged into the platform through the login page , take the following steps: Select API Keys on the left-hand side menu In the API Keys section, click the Issue New API Key button Enter a descriptive name for your API key in the popup, then click Create Key Copy and secure your API key ÔºÉ Once generated, your API key will be displayed Copy the key and store it in a secure location, such as a password manager Warning For security reasons, you won't be able to view the key again. If lost, you will need to generate a new one. Security tips Keep it secret - do not share your API key publicly or commit it to version control systems Use environment variables - store your API key in environment variables instead of hardcoding them Regenerate if compromised - if you suspect your API key has been exposed, regenerate it immediately from the API Keys section Managing your API keys ÔºÉ The API Key Management section allows you to efficiently manage your kluster.ai API keys. You can create, view, and delete API keys by navigating to the API Keys section. Your API keys will be listed in the API Key Management section. To delete an API key, take the following steps: Locate the API key you wish to delete in the list Click the trash bin icon ( ) in the Actions column Confirm the deletion when prompted Warning Once deleted, the API key cannot be used again and you must generate a new one if needed. Next steps ÔºÉ Now that you have your API key, you can start integrating kluster.ai's LLMs into your applications. Refer to our Getting Started guide for detailed instructions on using the API.",
      "scraped_at": 1749147206.020682
    },
    "https://docs.kluster.ai/get-started/get-api-key/": {
      "url": "https://docs.kluster.ai/get-started/get-api-key/",
      "title": "Get a kluster.ai API key | kluster.ai Docs",
      "content": "Generate your kluster.ai API key ÔºÉ The API key is a unique identifier that authenticates requests associated with your account. You must have at least one API key to access kluster.ai 's services. This guide will help you obtain an API key, the first step to leveraging kluster.ai's powerful and cost-effective AI capabilities. Create an account ÔºÉ If you haven't already created an account with kluster.ai, visit the registration page and take the following steps: Enter your full name Provide a valid email address Create a secure password Click the Sign up button Generate a new API key ÔºÉ After you've signed up or logged into the platform through the login page , take the following steps: Select API Keys on the left-hand side menu In the API Keys section, click the Issue New API Key button Enter a descriptive name for your API key in the popup, then click Create Key Copy and secure your API key ÔºÉ Once generated, your API key will be displayed Copy the key and store it in a secure location, such as a password manager Warning For security reasons, you won't be able to view the key again. If lost, you will need to generate a new one. Security tips Keep it secret - do not share your API key publicly or commit it to version control systems Use environment variables - store your API key in environment variables instead of hardcoding them Regenerate if compromised - if you suspect your API key has been exposed, regenerate it immediately from the API Keys section Managing your API keys ÔºÉ The API Key Management section allows you to efficiently manage your kluster.ai API keys. You can create, view, and delete API keys by navigating to the API Keys section. Your API keys will be listed in the API Key Management section. To delete an API key, take the following steps: Locate the API key you wish to delete in the list Click the trash bin icon ( ) in the Actions column Confirm the deletion when prompted Warning Once deleted, the API key cannot be used again and you must generate a new one if needed. Next steps ÔºÉ Now that you have your API key, you can start integrating kluster.ai's LLMs into your applications. Refer to our Getting Started guide for detailed instructions on using the API.",
      "scraped_at": 1749147204.3198748
    },
    "https://docs.kluster.ai/get-started/integrations/litellm/": {
      "url": "https://docs.kluster.ai/get-started/integrations/litellm/",
      "title": "Integrate LiteLLM with kluster.ai | kluster.ai Docs",
      "content": "Integrate LiteLLM with kluster.ai ÔºÉ LiteLLM is an open-source Python library that streamlines access to a broad range of Large Language Model (LLM) providers through a standardized interface inspired by the OpenAI format. By providing features like fallback mechanisms, cost tracking, and streaming support, LiteLLM reduces the complexity of working with different models, ensuring a more reliable and cost-effective approach to AI-driven applications. Integrating LiteLLM with the kluster.ai API enables the use of kluster.ai's powerful models alongside LiteLLM's flexible orchestration. This combination makes it simple to switch between models on the fly, handle token usage limits with context window fallback, and monitor usage costs in real-time‚Äîleading to robust, scalable, and adaptable AI workflows. Prerequisites ÔºÉ Before starting, ensure you have the following: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. A python virtual environment - this is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial LiteLLM installed - to install the library, use the following command: pip install litellm Configure LiteLLM ÔºÉ In this section, you'll learn how to integrate kluster.ai with LiteLLM. You'll configure your environment variables, specify a kluster.ai model, and make a simple request using LiteLLM's OpenAI-like interface. Import LiteLLM and its dependencies - create a new file (e.g., hello-litellm.py ) and start by importing the necessary Python modules: import os from litellm import completion Set your kluster.ai API key and Base URL - replace INSERT_API_KEY with your actual API key. If you don't have one yet, refer to the Get an API key guide # Set environment vars, shown in script for readability os . environ [ \"OPENAI_API_KEY\" ] = \"INSERT_KLUSTER_API_KEY\" os . environ [ \"OPENAI_API_BASE\" ] = \"https://api.kluster.ai/v1\" Define your conversation (system + user messages) - set up your initial system prompt and user message. The system message defines your AI assistant's role, while the user message is the actual question or prompt # Basic Chat messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of California?\" } ] Select your kluster.ai model - choose one of kluster.ai's available models that best fits your use case. Prepend the model name with openai/ so LiteLLM recognizes it as an OpenAI-like model request # Use an \"openai/...\" model prefix so LiteLLM treats this as an OpenAI-like call model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" Call the LiteLLM completion function - finally, invoke the completion function to send your request: response = completion ( model = model , messages = messages , max_tokens = 1000 , ) print ( response ) View complete script hello-litellm.py import os from litellm import completion # Set environment vars, shown in script for readability os . environ [ \"OPENAI_API_KEY\" ] = \"INSERT_KLUSTER_API_KEY\" os . environ [ \"OPENAI_API_BASE\" ] = \"https://api.kluster.ai/v1\" # Basic Chat messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of California?\" } ] # Use an \"openai/...\" model prefix so LiteLLM treats this as an OpenAI-like call model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" response = completion ( model = model , messages = messages , max_tokens = 1000 , ) print ( response ) Use the following command to run your script: python hello - litellm . py python hello-litellm.py ModelResponse(id='chatcmpl-9877dfe6-6f1d-483f-a392-d791b89c75d6', created=1739495162, model='klusterai/Meta-Llama-3.3-70B-Instruct-Turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The capital of California is Sacramento.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, refusal=None))], usage=Usage(completion_tokens=8, prompt_tokens=48, total_tokens=56, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, prompt_logprobs=None) That's it! You've successfully integrated LiteLLM with the kluster.ai API. Continue to learn how to experiment with more advanced features of LiteLLM. Explore LiteLLM features ÔºÉ In the previous section, you learned how to use LiteLLM with the kluster.ai API by properly configuring the model via an OpenAI-like call and configuring the API key and API base URL. The following sections demonstrate using LiteLLM's streaming response and multi-turn conversation features with the kluster.ai API. The following guide assumes you just finished the configuration exercise in the preceding section. If you haven't already done so, please complete the configuration steps in the Configure LiteLLM section before you continue. Use streaming responses ÔºÉ You can enable streaming by simply passing stream=True to the completion() function. Streaming returns a generator instead of a static response, letting you iterate over partial output chunks as they arrive. In the code sample below, each chunk is accessed in a for-in loop, allowing you to extract the textual content (e.g., chunk.choices[0].delta.content) rather than printing all metadata. To configure a streaming response, take the following steps: Update the messages system prompt and first user message - you can supply a user message or use the sample provided: messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful AI assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the significance of the California Gold Rush.\" }, ] Initiate a streaming request to the model - set stream=True in the completion() function to tell LiteLLM to return partial pieces (chunks) of the response as they become available rather than waiting for the entire response to be ready # --- 1) STREAMING CALL: Only print chunk text -------------------------------- try : response_stream = completion ( model = model , messages = messages , max_tokens = 300 , temperature = 0.3 , stream = True , # streaming enabled ) except Exception as err : print ( f \"Error calling model: { err } \" ) return print ( \" \\n --------- STREAMING RESPONSE (text only) ---------\" ) streamed_text = [] Isolate the returned text content - returning all of the streamed data will include a lot of excessive noise like token counts, etc. You can isolate the text content from the rest of the streamed response with the following code: # Iterate over each chunk from the streaming generator for chunk in response_stream : if hasattr ( chunk , \"choices\" ) and chunk . choices : # If the content is None, we replace it with \"\" (empty string) partial_text = getattr ( chunk . choices [ 0 ] . delta , \"content\" , \"\" ) or \"\" streamed_text . append ( partial_text ) print ( partial_text , end = \"\" , flush = True ) print ( \" \\n \" ) # new line after streaming ends Handle multi-turn conversation ÔºÉ LiteLLM can facilitate multi-turn conversations by maintaining message history in a sequential chain, enabling the model to consider the context of previous messages. This section demonstrates multi-turn conversation handling by updating the messages list each time we receive a new response from the assistant. This pattern can be repeated for as many turns as you need, continuously appending messages to maintain the conversational flow. Let's take a closer look at each step: Combine the streamed chunks of the first message - since the message is streamed in chunks, you must re-assemble them into a single message. After collecting partial responses in streamed_text , join them into a single string called complete_first_answer : # Combine the partial chunks into one string complete_first_answer = \"\" . join ( streamed_text ) Append the assistant's reply - to enhance the context of the conversation. Add complete_first_answer back into messages under the \"assistant\" role as follows: # Append the entire first answer to the conversation for multi-turn context messages . append ({ \"role\" : \"assistant\" , \"content\" : complete_first_answer }) Craft the second message to the assistant - append a new message object to messages with the user's next question as follows: # --- 2) SECOND CALL (non-streamed): Print just the text --------------------- messages . append ({ \"role\" : \"user\" , \"content\" : ( \"Thanks for that. Can you propose a short, 3-minute presentation outline \" \"about the Gold Rush, focusing on its broader implications?\" ), }) Ask the model to respond to the second question - this time, don't enable the streaming feature. Pass the updated messages to completion() with stream=False , prompting LiteLLM to generate a standard (single-shot) response as follows: try : response_2 = completion ( model = model , messages = messages , max_tokens = 300 , temperature = 0.6 , stream = False # non-streamed ) except Exception as err : print ( f \"Error calling model: { err } \" ) return Parse and print the second answer - extract response_2.choices[0].message[\"content\"] , store it in second_answer_text , and print to the console for your final output: print ( \"--------- RESPONSE 2 (non-streamed, text only) ---------\" ) second_answer_text = \"\" if response_2 . choices and hasattr ( response_2 . choices [ 0 ], \"message\" ): second_answer_text = response_2 . choices [ 0 ] . message . get ( \"content\" , \"\" ) or \"\" print ( second_answer_text ) You can view the full script below. It demonstrates a streamed response versus a regular response and how to handle a multi-turn conversation. View complete script hello-litellm.py import os import litellm.exceptions from litellm import completion # Set environment variables for kluster.ai os . environ [ \"OPENAI_API_KEY\" ] = \"INSERT_API_KEY\" # Replace with your key os . environ [ \"OPENAI_API_BASE\" ] = \"https://api.kluster.ai/v1\" def main (): model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful AI assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Explain the significance of the California Gold Rush.\" }, ] # --- 1) STREAMING CALL: Only print chunk text -------------------------------- try : response_stream = completion ( model = model , messages = messages , max_tokens = 300 , temperature = 0.3 , stream = True , # streaming enabled ) except Exception as err : print ( f \"Error calling model: { err } \" ) return print ( \" \\n --------- STREAMING RESPONSE (text only) ---------\" ) streamed_text = [] # Iterate over each chunk from the streaming generator for chunk in response_stream : if hasattr ( chunk , \"choices\" ) and chunk . choices : # If the content is None, we replace it with \"\" (empty string) partial_text = getattr ( chunk . choices [ 0 ] . delta , \"content\" , \"\" ) or \"\" streamed_text . append ( partial_text ) print ( partial_text , end = \"\" , flush = True ) print ( \" \\n \" ) # new line after streaming ends # Combine the partial chunks into one string complete_first_answer = \"\" . join ( streamed_text ) # Append the entire first answer to the conversation for multi-turn context messages . append ({ \"role\" : \"assistant\" , \"content\" : complete_first_answer }) # --- 2) SECOND CALL (non-streamed): Print just the text --------------------- messages . append ({ \"role\" : \"user\" , \"content\" : ( \"Thanks for that. Can you propose a short, 3-minute presentation outline \" \"about the Gold Rush, focusing on its broader implications?\" ), }) try : response_2 = completion ( model = model , messages = messages , max_tokens = 300 , temperature = 0.6 , stream = False # non-streamed ) except Exception as err : print ( f \"Error calling model: { err } \" ) return print ( \"--------- RESPONSE 2 (non-streamed, text only) ---------\" ) second_answer_text = \"\" if response_2 . choices and hasattr ( response_2 . choices [ 0 ], \"message\" ): second_answer_text = response_2 . choices [ 0 ] . message . get ( \"content\" , \"\" ) or \"\" print ( second_answer_text ) if __name__ == \"__main__\" : main () Put it all together ÔºÉ Use the following command to run your script: python hello-litellm.py You should see output that resembles the following: python streaming-litellm.py --------- STREAMING RESPONSE (text only) --------- The California Gold Rush, which occurred from 1848 to 1855, was a pivotal event in American history that had significant economic, social, and cultural impacts on the United States and the world. Here are some of the key reasons why the California Gold Rush was important: 1. **Mass Migration and Population Growth**: The Gold Rush triggered a massive influx of people to California, with estimates suggesting that over 300,000 people arrived in the state between 1848 and 1852. This migration helped to populate the western United States and contributed to the country's westward expansion. 2. **Economic Boom**: The Gold Rush created a huge economic boom, with thousands of people striking it rich and investing their newfound wealth in businesses, infrastructure, and other ventures. The gold rush helped to stimulate economic growth, create new industries, and establish California as a major economic hub. 3. **Technological Innovations**: The Gold Rush drove technological innovations, particularly in the areas of mining and transportation. The development of new mining techniques, such as hydraulic mining, and the construction of roads, bridges, and canals, helped to facilitate the extraction and transportation of gold. 4. **Impact on Native American Populations**: The Gold Rush had a devastating impact on Native American populations in California, who were forcibly removed from their lands, killed, or displaced by the influx of miners. The Gold Rush marked the beginning of a long and tragic period of colonization and marginalization for Native American communities in --------- RESPONSE 2 (non-streamed, text only) --------- Here's a suggested 3-minute presentation outline on the California Gold Rush, focusing on its broader implications: **Title:** The California Gold Rush: A Catalyst for Change **Introduction (30 seconds)** * Briefly introduce the California Gold Rush and its significance * Thesis statement: The California Gold Rush was a pivotal event in American history that had far-reaching implications for the country's economy, society, and politics. **Section 1: Economic Implications (45 seconds)** * Discuss how the Gold Rush stimulated economic growth and helped establish California as a major economic hub * Mention the impact on trade, commerce, and industry, including the growth of San Francisco and other cities * Highlight the role of the Gold Rush in shaping the US economy and contributing to the country's westward expansion **Section 2: Social and Cultural Implications (45 seconds)** * Discuss the impact of the Gold Rush on Native American populations, including forced removals, violence, and displacement * Mention the diversity of people who came to California during the Gold Rush, including immigrants from China, Latin America, and Europe * Highlight the social and cultural changes that resulted from this diversity, including the growth of cities and the development of new communities **Section 3: Lasting Legacy (45 seconds)** * Discuss the lasting legacy of the Gold Rush, including its contribution to the development of the US West Coast and the growth of the US economy * Mention the ongoing impact of the Gold Both responses appear to trail off abruptly, but that's because we limited the output to 300 tokens each. Feel free to tweak the parameters and rerun the script at your leisure!",
      "scraped_at": 1749147204.401299
    },
    "https://docs.kluster.ai/get-started/start-building/setup/": {
      "url": "https://docs.kluster.ai/get-started/start-building/setup/",
      "title": "Start building with the kluster.ai API | kluster.ai Docs",
      "content": "Start using the kluster.ai API ÔºÉ The kluster.ai API provides a straightforward way to work with Large Language Models (LLMs) at scale. It is compatible with OpenAI's API and SDKs , making it easy to integrate into your existing workflows with minimal code changes. Get your API key ÔºÉ Navigate to the kluster.ai developer console API Keys section and create a new key. You'll need this for all API requests. For step-by-step instructions, refer to the Get an API key guide. Set up the OpenAI client library ÔºÉ Developers can use the OpenAI libraries with kluster.ai with no changes. To start, you need to install the library: Python pip install \"openai>=1.0.0\" Once the library is installed, you can instantiate an OpenAI client pointing to kluster.ai with the following code and replacing INSERT_API_KEY : Python from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) Check the kluster.ai OpenAI compatibility page for detailed information about the integration. API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Where to go next ÔºÉ Guide Real-time inference Build AI-powered applications that deliver instant, real-time responses. Visit the guide Guide Batch inference Process large-scale data efficiently with AI-powered batch inference. Visit the guide Reference API reference Explore the complete kluster.ai API documentation and usage details. Reference",
      "scraped_at": 1749147205.6779847
    },
    "https://docs.kluster.ai/get-started/models/": {
      "url": "https://docs.kluster.ai/get-started/models/",
      "title": "Supported AI Models | kluster.ai Docs",
      "content": "Models on kluster.ai ÔºÉ kluster.ai offers a wide variety of open-source models for both real-time and batch inferences, with more being constantly added. This page covers all the models the API supports, with the API request limits for each. Model names ÔºÉ Each model supported by kluster.ai has a unique name that must be used when defining the model in the request. Model Model API name DeepSeek-R1 deepseek-ai/DeepSeek-R1 DeepSeek-R1-0528 deepseek-ai/DeepSeek-R1-0528 DeepSeek-V3-0324 deepseek-ai/DeepSeek-V3-0324 Gemma 3 27B google/gemma-3-27b-it Meta Llama 3.1 8B klusterai/Meta-Llama-3.1-8B-Instruct-Turbo Meta Llama 3.3 70B klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Meta Llama 4 Maverick meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 Meta Llama 4 Scout meta-llama/Llama-4-Scout-17B-16E-Instruct Mistral NeMo mistralai/Mistral-Nemo-Instruct-2407 Qwen2.5-VL 7B Qwen/Qwen2.5-VL-7B-Instruct Qwen3-235B-A22B Qwen/Qwen3-235B-A22B-FP8 Model comparison table ÔºÉ Model Description Real-time inference support Batch inference support Fine-tuning support Image analysis DeepSeek-R1 Mathematical problem-solving code generation complex data analysis. DeepSeek-R1-0528 Mathematical problem-solving code generation complex data analysis. DeepSeek-V3-0324 Natural language generation open-ended text creation contextually rich writing. Gemma 3 27B Multilingual applications extended-context tasks image analysis and complex reasoning. Llama 3.1 8B Low-latency or simple tasks cost-efficient inference. Llama 3.3 70B General-purpose AI balanced cost-performance. Llama 4 Maverick A state-of-the-art multimodal model with integrated vision and language understanding, optimized for complex reasoning, coding, and perception tasks Llama 4 Scout General-purpose multimodal AI extended context tasks and balanced cost-performance across text and vision. Mistral NeMo Natural language generation open-ended text creation contextually rich writing. Qwen2.5-VL 7B Visual question answering document analysis image-based reasoning multimodal chat. Qwen3-235B-A22B Qwen3's flagship 235 billion parameter model optimized with 8-bit quantization API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited",
      "scraped_at": 1749147205.7142377
    },
    "https://docs.kluster.ai/api-reference/reference/#the-request-input-object": {
      "url": "https://docs.kluster.ai/api-reference/reference/#the-request-input-object",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147205.3089478
    },
    "https://docs.kluster.ai/get-started/models/#api-request-limits": {
      "url": "https://docs.kluster.ai/get-started/models/#api-request-limits",
      "title": "Supported AI Models | kluster.ai Docs",
      "content": "Models on kluster.ai ÔºÉ kluster.ai offers a wide variety of open-source models for both real-time and batch inferences, with more being constantly added. This page covers all the models the API supports, with the API request limits for each. Model names ÔºÉ Each model supported by kluster.ai has a unique name that must be used when defining the model in the request. Model Model API name DeepSeek-R1 deepseek-ai/DeepSeek-R1 DeepSeek-R1-0528 deepseek-ai/DeepSeek-R1-0528 DeepSeek-V3-0324 deepseek-ai/DeepSeek-V3-0324 Gemma 3 27B google/gemma-3-27b-it Meta Llama 3.1 8B klusterai/Meta-Llama-3.1-8B-Instruct-Turbo Meta Llama 3.3 70B klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Meta Llama 4 Maverick meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 Meta Llama 4 Scout meta-llama/Llama-4-Scout-17B-16E-Instruct Mistral NeMo mistralai/Mistral-Nemo-Instruct-2407 Qwen2.5-VL 7B Qwen/Qwen2.5-VL-7B-Instruct Qwen3-235B-A22B Qwen/Qwen3-235B-A22B-FP8 Model comparison table ÔºÉ Model Description Real-time inference support Batch inference support Fine-tuning support Image analysis DeepSeek-R1 Mathematical problem-solving code generation complex data analysis. DeepSeek-R1-0528 Mathematical problem-solving code generation complex data analysis. DeepSeek-V3-0324 Natural language generation open-ended text creation contextually rich writing. Gemma 3 27B Multilingual applications extended-context tasks image analysis and complex reasoning. Llama 3.1 8B Low-latency or simple tasks cost-efficient inference. Llama 3.3 70B General-purpose AI balanced cost-performance. Llama 4 Maverick A state-of-the-art multimodal model with integrated vision and language understanding, optimized for complex reasoning, coding, and perception tasks Llama 4 Scout General-purpose multimodal AI extended context tasks and balanced cost-performance across text and vision. Mistral NeMo Natural language generation open-ended text creation contextually rich writing. Qwen2.5-VL 7B Visual question answering document analysis image-based reasoning multimodal chat. Qwen3-235B-A22B Qwen3's flagship 235 billion parameter model optimized with 8-bit quantization API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited",
      "scraped_at": 1749147204.3567505
    },
    "https://docs.kluster.ai/api-reference/reference/#create-chat-completion": {
      "url": "https://docs.kluster.ai/api-reference/reference/#create-chat-completion",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147204.692447
    },
    "https://docs.kluster.ai/get-started/verify/overview/": {
      "url": "https://docs.kluster.ai/get-started/verify/overview/",
      "title": "Overview of Verify | kluster.ai Docs",
      "content": "Verify ÔºÉ LLMs can generate non-factual or irrelevant information (hallucinations). For developers, this presents significant challenges: Difficulty in programmatically trusting LLM outputs. Increased complexity in error handling and quality assurance. Potential for cascading failures in chained AI operations. Requirement for manual review cycles, slowing down development and deployment. Traditional validation methods may involve complex rule sets, fine-tuning, or exhibit high false-positive rates, adding to the development burden. Verify is an intelligent verification service that validates LLM outputs in real-time. It's designed to give you the trust needed to deploy AI at scale in production environments where accuracy matters most. This page provides an overview of the Verify service. How Verify works ÔºÉ The Verify service functions as an intelligent agent. It assesses LLM output reliability based on three key inputs provided in the API call: prompt : The original input or question provided to the LLM. This gives context to the user's intent. output : The response generated by the LLM that requires validation. context (Optional) : Any source material or documents provided to the LLM (e.g., in RAG scenarios) against which the output's claims should be verified. Verify analyzes these inputs and can leverage real time internet access to validating claims against up-to-date public information, extending its capabilities beyond static knowledge bases. Performance benchmarks ÔºÉ Verify has been benchmarked against other solutions on HaluEval and HaluBench datasets (over 25,000 samples). Non-RAG Scenarios (Context-Free): Compared against CleanLab TLM (GPT 4o-mini, medium quality, optimized threshold). Results: Verify showed 11% higher overall accuracy, a 2.8% higher median F1 score (72.3% vs. 69.5%), and higher precision (fewer false positives). Response times are comparable (sub-10 seconds). RAG Validation (Context-Provided): Compared against Patronus AI's Lynx (70B) and CleanLab TLM. Results: On RAGTruth (factual consistency), Verify significantly outperformed Lynx 70B and CleanLab TLM. On DROP (numerical/logical reasoning), Verify showed competitive performance against Lynx and outperformed CleanLab TLM. Note: Lynx was trained on the training sets of DROP and RAGTruth, highlighting Verify's generalization capabilities to unseen data configurations. These results indicate Verify's effectiveness in diverse scenarios relevant to production AI systems. Target applications & use cases ÔºÉ Developers can integrate Verify into applications where LLM output accuracy is paramount: Automated content generation pipelines. Customer-facing chatbots and virtual assistants. Question-answering systems over private or public data (RAG). AI-driven data extraction and summarization tools. Internal workflow automation involving LLM-generated text.",
      "scraped_at": 1749147206.091629
    },
    "https://docs.kluster.ai/get-started/integrations/sillytavern": {
      "url": "https://docs.kluster.ai/get-started/integrations/sillytavern",
      "title": "Integrate SillyTavern with kluster.ai | kluster.ai Docs",
      "content": "Integrate SillyTavern with kluster.ai ÔºÉ SillyTavern is a locally installed customizable LLM user interface that focuses on persona-driven LLM interactions‚Äîletting you create unique characters or group chats for tasks like code reviews and text editing. It provides custom prompt fields, bookmarks for revisiting specific points in a conversation, and a mobile-friendly design to manage your chat sessions easily. By integrating SillyTavern with the kluster.ai API, you can tap into kluster.ai's high-performance language models as your primary or backup backend. This combination merges SillyTavern's customizable UI and advanced prompt options with kluster.ai's reliable inference, offering a scalable and tailored chat environment for casual users and AI enthusiasts. Prerequisites ÔºÉ Before starting, ensure you have the following: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. Configure SillyTavern ÔºÉ Launch SillyTavern and open it in your browser at http://127.0.0.1:8000/ (default port) Click on the API Connections icon (plug) in the top navigation menu In the API drop-down menu, select Chat Completion In the Chat Completion Source option, choose Custom (OpenAI-compatible) Enter the kluster.ai API endpoint in the Custom Endpoint (Base URL) field: https://api.kluster.ai/v1 There should be no trailing slash ( / ) at the end of the URL Paste your kluster.ai API Key into the designated field Enter a Model ID . For this example, you can enter: klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Click the Connect button. If you've configured the API correctly, you should see a üü¢ Valid message next to the button Select one of the kluster.ai-supported models from the Available Models drop-down menu That's it! You're now ready to start chatting with your bot powered by kluster.ai. Test the connection ÔºÉ Now that you've configured kluster.ai with SillyTavern, you can test the API connection by starting a new conversation. Follow these steps to get started: Click the menu icon on the bottom-left corner of the page Select Start New Chat to open a new chat with the model Type a message in the Type a message bar at the bottom and send it Verify that the chatbot has returned a response successfully Troubleshooting If you encounter errors, revisit the configuration instructions and double-check your API key and base URL and that you've received a Valid response after connecting the API (see step 8).",
      "scraped_at": 1749147206.6267278
    },
    "https://docs.kluster.ai/api-reference/": {
      "url": "https://docs.kluster.ai/api-reference/",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147206.9183714
    },
    "https://docs.kluster.ai/get-started/openai-compatibility/": {
      "url": "https://docs.kluster.ai/get-started/openai-compatibility/",
      "title": "Compatibility with OpenAI client libraries | kluster.ai Docs",
      "content": "OpenAI compatibility ÔºÉ The kluster.ai API is compatible with OpenAI 's API and SDKs, allowing seamless integration into your existing applications. If you already have an application running with the OpenAI client library, you can easily switch to kluster.ai's API with minimal changes. This ensures a smooth transition without the need for significant refactoring or rework. Configuring OpenAI to use kluster.ai's API ÔºÉ Developers can use the OpenAI libraries with kluster.ai with no changes. To start, you need to install the library: Python pip install \"openai>=1.0.0\" To start using kluster.ai with OpenAI's client libraries, set your API key and change the base URL to https://api.kluster.ai/v1 : Python from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) Unsupported OpenAI features ÔºÉ While kluster.ai's API is largely compatible with OpenAI's, the following sections outline the specific features and fields that are currently unsupported. Chat completions parameters ÔºÉ When creating a chat completion via the POST https://api.kluster.ai/v1/chat/completions endpoint , the following request parameters are not supported: messages[].name - attribute in system , user , and assistant type message objects messages[].refusal - attribute in assistant type message objects messages[].audio - attribute in assistant type message objects messages[].tool_calls - attribute in assistant type message objects store n modalities response_format service_tier stream_options The following request parameters are supported only with Llama models: tools tool_choice parallel_tool_calls The following request parameters are deprecated : messages[].function_call - attribute in assistant type message objects max_tokens - use max_completion_tokens instead function_call functions For more information on these parameters, refer to OpenAI's API documentation on creating chat completions . Chat completion object ÔºÉ The following fields of the chat completion object are not supported: system_fingerprint usage.completion_tokens_details usage.prompt_tokens_details For more information on these parameters, refer to OpenAI's API documentation on the chat completion object .",
      "scraped_at": 1749147206.055659
    },
    "https://docs.kluster.ai/get-started/integrations/crewai/": {
      "url": "https://docs.kluster.ai/get-started/integrations/crewai/",
      "title": "Integrate CrewAI with kluster.ai API | kluster.ai Docs",
      "content": "Integrate CrewAI with kluster.ai ÔºÉ CrewAI is a multi-agent platform that organizes specialized AI agents‚Äîeach with defined roles, tools, and goals‚Äîwithin a structured process to tackle complex tasks efficiently. CrewAI agents streamline workflows and deliver reliable, scalable solutions by coordinating tasks and ensuring smooth collaboration. This guide walks you through integrating kluster.ai with CrewAI to create and run a simple AI agent chatbot that leverages the kluster.ai API. Prerequisites ÔºÉ Before starting, ensure you have the following prerequisites: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. CrewAI installed - the Installation Guide on the CrewAI website will walk you through installing CrewAI, setting up a virtual Python environment, and creating a new project. Note that CrewAI requires a Python version >= 3.10 and < 3.13 Create a project with the CLI ÔºÉ Open your Python virtual environment, and then follow these steps to use the CrewAI CLI to create a new project: Create a project - following the installation guide, create your first project with the following command: crewai create crew INSERT_PROJECT_NAME Select model and provider - during setup, the CLI will ask you to choose a provider and a model. Select openai as the provider and then choose any available model. Because you'll configure kluster.ai as a custom model, your initial model choice won't affect the final integration. The CLI will prompt you for an OpenAI API key, but this isn‚Äôt required. Simply press enter to skip Build a simple AI agent ÔºÉ After finishing the CLI setup, you will see a src directory with files crew.py and main.py . This guide won't use these sample files because they include extra features outside the scope. Follow these steps to continue: Create your first file - create a hello_crew.py file in src/YOUR_PROJECT_NAME to correspond to a simple AI agent chatbot Import modules and select model - open hello_crew.py to add imports and define a custom LLM for kluster.ai by setting the following parameters: provider - you can specify openai_compatible model - choose one of kluster.ai's available models based on your use case. Regardless of which model you choose, prepend its name with openai/ to ensure CrewAI, which relies on LiteLLM, processes your requests correctly base_url - use https://api.kluster.ai/v1 to send requests to the kluster.ai endpoint api_key - replace INSERT_API_KEY in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide hello_crew.py import random from crewai import LLM , Agent , Crew , Process , Task from crewai.project import CrewBase , agent , crew , task @CrewBase class HelloWorldCrew : # Override any default YAML references agents_config = {} tasks_config = {} def __init__ ( self ): \"\"\" When this crew is instantiated, create a custom LLM with your base_url. \"\"\" self . custom_llm = LLM ( provider = \"openai_compatible\" , model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_KLUSTER_API_KEY\" ) This example overrides agents_config and tasks_config with empty dictionaries to tell CrewAI to ignore all YAML files and rely solely on your code, keeping this guide as streamlined as possible. Define your agent - set the agent's role, goal, and backstory, and assign the custom LLM (via the kluster.ai API) for generating creative greetings: hello_crew.py @agent def hello_agent ( self ) -> Agent : \"\"\" A super simple agent with a single purpose: greet the user in a friendly, varied way. \"\"\" return Agent ( role = \"HelloWorldAgent\" , goal = \"Greet the user in a fun and creative way.\" , backstory = \"I'm a friendly agent who greets everyone in a slightly different manner!\" , llm = self . custom_llm , verbose = True ) Give the agent a task - define a task that prompts the agent for a unique, creative greeting using randomness to avoid repetition. Passing this prompt to hello_agent() ensures varied responses. CrewAI requires an expected_output field, defined here as a short greeting: hello_crew.py @task def hello_task ( self ) -> Task : \"\"\" A task that asks the agent to produce a dynamic greeting. \"\"\" random_factor = random . randint ( 100000 , 999999 ) prompt = f \"\"\" You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: { random_factor } Example: \"Hey there, how's your day going?\" \"\"\" return Task ( description = prompt , expected_output = \"A short, creative greeting\" , agent = self . hello_agent () ) Tie it all together with a @crew method - add the following method to return the assembled Crew object with a single agent and task. This method enables CrewAI to coordinate the agent and task you defined: hello_crew.py @crew def hello_crew ( self ) -> Crew : \"\"\" Our entire 'Hello World' crew‚Äîonly 1 agent + 1 task in sequence. \"\"\" return Crew ( agents = self . agents , tasks = self . tasks , process = Process . sequential , verbose = True ) Set up the entry point for the agent - create a new file named hello_main.py . In hello_main.py , import and initialize the HelloWorldCrew class, call its hello_crew() method, and then kickoff() to launch the task sequence: hello_main.py #!/usr/bin/env python from hello_crew import HelloWorldCrew def run (): \"\"\" Kick off the HelloWorld crew with no inputs. \"\"\" HelloWorldCrew () . hello_crew () . kickoff ( inputs = {}) if __name__ == \"__main__\" : run () View complete script hello_crew.py import random from crewai import LLM , Agent , Crew , Process , Task from crewai.project import CrewBase , agent , crew , task @CrewBase class HelloWorldCrew : # Override any default YAML references agents_config = {} tasks_config = {} def __init__ ( self ): \"\"\" When this crew is instantiated, create a custom LLM with your base_url. \"\"\" self . custom_llm = LLM ( provider = \"openai_compatible\" , model = \"openai/klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_KLUSTER_API_KEY\" ) @agent def hello_agent ( self ) -> Agent : \"\"\" A super simple agent with a single purpose: greet the user in a friendly, varied way. \"\"\" return Agent ( role = \"HelloWorldAgent\" , goal = \"Greet the user in a fun and creative way.\" , backstory = \"I'm a friendly agent who greets everyone in a slightly different manner!\" , llm = self . custom_llm , verbose = True ) @task def hello_task ( self ) -> Task : \"\"\" A task that asks the agent to produce a dynamic greeting. \"\"\" random_factor = random . randint ( 100000 , 999999 ) prompt = f \"\"\" You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: { random_factor } Example: \"Hey there, how's your day going?\" \"\"\" return Task ( description = prompt , expected_output = \"A short, creative greeting\" , agent = self . hello_agent () ) @crew def hello_crew ( self ) -> Crew : \"\"\" Our entire 'Hello World' crew‚Äîonly 1 agent + 1 task in sequence. \"\"\" return Crew ( agents = self . agents , tasks = self . tasks , process = Process . sequential , verbose = True ) Put it all together ÔºÉ To run your agent, ensure you are in the same directory as your hello_main.py file, then use the following command: python hello_main.py Upon running the script, you'll see output that looks like the following: # Agent: HelloWorldAgent ## Task: You are a friendly greeting bot. Please produce a short, creative greeting that changes each time. Random factor: 896380 Example: \"Hey there, how's your day going?\" # Agent: HelloWorldAgent ## Final Answer: Hello, it's a beautiful day to shine, how's your sparkle today? And that's it! You've now successfully configured your AI agent harnessing CrewAI and the power of the kluster.ai API!",
      "scraped_at": 1749147206.223024
    },
    "https://docs.kluster.ai/get-started/dedicated-deployments/": {
      "url": "https://docs.kluster.ai/get-started/dedicated-deployments/",
      "title": "Launch dedicated deployments | kluster.ai Docs",
      "content": "Dedicated deployments ÔºÉ Dedicated deployments let you run a private instance of any Hugging Face text model on hardware reserved just for you. Enjoy full control, predictable per‚Äëminute billing, and zero per‚Äëtoken costs. This page covers how to create, use, and stop your dedicated deployments. Create a deployment ÔºÉ Ensure you're logged in to the kluster.ai platform , then navigate to the Dedicated deployments page, then press Launch deployment . Then, complete the following fields to configure your deployment: Deployment name : Enter a clear deployment name (e.g., mydedicated ) so you can spot it later in the console. Model selection : Paste the Hugging Face model ID or URL (e.g., deepseek-ai/DeepSeek-R1 ). If the model is private, provide a Hugging Face access token. Select hardware : Confirm a GPU configuration. Specify auto-shutdown : Set an auto‚Äëshutdown window for your instance to power down after a specified period of inactivity, between 15 minutes to 12 hours. Launch : Review the estimated price and then Click Launch deployment . Spin‚Äëup takes ‚âà20‚Äì30 min; once the status shows Running , copy the endpoint ID, as you'll use that to submit requests. Use your dedicated deployment ÔºÉ After waiting 20-30 minutes for your instance to spin up, you can call it by using the endpoint ID as the model name when making a request. If you're unsure of your endpoint ID, look for it in the Dedicated deployments page . To call your dedicated deployment, you'll need to provide the endpoint ID as the model name when making a request ( INSERT_ENDPOINT_ID in the following example): Python curl from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://api.kluster.ai/v1\" ) response = client . chat . completions . create ( model = \"INSERT_ENDPOINT_ID\" , # Your endpoint ID messages = [{ \"role\" : \"user\" , \"content\" : \"What is the best taco place in SF?\" }], ) print ( response . choices [ 0 ] . message . content ) curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"INSERT_ENDPOINT_ID\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the best taco place in SF?\"}] }' Stop your deployment ÔºÉ Click Stop next to your deployment on the Dedicated deployments page to shut your VM down immediately. Billing ends the moment it powers off. Otherwise, an auto‚Äëshutdown timer kicks in after your specified auto-shutdown period (between 15 minutes and 12 hours of inactivity), depending on the period you chose when spinning up the instance. Questions? Email support@kluster.ai , and we‚Äôll be happy to help!",
      "scraped_at": 1749147206.277924
    },
    "https://docs.kluster.ai/get-started/integrations/eliza/": {
      "url": "https://docs.kluster.ai/get-started/integrations/eliza/",
      "title": "Integrate eliza with kluster.ai | kluster.ai Docs",
      "content": "Integrate eliza with kluster.ai ÔºÉ eliza is an open-source framework designed to create and manage AI agents that can handle a variety of tasks, from simple chat interactions to more complex automation. In this guide, you'll learn how to integrate kluster.ai into eliza to leverage its powerful models and quickly set up your AI-driven workflows. Prerequisites ÔºÉ Before starting, ensure you have the following kluster prerequisites: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. Clone and install the eliza repository - follow the installation instructions on the eliza Quick Start guide Warning Pay careful attention to the eliza prerequisites, including the minimum supported versions of Node.js and pnpm. You will not be able to successfully follow this guide using npm or yarn. Stop at the Configure Environment section in the Quick Start guide, as this guide covers those steps Configure your environment ÔºÉ After installing eliza, it's simple to utilize kluster.ai with eliza. Only three main changes to the .env file are required. Create .env file - run the following command to generate a .env file from the eliza repository example: cp .env.example .env Set variables - update the following variables in the .env file: OPENAI_API_KEY - replace INSERT_API_KEY in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide OPENAI_API_URL - use https://api.kluster.ai/v1 to send requests to the kluster.ai endpoint OPENAI_DEFAULT_MODEL - choose one of kluster.ai's available models based on your use case. You should also set SMALL_OPENAI_MODEL , MEDIUM_OPENAI_MODEL , and LARGE_OPENAI_MODEL to the same value to allow seamless experimentation as different characters use different default models The OpenAI configuration section of your .env file should resemble the following: .env # OpenAI Configuration OPENAI_API_KEY = INSERT_KLUSTER_API_KEY OPENAI_API_URL = https://api.kluster.ai/v1 # Community Plugin for OpenAI Configuration OPENAI_DEFAULT_MODEL = klusterai/Meta-Llama-3.3-70B-Instruct-Turbo SMALL_OPENAI_MODEL = klusterai/Meta-Llama-3.3-70B-Instruct-Turbo MEDIUM_OPENAI_MODEL = klusterai/Meta-Llama-3.3-70B-Instruct-Turbo LARGE_OPENAI_MODEL = klusterai/Meta-Llama-3.3-70B-Instruct-Turbo Run and interact with your first agent ÔºÉ Now that you've configured your environment, you're ready to run your first agent! eliza has several characters you can interact with by prompting or through autonomous tasks like tweeting. This guide relies on the Dobby character for its minimal setup requirements. Verify character configuration - open the dobby.character.json file inside the characters folder. By default, Dobby uses the openai model, which you've already configured to use the kluster.ai API. The Dobby configuration should start with the following: dobby.character.json { \"name\" : \"Dobby\" , \"clients\" : [], \"modelProvider\" : \"openai\" // json truncated for clarity } Run the agent - run the following command from the project root directory to run the Dobby agent: pnpm start --character = \"characters/dobby.character.json\" Launch the UI - in another terminal window, run the following command to launch the web UI: pnpm start:client Your terminal output should resemble the following: pnpm start:client VITE v6.0.11 ready in 824 ms ‚ûú Local: http://localhost:5173/ ‚ûú Network: use --host to expose ‚ûú press h + enter to show help Open your browser - follow the prompts and open your browser to http://localhost:5173/ Put it all together ÔºÉ You can now interact with Dobby by selecting on the Send Message button and starting the conversation: That's it! You've successfully integrated eliza with the kluster.ai API. You're now ready to harness the power of AI agents with the kluster.ai API!",
      "scraped_at": 1749147206.1678784
    },
    "https://docs.kluster.ai/get-started/verify/reliability/chat-completion/": {
      "url": "https://docs.kluster.ai/get-started/verify/reliability/chat-completion/",
      "title": "Chat completion reliability endpoint | kluster.ai Docs",
      "content": "Reliability check via chat completion ÔºÉ Developers can access the reliability check feature via the regular chat completion endpoint. This allows you to validate responses in full conversation histories using the same format as the standard chat completions API. This approach enables verification of reliability within the complete context of a conversation. This guide provides a quick example of how the chat completion endpoint can be used for reliability checks. Prerequisites ÔºÉ Before getting started with reliability verification, ensure the following requirements are met: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. A virtual Python environment : (Optional) Recommended for developers using Python. It helps isolate Python installations in a virtual environment to reduce the risk of environment or package conflicts between your projects Required Python libraries : Install the following Python libraries: OpenAI Python API library : to access the openai module getpass : To handle API keys safely Integration options ÔºÉ You can access the reliability verification service in two flexible OpenAI compatible ways, depending on your preferred development workflow. For both, you'll need to set the model to klusterai/verify-reliability : OpenAI compatible endpoint : Use the OpenAI API /v1/chat/completions pointing to kluster.ai. OpenAI SDK : Configure kluster.ai with OpenAI libraries . Next, the chat.completions.create endpoint. Reliability checks via chat completions ÔºÉ This example shows how to use the service with the chat completion endpoint via the OpenAI /v1/chat/completions endpoint and OpenAI libraries, using the specialized klusterai/verify-reliability model to enable Verify reliability check. Python CLI from os import environ from openai import OpenAI from getpass import getpass # Get API key from user input api_key = environ . get ( \"API_KEY\" ) or getpass ( \"Enter your kluster.ai API key: \" ) print ( f \"üì§ Sending a reliability check request to kluster.ai... \\n \" ) # Initialize OpenAI client pointing to kluster.ai API client = OpenAI ( api_key = api_key , base_url = \"https://api.kluster.ai/v1\" ) # Create chat completion request completion = client . chat . completions . create ( model = \"klusterai/verify-reliability\" , # Note special model messages = [ { \"role\" : \"system\" , \"content\" : \"You are a knowledgeable assistant that provides accurate medical information.\" }, { \"role\" : \"user\" , \"content\" : \"Does vitamin C cure the common cold?\" }, { \"role\" : \"assistant\" , \"content\" : \"Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours.\" } ] ) # Extract the reliability verification response text_response = completion . choices [ 0 ] . message . content # Print response to console print ( text_response ) #!/bin/bash # Check if API_KEY is set and not empty if [[ -z \" $API_KEY \" ]] ; then echo -e \"\\nError: API_KEY environment variable is not set.\\n\" > & 2 fi echo -e \"üì§ Sending a chat completion request to kluster.ai...\\n\" # Submit real-time request curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"deepseek-ai/DeepSeek-R1\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a knowledgeable assistant that provides accurate medical information.\" }, { \"role\": \"user\", \"content\": \"Does vitamin C cure the common cold?\" }, { \"role\": \"assistant\", \"content\": \"Yes, taking large doses of vitamin C has been scientifically proven to cure the common cold within 24 hours.\" } ] }' Next steps ÔºÉ Learn how to use the reliability dedicated endpoint for simpler verification scenarios Review the complete API documentation for detailed endpoint specifications",
      "scraped_at": 1749147206.302951
    },
    "https://docs.kluster.ai/api-reference/reference/#list-supported-models": {
      "url": "https://docs.kluster.ai/api-reference/reference/#list-supported-models",
      "title": "API Reference | kluster.ai Docs",
      "content": "API reference ÔºÉ API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Chat ÔºÉ Create chat completion ÔºÉ POST https://api.kluster.ai/v1/chat/completions To create a chat completion, send a request to the chat/completions endpoint. Please ensure your request is compliant with the API request limits . Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . Show possible types System message object Show properties content string or array The contents of the system message. role string or null required The role of the messages author, in this case, system . User message object Show properties content string or array The contents of the user message. role string or null required The role of the messages author, in this case, user . Assistant message object Show properties content string or array The contents of the assistant message. role string or null required The role of the messages author, in this case, assistant . store boolean or null Whether or not to store the output of this chat completion request. Defaults to false . metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Indicates whether the request should be asynchronous. For more information, see the Submit an async request section. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. frequency_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood of repeating the same line verbatim. Defaults to 0 . logit_bias map Modify the likelihood of specified tokens appearing in the completion. Defaults to null . Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase the likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . top_logprobs integer or null An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used. max_completion_tokens integer or null An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. presence_penalty number or null Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0 . seed integer or null If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. stop string or array or null Up to four sequences where the API will stop generating further tokens. Defaults to null . stream boolean or null If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Defaults to false . temperature number or null The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1 . It is generally recommended to alter this or top_p but not both. top_p number or null An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1 . It is generally recommended to alter this or temperature but not both. Returns The created Chat completion object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) chat_completion = client . chat . completions . create ( model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" }, ], ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"What is the capital of Argentina?\" } ] }' Response { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Chat completion object ÔºÉ id string Unique identifier for the chat completion. object string The object type, which is always chat.completion . created integer The Unix timestamp (in seconds) of when the chat completion was created. model string The model used for the chat completion. You can use the models endpoint to retrieve the list of supported models . choices array A list of chat completion choices. Show properties index integer The index of the choice in the list of returned choices. message object A chat completion message generated by the model. Can be one of system , user , or assistant . Show properties content string or array The contents of the message. role string or null The role of the messages author. Can be one of system , user , or assistant . logprobs boolean or null Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message . Defaults to false . finish_reason string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call ( deprecated ) if the model called a function. stop_reason string or null The reason the model stopped generating text. usage object Usage statistics for the completion request. Show properties completion_tokens integer Number of tokens in the generated completion. prompt_tokens integer Number of tokens in the prompt. total_tokens integer Total number of tokens used in the request (prompt + completion). Chat completion object { \"id\" : \"chat-d187c103e189483485b3bcd3eb899c62\" , \"object\" : \"chat.completion\" , \"created\" : 1736136422 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 }, \"prompt_logprobs\" : null } Async ÔºÉ Submit an async request ÔºÉ Asynchronous inference is a cost-effective option when you don't need immediate results, such as for workloads that vary or have unpredictable timelines. Submitting an asynchronous inference request to the chat completions endpoint works like submitting a real-time request. The main difference is that you include a metadata object specifying the request as async and a completion_window , defining the time window you expect the response. Request model string required ID of the model to use. You can use the models endpoint to retrieve the list of supported models . messages array required A list of messages comprising the conversation so far. The messages object can be one of system , user , or assistant . endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. metadata object Required Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API. Show properties @kluster.ai object Required kluster.ai-specific options for the request. Show properties callback_url string A URL to which the system will send a callback when the request is complete. async boolean Required Indicates whether the request should be asynchronous. strict_completion_window boolean Indicates whether the request must be completed within the specified completion_window . If enabled and the request isn't completed within the window, it will be considered unsuccessful. completion_window string required The time frame within which the batch should be processed. The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . additionalProperties any Allows any other properties to be included in the metadata object without enforcing a specific schema for them. These properties can have any key and any value type. Returns The Batch object including the job ID. All async jobs are treated as batch jobs but submitted through the real-time chat completions endpoint. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Create a chat completion request with async flag in metadata chat_completion = client . chat . completions . create ( model = \"google/gemma-3-27b-it\" , messages = [ { \"role\" : \"user\" , \"content\" : \"Please give me your honest opinion on the best stock as an investment.\" } ], metadata = { \"@kluster.ai\" : { \"async\" : True , \"completion_window\" : \"24h\" } } ) print ( chat_completion . to_dict ()) Example request curl -s https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"google/gemma-3-27b-it\", \"messages\": [ { \"role\": \"user\", \"content\": \"Please give me your honest opinion on the best stock as an investment.\" } ], \"metadata\": { \"@kluster.ai\": { \"async\": true, \"completion_window\": \"24h\" } } }' Response { \"id\" : \"67783976bf636f79b49643ee_1743267849127\" , \"object\" : \"chat.completion\" , \"created\" : 1743267849 , \"model\" : \"google/gemma-3-27b-it\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"Your request has been queued for adaptive inference. Check your batch jobs for results.\" }, \"finish_reason\" : \"stop\" } ] } Batch ÔºÉ Submit a batch job ÔºÉ POST https://api.kluster.ai/v1/batches To submit a batch job, send a request to the batches endpoint. Please ensure your request is compliant with the API request limits . Request input_file_id string required The ID of an uploaded file that contains requests for the new batch. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. endpoint string required The endpoint to be used for all requests in the batch. Currently, only /v1/chat/completions is supported. completion_window string required The supported completion windows are 24, 48, and 72 hours to accommodate a range of use cases and budget requirements. The code samples provided utilize the 24-hour completion window. Learn more about how completion window selection affects cost by visiting the pricing section of the kluster.ai website . metadata Object or null Custom metadata for the batch. Returns The created Batch object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) batch_request = client . batches . create ( input_file_id = \"myfile-123\" , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" , ) print ( batch_request . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input_file_id\": \"myfile-123\", \"endpoint\": \"/v1/chat/completions\", \"completion_window\": \"24h\" }' Response { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } Retrieve a batch ÔºÉ GET https://api.kluster.ai/v1/batches/{batch_id} To retrieve a batch job, send a request to the batches endpoint with your batch_id . You can also monitor jobs in the Batch tab of the kluster.ai platform UI. Path parameters batch_id string required The ID of the batch to retrieve. Returns The Batch object matching the specified batch_id . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) client . batches . retrieve ( \"mybatch-123\" ) Example request curl -s https://api.kluster.ai/v1/batches/mybatch-123 \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} } Cancel a batch ÔºÉ POST https://api.kluster.ai/v1/batches/{batch_id}/cancel To cancel a batch job that is currently in progress, send a request to the cancel endpoint with your batch_id . Note that cancellation may take up to 10 minutes to complete, during which time the status will show as cancelling . Path parameters batch_id string required The ID of the batch to cancel. Returns The Batch object matching the specified ID. Python curl Example from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) client . batches . cancel ( \"mybatch-123\" ) # Replace with your batch id Example curl -s https://api.kluster.ai/v1/batches/ $BATCH_ID /cancel \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -X POST Response { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"cancelling\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1730821906\" , \"in_progress_at\" : \"1730821911\" , \"expires_at\" : \"1730821906\" , \"finalizing_at\" : null , \"completed_at\" : null , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : \"1730821906\" , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 3 , \"completed\" : 3 , \"failed\" : 0 }, \"metadata\" : {} } List all batch jobs ÔºÉ GET https://api.kluster.ai/v1/batches To list all batch jobs, send a request to the batches endpoint without specifying a batch_id . To constrain the query response, you can also use a limit parameter. Query parameters after string A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo , your subsequent call can include after=obj_foo in order to fetch the next page of the list. limit integer A limit on the number of objects to be returned. Limit can range between 1 and 100. Default is 20. Returns A list of paginated Batch objects . The status of a batch object can be one of the following: Status Description validating The input file is being validated. failed The input file failed the validation process. in_progress The input file was successfully validated and the batch is in progress. finalizing The batch job has completed and the results are being finalized. completed The batch has completed and the results are ready. expired The batch was not completed within the 24-hour time window. cancelling The batch is being cancelled (may take up to 10 minutes). cancelled The batch was cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . batches . list ( limit = 2 ) . to_dict ()) Example request curl -s https://api.kluster.ai/v1/batches \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"mybatch-123\" , \"object\" : \"batch\" , \"endpoint\" : \"/v1/chat/completions\" , \"errors\" : null , \"input_file_id\" : \"myfile-123\" , \"completion_window\" : \"24h\" , \"status\" : \"completed\" , \"output_file_id\" : \"myfile-123-output\" , \"error_file_id\" : null , \"created_at\" : \"1733832777\" , \"in_progress_at\" : \"1733832777\" , \"expires_at\" : \"1733919177\" , \"finalizing_at\" : \"1733832781\" , \"completed_at\" : \"1733832781\" , \"failed_at\" : null , \"expired_at\" : null , \"cancelling_at\" : null , \"cancelled_at\" : null , \"request_counts\" : { \"total\" : 4 , \"completed\" : 4 , \"failed\" : 0 }, \"metadata\" : {} }, { ... }, ], \"first_id\" : \"mybatch-123\" , \"last_id\" : \"mybatch-789\" , \"has_more\" : false , \"count\" : 1 , \"page\" : 1 , \"page_count\" : -1 , \"items_per_page\" : 9223372036854775807 } Batch object ÔºÉ id string The ID of the batch. object string The object type, which is always batch . endpoint string The kluster.ai API endpoint used by the batch. errors object Show properties object string The object type, which is always list . data array Show properties code string An error code identifying the error type. message string A human-readable message providing more details about the error. param string or null The name of the parameter that caused the error, if applicable. line integer or null The line number of the input file where the error occurred, if applicable. input_file_id string The ID of the input file for the batch. completion_window string The time frame within which the batch should be processed. status string The current status of the batch. output_file_id string The ID of the file containing the outputs of successfully executed requests. error_file_id string The ID of the file containing the outputs of requests with errors. created_at integer The Unix timestamp (in seconds) for when the batch was created. in_progress_at integer The Unix timestamp (in seconds) for when the batch started processing. expires_at integer The Unix timestamp (in seconds) for when the batch will expire. finalizing_at integer The Unix timestamp (in seconds) for when the batch started finalizing. completed_at integer The Unix timestamp (in seconds) for when the batch was completed. failed_at integer The Unix timestamp (in seconds) for when the batch failed. expired_at integer The Unix timestamp (in seconds) for when the batch expired. cancelling_at integer The Unix timestamp (in seconds) for when the batch started cancelling. cancelled_at integer The Unix timestamp (in seconds) for when the batch was cancelled. request_counts object The request counts for different statuses within the batch. Show properties total integer Total number of requests in the batch. completed integer Number of requests that have been completed successfully. failed integer Number of requests that have failed. Batch object { \"id\" : \"mybatch-123\" , \"completion_window\" : \"24h\" , \"created_at\" : 1733832777 , \"endpoint\" : \"/v1/chat/completions\" , \"input_file_id\" : \"myfile-123\" , \"object\" : \"batch\" , \"status\" : \"validating\" , \"cancelled_at\" : null , \"cancelling_at\" : null , \"completed_at\" : null , \"error_file_id\" : null , \"errors\" : null , \"expired_at\" : null , \"expires_at\" : 1733919177 , \"failed_at\" : null , \"finalizing_at\" : null , \"in_progress_at\" : null , \"metadata\" : {}, \"output_file_id\" : null , \"request_counts\" : { \"completed\" : 0 , \"failed\" : 0 , \"total\" : 0 } } The request input object ÔºÉ The per-line object of the batch input file. custom_id string A developer-provided per-request ID. method string The HTTP method to be used for the request. Currently, only POST is supported. url string The /v1/chat/completions endpoint. body map The JSON body of the input file. Request input object [ { \"custom_id\" : \"request-1\" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is the capital of Argentina?\" } ], \"max_tokens\" : 1000 } } ] The request output object ÔºÉ The per-line object of the batch output files. id string A unique identifier for the batch request. custom_id string A developer-provided per-request ID that will be used to match outputs to inputs. response object or null Show properties status_code integer The HTTP status code of the response. request_id string A unique identifier for the request. You can reference this request ID if you need to contact support for assistance. body map The JSON body of the response. error object or null For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure. Show properties code string A machine-readable error code. message string A human-readable error message. Request output object { \"id\" : \"batch-req-123\" , \"custom_id\" : \"request-1\" , \"response\" : { \"status_code\" : 200 , \"request_id\" : \"req-123\" , \"body\" : { \"id\" : \"chatcmpl-5a5ba6c6-2f95-4136-815b-23275c4f1efb\" , \"object\" : \"chat.completion\" , \"created\" : 1737472126 , \"model\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"The capital of Argentina is Buenos Aires.\" , \"tool_calls\" : [] }, \"logprobs\" : null , \"finish_reason\" : \"stop\" , \"stop_reason\" : null } ], \"usage\" : { \"prompt_tokens\" : 48 , \"total_tokens\" : 57 , \"completion_tokens\" : 9 , \"prompt_tokens_details\" : null }, \"prompt_logprobs\" : null } } } Files ÔºÉ Upload files ÔºÉ POST https://api.kluster.ai/v1/files/ Upload a JSON Lines file to the files endpoint. Please ensure your file is compliant with the API request limits . You can also view all your uploaded files in the Files tab of the kluster.ai platform. Request file file required The file object (not file name) to be uploaded. Warning For the free tier, the maximum number of batch request (lines in the JSONL file) must be less than 1000, and each file must not exceed 100 MB. For the standard tier, there is no limit to the number of batch requests, but the maximum batch file size is 100 MB per file. purpose string required The intended purpose of the uploaded file. Use batch for the batch API. Returns The uploaded File object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) batch_input_file = client . files . create ( file = open ( file_name , \"rb\" ), purpose = \"batch\" ) print ( batch_input_file . to_dict ()) Example request curl -s https://api.kluster.ai/v1/files \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: multipart/form-data\" \\ -F \"file=@my_batch_request.jsonl\" \\ -F \"purpose=batch\" Response { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Retrieve file content ÔºÉ GET https://api.kluster.ai/v1/files/{output_file_id}/content To retrieve the content of your batch jobs output file, send a request to the files endpoint specifying the output_file_id . The output file will be a JSONL file, where each line contains the custom_id from your input file request, and the corresponding response. Path parameters file_id string required The ID of the file to use for this request Returns The file content. Refer to the input and output format specifications for batch requests. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Get the status of the batch, which returns the output_file_id batch_status = client . batches . retrieve ( batch_request . id ) # Check if the batch completed successfully if batch_status . status . lower () == \"completed\" : # Retrieve the results result_file_id = batch_status . output_file_id results = client . files . content ( result_file_id ) . content # Save results to a file result_file_name = \"batch_results.jsonl\" with open ( result_file_name , \"wb\" ) as file : file . write ( results ) print ( f \"Results saved to { result_file_name } \" ) else : print ( f \"Batch failed with status: { batch_status . status } \" ) Example request curl -s https://api.kluster.ai/v1/files/kluster-output-file-123/content \\ -H \"Authorization: Bearer $API_KEY \" > batch_output.jsonl File object ÔºÉ id string The file identifier, which can be referenced in the API endpoints. object string The object type, which is always file . bytes integer The size of the file, in bytes. created_at integer The Unix timestamp (in seconds) for when the file was created. filename string The name of the file. purpose string The intended purpose of the file. Currently, only batch is supported. File object { \"id\" : \"myfile-123\" , \"bytes\" : 2797 , \"created_at\" : \"1733832768\" , \"filename\" : \"my_batch_request.jsonl\" , \"object\" : \"file\" , \"purpose\" : \"batch\" } Uploads ÔºÉ Create upload ÔºÉ POST https://api.kluster.ai/v1/uploads Creates an intermediate upload object that accepts additional parts. Each upload can hold up to 8 GB of data and expires one hour after creation. Upon completion, the uploaded parts are assembled into a single File object that functions as a regular file across the platform. Request bytes integer required The number of bytes in the file you are uploading. filename string required The name of the file to upload. mime_type string required The MIME type of the file. This must fall within the supported MIME types for your file purpose. purpose string required The intended purpose of the uploaded file. Accepted values are assistants , vision , batch , batch_output , fine-tune , or fine-tune-results . Returns The Upload object with status pending. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) upload = client . uploads . create ( purpose = \"fine-tune\" , filename = \"training_examples.jsonl\" , bytes = 2147483648 , mime_type = \"text/jsonl\" ) print ( upload . to_dict ()) Example request curl https://api.kluster.ai/v1/uploads \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"purpose\": \"batch\", \"filename\": \"training_examples.jsonl\", \"bytes\": 2147483648, \"mime_type\": \"text/jsonl\" }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"pending\" , \"expires_at\" : 1744747593 } Add upload part ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/parts Adds an upload part to an Upload object . A upload part represents a chunk of bytes from the file you are trying to upload. Each upload part can be at most 64 MB, and you can add upload parts until you hit the upload maximum of 8 GB. It is possible to add multiple upload parts in parallel. You can decide the intended order of the upload parts when you complete the upload. Path parameters upload_id string required The ID of the upload. Request data file required The chunk of bytes for this upload part. Returns The Upload part object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) # Open the file you want to upload and replace upload ID with open ( \"training_examples.jsonl\" , \"rb\" ) as file : part = client . uploads . parts . create ( upload_id = \"INSERT_UPLOAD_ID\" , data = file ) print ( part . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/parts \\ -H \"Authorization: Bearer $API_KEY \" \\ -F \"data=@training_examples.jsonl\" Response { \"id\" : \"67feb0c9f6f1dad39fec8d39\" , \"object\" : \"upload.part\" , \"created_at\" : 1744744649 , \"upload_id\" : \"67feae39d63649ef421416f8\" } Complete upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/complete Completes the upload. Within the returned Upload object , there is a nested file object that is ready to use in the rest of the platform. You can specify the order of the upload parts by passing in an ordered list of the part IDs. The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the upload object. No upload parts may be added after an upload is completed. Path parameters upload_id string required The ID of the upload. Request part_ids array required The ordered list of Part IDs. md5 string The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. Returns The Upload object with status completed with an additional file property containing the created usable file object. If the complete upload operation is unsuccessful, the upload object will remain in pending state. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) completed_upload = client . uploads . complete ( upload_id = \"INSERT_UPLOAD_ID\" , part_ids = [ \"INSERT_PART_ID\" , \"INSERT_PART_ID\" ] ) print ( completed_upload . to_dict ()) Example request curl -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/complete \\ -H \"Authorization: Bearer $API_KEY \" \\ -d '{ \"part_ids\": [\"INSERT_PART_ID\", \"INSERT_PART_ID\"] }' Response { \"id\" : \"67feae39d63649ef421416f8\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744743993 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"completed\" , \"expires_at\" : 1744747593 , \"file\" : { \"id\" : \"67feb2bff6f1dad39feca157\" , \"object\" : \"file\" , \"created_at\" : 1744745151 , \"filename\" : \"67783976bf636f79b49643ee/d0b911d4-1fcf-4809-b8af-ca65c6a689a0-e7726314-de6d-4cb7-bed7-822cc12f9bbc\" , \"purpose\" : \"batch\" , \"bytes\" : 1776 } } Cancel upload ÔºÉ POST https://api.kluster.ai/v1/uploads/{upload_id}/cancel Cancels the upload. No upload parts may be added after an upload is cancelled. Path parameters upload_id string required The ID of the upload. Returns The upload object with status cancelled. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) cancelled_upload = client . uploads . cancel ( upload_id = \"INSERT_UPLOAD_ID\" ) print ( cancelled_upload . to_dict ()) Example request curl -X POST -s https://api.kluster.ai/v1/uploads/INSERT_UPLOAD_ID/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"id\" : \"67feb41fb779611ad5b3b635\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1744745503 , \"filename\" : \"training_examples.jsonl\" , \"status\" : \"cancelled\" , \"expires_at\" : 1744749103 } Upload object ÔºÉ id string The upload unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload . bytes integer The intended number of bytes to be uploaded. created_at integer The Unix timestamp (in seconds) for when the upload was created. expires_at integer The Unix timestamp (in seconds) for when the upload will expire. filename string The name of the file to upload. purpose string The intended purpose of the uploaded file. status string The current status of the upload. Possible values are pending , completed , or cancelled . file undefined or null The ready file object after the upload is completed. Upload object { \"id\" : \"upload_abc123\" , \"object\" : \"upload\" , \"bytes\" : 2147483648 , \"created_at\" : 1719184911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" , \"status\" : \"completed\" , \"expires_at\" : 1719127296 , \"file\" : { \"id\" : \"file-xyz321\" , \"object\" : \"file\" , \"bytes\" : 2147483648 , \"created_at\" : 1719186911 , \"filename\" : \"training_examples.jsonl\" , \"purpose\" : \"fine-tune\" } } Upload part object ÔºÉ id string The upload part unique identifier, which can be referenced in API endpoints. object string The object type, which is always upload.part . created_at integer The Unix timestamp (in seconds) for when the UploadPart was created. upload_id string The ID of the Upload object that this upload part was added to. Upload part object { \"id\" : \"part_def456\" , \"object\" : \"upload.part\" , \"created_at\" : 1719185911 , \"upload_id\" : \"upload_abc123\" } Embeddings ÔºÉ Create embeddings ÔºÉ POST https://api.kluster.ai/v1/embeddings Creates an embedding vector representing the input text. Machine learning models and algorithms can easily consume these vector representations to understand semantic relationships between pieces of text. Request input string or array required Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for BAAI/bge-m3 ), it cannot be an empty string, and any array must be 2048 dimensions or less. model string required ID of the model to use. encoding_format string The format to return the embeddings in. Can be either float or base64 . Defaults to float . dimensions not supported Please note, the embeddings endpoint doesn‚Äôt support the dimensions parameter. Models such as BAAI/bge‚Äëm3 always return a fixed 1024‚Äëdimensional vector. Supplying dimensions will trigger a 400 Bad Request . Returns A list of Embedding objects . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) response = client . embeddings . create ( model = \"BAAI/bge-m3\" , input = \"The food was delicious and the waiter...\" , encoding_format = \"float\" ) print ( response ) Example request curl -s https://api.kluster.ai/v1/embeddings \\ -H \"Authorization: Bearer $API_KEY \" \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"BAAI/bge-m3\", \"encoding_format\": \"float\" }' Response { \"object\" : \"list\" , \"created\" : 1744935248 , \"model\" : \"BAAI/bge-m3\" , \"data\" : [ { \"object\" : \"embedding\" , \"index\" : 0 , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ] } ], \"usage\" : { \"prompt_tokens\" : 13 } } Embedding object ÔºÉ Represents an embedding vector returned by the embeddings endpoint. Properties embedding array The embedding vector, which is a list of floats. index integer The index of the embedding in the list of embeddings. object string The object type, which is always \"embedding\" . The embedding object { \"object\" : \"embedding\" , \"embedding\" : [ 0.00885772705078125 , -0.0010204315185546875 , -0.045135498046875 , 0.00478363037109375 , -0.02642822265625 , /* ... truncated for brevity ... */ -0.0168304443359375 , 0.0229339599609375 , 0.007648468017578125 , -0.03875732421875 , 0.05487060546875 ], \"index\" : 0 } Models ÔºÉ List supported models ÔºÉ GET https://api.kluster.ai/v1/models Lists the currently available models . You can use this endpoint to retrieve a list of all available models for the kluster.ai API. If you have created any fine-tuned models, they will also appear when you query this endpoint. Returns id string The model identifier, which can be referenced in the API endpoints. created integer The Unix timestamp (in seconds) when the model was created. object string The object type, which is always model . owned_by string The organization that owns the model. Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"http://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) print ( client . models . list () . to_dict ()) Example request curl https://api.kluster.ai/v1/models \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"id\" : \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1731336610 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" , \"object\" : \"model\" , \"created\" : 1733777629 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"deepseek-ai/DeepSeek-R1\" , \"object\" : \"model\" , \"created\" : 1737385699 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"Qwen/Qwen2.5-VL-7B-Instruct\" , \"object\" : \"model\" , \"created\" : 1741303075 , \"owned_by\" : \"qwen\" }, { \"id\" : \"deepseek-ai/DeepSeek-V3-0324\" , \"object\" : \"model\" , \"created\" : 1742848965 , \"owned_by\" : \"klusterai\" }, { \"id\" : \"google/gemma-3-27b-it\" , \"object\" : \"model\" , \"created\" : 1742913870 , \"owned_by\" : \"google\" }, { \"id\" : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" }, { \"id\" : \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" , \"object\" : \"model\" , \"created\" : 1743944115 , \"owned_by\" : \"meta\" } ] } Fine-tuning ÔºÉ Fine-tuning is the process of refining a pre-trained model on specialized data. By adjusting the parameters with new, domain-specific examples, the model performs better on targeted tasks while retaining the general knowledge learned in its original training. Supported models ÔºÉ Currently, two base models are supported for fine-tuning: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo - has a 64,000 tokens max context window, best for long-context tasks, cost-sensitive scenarios klusterai/Meta-Llama-3.3-70B-Instruct-Turbo - has a 32,000 tokens max context window, best for complex reasoning, high-stakes accuracy Create a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs To initiate a fine-tuning job for one of the supported models, first upload the dataset file (see Files section for instructions). Request training_file string required ID of an uploaded file that will serve as training data. This file must have purpose=\"fine-tune\" . model string required The base model ID to fine-tune. Must be a fine-tunable model, for example meta-llama/Meta-Llama-3.1-8B-Instruct or meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo . validation_file string or null Optionally specify a separate file to serve as your validation dataset. hyperparameters object or null Optionally specify an object containing hyperparameters for fine-tuning: Show properties batch_size number The number of training examples processed in one forward/backward pass. Larger batch sizes reduce the frequency of weight updates per epoch, leading to more stable gradients but slower updates. Gradient accumulation is used, so larger batches may increase the duration of the job. learning_rate_multiplier number A multiplier for the base step size used in model weight updates. Lower values slow training but improve precision (helping avoid overshooting optimal weights or overfitting). Higher values speed up convergence but risk instability. Adjust carefully to balance training efficiency and model performance. n_epochs number The number of times the entire training dataset is passed through the model. More epochs can improve learning but risk overfitting if the model memorizes training data. Monitor validation metrics to determine the optimal number. nickname string or null Add a custom suffix that will be appended to the output model name. This can help identify a fine tuned model. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI # Configure OpenAI client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" # Replace with your actual API key ) job = client . fine_tuning . jobs . create ( training_file = \"INSERT_TRAINING_FILE_ID\" , # ID from uploaded training file model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , hyperparameters = { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } ) print ( job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"training_file\": \"INSERT_TRAINING_FILE_ID\", \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"hyperparameters\": { \"batch_size\": 4, \"learning_rate_multiplier\": 1, \"n_epochs\": 3 } }' Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"queued\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } Retrieve a fine-tuning job ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id} Fetch details of a single fine-tuning job by specifying its fine_tuning_job_id . Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to retrieve. Returns A Fine-tuning job object . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) job_details = client . fine_tuning . jobs . retrieve ( \"INSERT_JOB_ID\" ) print ( job_details . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs/INSERT_JOB_ID \\ -H \"Authorization: Bearer INSERT_API_KEY\" Response { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 } }, \"integrations\" : [] } List all fine-tuning jobs ÔºÉ GET https://api.kluster.ai/v1/fine_tuning/jobs Retrieve a paginated list of all fine-tuning jobs. Query parameters after string A cursor for use in pagination. limit integer A limit on the number of objects returned (1 to 100). Default is 20. Returns A paginated list of Fine-tuning job objects . Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) jobs = client . fine_tuning . jobs . list ( limit = 3 ) print ( jobs . to_dict ()) Example request curl -s https://api.kluster.ai/v1/fine_tuning/jobs \\ -H \"Authorization: Bearer $API_KEY \" Response { \"object\" : \"list\" , \"data\" : [ { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae81b59b08392687ea5f69\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489717 , \"result_files\" : [], \"status\" : \"running\" , \"training_file\" : \"67ae81587772e8a89c8fd5cf\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }, { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ae7f7d965c187d5cda039f\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739489149 , \"result_files\" : [], \"status\" : \"cancelled\" , \"training_file\" : \"67ae7f7c965c187d5cda0397\" , \"hyperparameters\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 1 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 10 } }, \"integrations\" : [] } ], \"first_id\" : \"67ae81b59b08392687ea5f69\" , \"last_id\" : \"67abefddbee1f22fb0a742ef\" , \"has_more\" : true } Cancel a fine-tuning job ÔºÉ POST https://api.kluster.ai/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel To cancel a job that is in progress, send a POST request to the cancel endpoint with the job ID. Path parameters fine_tuning_job_id string required The ID of the fine-tuning job to cancel. Returns The Fine-tuning job object with updated status. Python curl Example request from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" ) cancelled_job = client . fine_tuning . jobs . cancel ( \"67ae7f7d965c187d5cda039f\" ) print ( cancelled_job . to_dict ()) Example request curl -X POST https://api.kluster.ai/v1/fine_tuning/jobs/67ae7f7d965c187d5cda039f/cancel \\ -H \"Authorization: Bearer INSERT_API_KEY\" \\ -H \"Content-Type: application/json\" Response { \"id\" : \"67ae7f7d965c187d5cda039f\" , \"object\" : \"fine_tuning.job\" , \"model\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\" , \"fine_tuned_model\" : null , \"status\" : \"cancelling\" , \"created_at\" : 1738382911 , \"training_file\" : \"file-123abc\" , \"validation_file\" : null , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 3 }, \"metrics\" : {}, \"error\" : null } Fine-tuning job object ÔºÉ object string The object type, which is always fine_tuning.job . id string Unique identifier for the fine-tuning job. model string ID of the base model being fine-tuned. created_at integer Unix timestamp (in seconds) when the fine-tuning job was created. finished_at integer Unix timestamp (in seconds) when the fine-tuning job was completed. fine_tuned_model string or null The ID of the resulting fine-tuned model if the job succeeded; otherwise null . result_files array Array of file IDs associated with the fine-tuning job results. status string The status of the fine-tuning job (e.g., pending , running , succeeded , failed , or cancelled ). training_file string ID of the uploaded file used for training data. hyperparameters object Training hyperparameters used in the job (e.g., batch_size , n_epochs , learning_rate_multiplier ). method object Details about the fine-tuning method used, including type and specific parameters. trained_tokens integer The total number of tokens processed during training. integrations array Array of integrations associated with the fine-tuning job. Example { \"object\" : \"fine_tuning.job\" , \"id\" : \"67ad3877720af9f9ba78b684\" , \"model\" : \"meta-llama/Llama-3.1-8B-Instruct\" , \"created_at\" : 1739405431 , \"finished_at\" : 1739405521 , \"fine_tuned_model\" : \"ft:meta-llama:Llama-3.1-8B-Instruct:personal:805b5d69\" , \"result_files\" : [], \"status\" : \"succeeded\" , \"training_file\" : \"67ad38760272045e7006171b\" , \"hyperparameters\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 }, \"method\" : { \"type\" : \"supervised\" , \"supervised\" : { \"batch_size\" : 4 , \"learning_rate_multiplier\" : 1 , \"n_epochs\" : 2 } }, \"trained_tokens\" : 3065 , \"integrations\" : [] }",
      "scraped_at": 1749147206.6069396
    },
    "https://docs.kluster.ai/tutorials/klusterai-api/text-classification/text-classification-openai-api/": {
      "url": "https://docs.kluster.ai/tutorials/klusterai-api/text-classification/text-classification-openai-api/",
      "title": "Using OpenAI API | kluster.ai Docs",
      "content": "Text classification with kluster.ai API ¬∂ Text classification is assigning a class/label to a given text, and it is a common go-to example to demonstrate how helpful an AI model can be. This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to classify a dataset based on a predefined set of categories. The example uses an extract from the IMDB top 1000 movies dataset and categorizes them into \"Action,\" \"Adventure,\" \"Comedy,\" \"Crime,\" \"Documentary,\" \"Drama,\" \"Fantasy,\" \"Horror,\" \"Romance,\" or \"Sci-Fi.\" You can adapt this example by using your data and categories relevant to your use case. With this approach, you can effortlessly process datasets of any scale, big or small, and obtain categorized results powered by a state-of-the-art language model. Prerequisites ¬∂ Before getting started, ensure you have the following: A kluster.ai account - sign up on the kluster.ai platform if you don't have one A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide Setup ¬∂ In this notebook, we'll use Python's getpass module to safely input the key. After execution, please provide your unique kluster.ai API key (ensure no spaces). In [1]: Copied! from getpass import getpass api_key = getpass ( \"Enter your kluster.ai API key: \" ) from getpass import getpass api_key = getpass(\"Enter your kluster.ai API key: \") Next, ensure you've installed OpenAI Python library: In [2]: Copied! % pip install - q openai %pip install -q openai Note: you may need to restart the kernel to use updated packages. With the OpenAI Python library installed, we import the necessary dependencies for the tutorial: In [3]: Copied! from openai import OpenAI import pandas as pd import time import json import os from IPython.display import clear_output , display from openai import OpenAI import pandas as pd import time import json import os from IPython.display import clear_output, display And then, initialize the client by pointing it to the kluster.ai endpoint, and passing your API key. In [4]: Copied! # Set up the client client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = api_key , ) # Set up the client client = OpenAI( base_url=\"https://api.kluster.ai/v1\", api_key=api_key, ) Get the data ¬∂ Now that you've initialized an OpenAI-compatible client pointing to kluster.ai, we can talk about the data. This notebook includes a preloaded sample dataset derived from the Top 1000 IMDb Movies dataset. It contains movie descriptions ready for classification. No additional setup is needed. Simply proceed to the next steps to begin working with this data. In [5]: Copied! df = pd . DataFrame ({ \"text\" : [ \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\" , \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\" , \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\" , \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\" , \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\" ] }) df = pd.DataFrame({ \"text\": [ \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\", \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\", \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\", \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\", \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\" ] }) Perform batch inference ¬∂ To execute the batch inference job, we'll take the following steps: Create the batch job file - we'll generate a JSON lines file with the desired requests to be processed by the model Upload the batch job file - once it is ready, we'll upload it to the kluster.ai platform using the API, where it will be processed. We'll receive a unique ID associated with our file Start the batch job - after the file is uploaded, we'll initiate the job to process the uploaded data, using the file ID obtained before Monitor job progress - (optional) track the status of the batch job to ensure it has been successfully completed Retrieve results - once the job has completed execution, we can access and process the resultant data This notebook is prepared for you to follow along. Run the cells below to watch it all come together. Create the batch job file ¬∂ This example selects the deepseek-ai/DeepSeek-V3-0324 model. If you'd like to use a different model, feel free to change it by modifying the model field. Please refer to the Supported models section for a list of the models we support. The following snippets prepare the JSONL file, where each line represents a different request. Note that each separate batch request can have its own model. Also, we are using a temperature of 0.5 but feel free to change it and play around with the different outcomes (but we are only asking to respond with a single word, the genre). In [6]: Copied! # Prompt SYSTEM_PROMPT = ''' Classify the main genre of the given movie description based on the following genres (Respond with only the genre): ‚ÄúAction‚Äù, ‚ÄúAdventure‚Äù, ‚ÄúComedy‚Äù, ‚ÄúCrime‚Äù, ‚ÄúDocumentary‚Äù, ‚ÄúDrama‚Äù, ‚ÄúFantasy‚Äù, ‚ÄúHorror‚Äù, ‚ÄúRomance‚Äù, ‚ÄúSci-Fi‚Äù. ''' # Models #model=\"deepseek-ai/DeepSeek-R1\" model = \"deepseek-ai/DeepSeek-V3-0324\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\" # Ensure the directory exists os . makedirs ( \"text_clasification\" , exist_ok = True ) # Create the batch job file with the prompt and content def create_batch_file ( df ): batch_list = [] for index , row in df . iterrows (): content = row [ 'text' ] request = { \"custom_id\" : f \"movie_classification- { index } \" , \"method\" : \"POST\" , \"url\" : \"/v1/chat/completions\" , \"body\" : { \"model\" : model , \"temperature\" : 0.5 , \"messages\" : [ { \"role\" : \"system\" , \"content\" : SYSTEM_PROMPT }, { \"role\" : \"user\" , \"content\" : content } ], } } batch_list . append ( request ) return batch_list # Save file def save_batch_file ( batch_list ): filename = f \"text_clasification/batch_job_request.jsonl\" with open ( filename , 'w' ) as file : for request in batch_list : file . write ( json . dumps ( request ) + ' \\n ' ) return filename # Prompt SYSTEM_PROMPT = ''' Classify the main genre of the given movie description based on the following genres (Respond with only the genre): ‚ÄúAction‚Äù, ‚ÄúAdventure‚Äù, ‚ÄúComedy‚Äù, ‚ÄúCrime‚Äù, ‚ÄúDocumentary‚Äù, ‚ÄúDrama‚Äù, ‚ÄúFantasy‚Äù, ‚ÄúHorror‚Äù, ‚ÄúRomance‚Äù, ‚ÄúSci-Fi‚Äù. ''' # Models #model=\"deepseek-ai/DeepSeek-R1\" model=\"deepseek-ai/DeepSeek-V3-0324\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\" # Ensure the directory exists os.makedirs(\"text_clasification\", exist_ok=True) # Create the batch job file with the prompt and content def create_batch_file(df): batch_list = [] for index, row in df.iterrows(): content = row['text'] request = { \"custom_id\": f\"movie_classification-{index}\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": { \"model\": model, \"temperature\": 0.5, \"messages\": [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": content} ], } } batch_list.append(request) return batch_list # Save file def save_batch_file(batch_list): filename = f\"text_clasification/batch_job_request.jsonl\" with open(filename, 'w') as file: for request in batch_list: file.write(json.dumps(request) + '\\n') return filename Let's run the functions we've defined before: In [7]: Copied! batch_list = create_batch_file ( df ) data_dir = save_batch_file ( batch_list ) print ( data_dir ) batch_list = create_batch_file(df) data_dir = save_batch_file(batch_list) print(data_dir) text_clasification/batch_job_request.jsonl Next, we can preview what that batch job file looks like: In [8]: Copied! ! head - n 1 text_clasification / batch_job_request . jsonl !head -n 1 text_clasification/batch_job_request.jsonl {\"custom_id\": \"movie_classification-0\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3-0324\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n Classify the main genre of the given movie description based on the following genres (Respond with only the genre):\\n \\u201cAction\\u201d, \\u201cAdventure\\u201d, \\u201cComedy\\u201d, \\u201cCrime\\u201d, \\u201cDocumentary\\u201d, \\u201cDrama\\u201d, \\u201cFantasy\\u201d, \\u201cHorror\\u201d, \\u201cRomance\\u201d, \\u201cSci-Fi\\u201d.\\n \"}, {\"role\": \"user\", \"content\": \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\"}]}} Upload batch job file to kluster.ai ¬∂ Now that we‚Äôve prepared our input file, it‚Äôs time to upload it to the kluster.ai platform. To do so, you can use the files.create endpoint of the client, where the purpose is set to batch . This will return the file ID, which we need to log for the next steps. In [9]: Copied! # Upload batch job request file with open ( data_dir , 'rb' ) as file : upload_response = client . files . create ( file = file , purpose = \"batch\" ) # Print job ID file_id = upload_response . id print ( f \"File uploaded successfully. File ID: { file_id } \" ) # Upload batch job request file with open(data_dir, 'rb') as file: upload_response = client.files.create( file=file, purpose=\"batch\" ) # Print job ID file_id = upload_response.id print(f\"File uploaded successfully. File ID: {file_id}\") File uploaded successfully. File ID: 6801403efba4aabfd069a682 Start the batch job ¬∂ Once the file has been successfully uploaded, we're ready to start (create) the batch job by providing the file ID we got in the previous step. To do so, we use the batches.create method, for which we need to set the endpoint to /v1/chat/completions . This will return the batch job details, with the ID. In [10]: Copied! # Create batch job with completions endpoint batch_job = client . batches . create ( input_file_id = file_id , endpoint = \"/v1/chat/completions\" , completion_window = \"24h\" ) print ( \" \\n Batch job created:\" ) batch_dict = batch_job . model_dump () print ( json . dumps ( batch_dict , indent = 2 )) # Create batch job with completions endpoint batch_job = client.batches.create( input_file_id=file_id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\" ) print(\"\\nBatch job created:\") batch_dict = batch_job.model_dump() print(json.dumps(batch_dict, indent=2)) Batch job created: { \"id\": \"6801403e0969ab67de09ec8d\", \"completion_window\": \"24h\", \"created_at\": 1744912446, \"endpoint\": \"/v1/chat/completions\", \"input_file_id\": \"6801403efba4aabfd069a682\", \"object\": \"batch\", \"status\": \"pre_schedule\", \"cancelled_at\": null, \"cancelling_at\": null, \"completed_at\": null, \"error_file_id\": null, \"errors\": [], \"expired_at\": null, \"expires_at\": 1744998846, \"failed_at\": null, \"finalizing_at\": null, \"in_progress_at\": null, \"metadata\": {}, \"output_file_id\": null, \"request_counts\": { \"completed\": 0, \"failed\": 0, \"total\": 0 } } /Users/kevin/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings: Expected `Errors` but got `list` with value `[]` - serialized value may not be as expected return self.__pydantic_serializer__.to_python( Check job progress ¬∂ Now that your batch job has been created, you can track its progress. To monitor the job's progress, we can use the batches.retrieve method and pass the batch job ID. The response contains an status field that tells us if it is completed or not, and the subsequent status of each job separately. The following snippet checks the status every 10 seconds until the entire batch is completed: In [11]: Copied! all_completed = False # Loop to check status every 10 seconds while not all_completed : all_completed = True output_lines = [] updated_job = client . batches . retrieve ( batch_job . id ) if updated_job . status != \"completed\" : all_completed = False completed = updated_job . request_counts . completed total = updated_job . request_counts . total output_lines . append ( f \"Job status: { updated_job . status } - Progress: { completed } / { total } \" ) else : output_lines . append ( f \"Job completed!\" ) # Clear the output and display updated status clear_output ( wait = True ) for line in output_lines : display ( line ) if not all_completed : time . sleep ( 10 ) all_completed = False # Loop to check status every 10 seconds while not all_completed: all_completed = True output_lines = [] updated_job = client.batches.retrieve(batch_job.id) if updated_job.status != \"completed\": all_completed = False completed = updated_job.request_counts.completed total = updated_job.request_counts.total output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\") else: output_lines.append(f\"Job completed!\") # Clear the output and display updated status clear_output(wait=True) for line in output_lines: display(line) if not all_completed: time.sleep(10) 'Job completed!' Get the results ¬∂ With the job completed, we'll retrieve the results and review the responses generated for each request. The results are parsed. To fetch the results from the platform, you need to retrieve the output_file_id from the batch job, and then use the files.content endpoint, providing that specific file ID. Note that the job status must be completed for you to retrieve the results! In [12]: Copied! #Parse results as a JSON object def parse_json_objects ( data_string ): if isinstance ( data_string , bytes ): data_string = data_string . decode ( 'utf-8' ) json_strings = data_string . strip () . split ( ' \\n ' ) json_objects = [] for json_str in json_strings : try : json_obj = json . loads ( json_str ) json_objects . append ( json_obj ) except json . JSONDecodeError as e : print ( f \"Error parsing JSON: { e } \" ) return json_objects # Retrieve results with job ID job = client . batches . retrieve ( batch_job . id ) result_file_id = job . output_file_id result = client . files . content ( result_file_id ) . content # Parse JSON results parsed_result = parse_json_objects ( result ) # Extract and print only the content of each response print ( \" \\n Extracted Responses:\" ) for item in parsed_result : try : content = item [ \"response\" ][ \"body\" ][ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ] print ( content ) except KeyError as e : print ( f \"Missing key in response: { e } \" ) #Parse results as a JSON object def parse_json_objects(data_string): if isinstance(data_string, bytes): data_string = data_string.decode('utf-8') json_strings = data_string.strip().split('\\n') json_objects = [] for json_str in json_strings: try: json_obj = json.loads(json_str) json_objects.append(json_obj) except json.JSONDecodeError as e: print(f\"Error parsing JSON: {e}\") return json_objects # Retrieve results with job ID job = client.batches.retrieve(batch_job.id) result_file_id = job.output_file_id result = client.files.content(result_file_id).content # Parse JSON results parsed_result = parse_json_objects(result) # Extract and print only the content of each response print(\"\\nExtracted Responses:\") for item in parsed_result: try: content = item[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"] print(content) except KeyError as e: print(f\"Missing key in response: {e}\") Extracted Responses: Romance Drama Drama Drama Crime Summary ¬∂ This tutorial used the chat completion endpoint to perform a simple text classification task with batch inference. This particular example clasified a series of movies based on their description. To submit a batch job we've: Created the JSONL file, where each line of the file represented a separate request Submitted the file to the platform Started the batch job, and monitored its progress Once completed, we fetched the results All of this using the OpenAI Python library and API, no changes needed! Kluster.ai's batch API empowers you to scale your workflows seamlessly, making it an invaluable tool for processing extensive datasets. As next steps, feel free to create your own dataset, or expand on top of this existing example. Good luck!",
      "scraped_at": 1749147206.148543
    },
    "https://docs.kluster.ai/get-started/start-building/": {
      "url": "https://docs.kluster.ai/get-started/start-building/",
      "title": "Start building with the kluster.ai API | kluster.ai Docs",
      "content": "Start using the kluster.ai API ÔºÉ The kluster.ai API provides a straightforward way to work with Large Language Models (LLMs) at scale. It is compatible with OpenAI's API and SDKs , making it easy to integrate into your existing workflows with minimal code changes. Get your API key ÔºÉ Navigate to the kluster.ai developer console API Keys section and create a new key. You'll need this for all API requests. For step-by-step instructions, refer to the Get an API key guide. Set up the OpenAI client library ÔºÉ Developers can use the OpenAI libraries with kluster.ai with no changes. To start, you need to install the library: Python pip install \"openai>=1.0.0\" Once the library is installed, you can instantiate an OpenAI client pointing to kluster.ai with the following code and replacing INSERT_API_KEY : Python from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) Check the kluster.ai OpenAI compatibility page for detailed information about the integration. API request limits ÔºÉ The following limits apply to API requests based on your plan : Trial Core Scale Enterprise Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 32k 4k 1000 20 30 1 DeepSeek-R1-0528 32k 4k 1000 20 30 1 DeepSeek-V3-0324 32k 4k 1000 20 30 1 Gemma 3 27B 32k 4k 1000 20 30 1 Meta Llama 3.1 8B 32k 4k 1000 20 30 1 Meta Llama 3.3 70B 32k 4k 1000 20 30 1 Meta Llama 4 Maverick 32k 4k 1000 20 30 1 Meta Llama 4 Scout 32k 4k 1000 20 30 1 Mistral NeMo 32k 4k 1000 20 30 1 Qwen2.5-VL 7B 32k 4k 1000 20 30 1 Qwen3-235B-A22B 32k 4k 1000 20 30 1 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 100k 100 600 10 DeepSeek-R1-0528 163k 163k 100k 100 600 10 DeepSeek-V3-0324 163k 163k 100k 100 600 10 Gemma 3 27B 64k 8k 100k 100 600 10 Meta Llama 3.1 8B 131k 131k 100k 100 600 10 Meta Llama 3.3 70B 131k 131k 100k 100 600 10 Meta Llama 4 Maverick 1M 1M 100k 100 600 10 Meta Llama 4 Scout 131k 131k 100k 100 600 10 Mistral NeMo 131k 131k 100k 100 600 10 Qwen2.5-VL 7B 32k 32k 100k 100 600 10 Qwen3-235B-A22B 40k 40k 100k 100 600 10 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k 500k 100 1200 25 DeepSeek-R1-0528 163k 163k 500k 100 1200 25 DeepSeek-V3-0324 163k 163k 500k 100 1200 25 Gemma 3 27B 64k 8k 500k 100 1200 25 Meta Llama 3.1 8B 131k 131k 500k 100 1200 25 Meta Llama 3.3 70B 131k 131k 500k 100 1200 25 Meta Llama 4 Maverick 1M 1M 500k 100 1200 25 Meta Llama 4 Scout 131k 131k 500k 100 1200 25 Mistral NeMo 131k 131k 500k 100 1200 25 Qwen2.5-VL 7B 32k 32k 500k 100 1200 25 Qwen3-235B-A22B 40k 40k 500k 100 1200 25 Model Context size [tokens] Max output [tokens] Max batch requests Concurrent requests Requests per minute Hosted fine-tuned models DeepSeek-R1 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-R1-0528 163k 163k Unlimited 100 Unlimited Unlimited DeepSeek-V3-0324 163k 163k Unlimited 100 Unlimited Unlimited Gemma 3 27B 64k 8k Unlimited 100 Unlimited Unlimited Meta Llama 3.1 8B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 3.3 70B 131k 131k Unlimited 100 Unlimited Unlimited Meta Llama 4 Maverick 1M 1M Unlimited 100 Unlimited Unlimited Meta Llama 4 Scout 131k 131k Unlimited 100 Unlimited Unlimited Mistral NeMo 131k 131k Unlimited 100 Unlimited Unlimited Qwen2.5-VL 7B 32k 32k Unlimited 100 Unlimited Unlimited Qwen3-235B-A22B 40k 40k Unlimited 100 Unlimited Unlimited Where to go next ÔºÉ Guide Real-time inference Build AI-powered applications that deliver instant, real-time responses. Visit the guide Guide Batch inference Process large-scale data efficiently with AI-powered batch inference. Visit the guide Reference API reference Explore the complete kluster.ai API documentation and usage details. Reference",
      "scraped_at": 1749147206.9533315
    },
    "https://docs.kluster.ai/get-started/openai-compatibility/#configuring-openai-to-use-klusterais-api": {
      "url": "https://docs.kluster.ai/get-started/openai-compatibility/#configuring-openai-to-use-klusterais-api",
      "title": "Compatibility with OpenAI client libraries | kluster.ai Docs",
      "content": "OpenAI compatibility ÔºÉ The kluster.ai API is compatible with OpenAI 's API and SDKs, allowing seamless integration into your existing applications. If you already have an application running with the OpenAI client library, you can easily switch to kluster.ai's API with minimal changes. This ensures a smooth transition without the need for significant refactoring or rework. Configuring OpenAI to use kluster.ai's API ÔºÉ Developers can use the OpenAI libraries with kluster.ai with no changes. To start, you need to install the library: Python pip install \"openai>=1.0.0\" To start using kluster.ai with OpenAI's client libraries, set your API key and change the base URL to https://api.kluster.ai/v1 : Python from openai import OpenAI client = OpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key ) Unsupported OpenAI features ÔºÉ While kluster.ai's API is largely compatible with OpenAI's, the following sections outline the specific features and fields that are currently unsupported. Chat completions parameters ÔºÉ When creating a chat completion via the POST https://api.kluster.ai/v1/chat/completions endpoint , the following request parameters are not supported: messages[].name - attribute in system , user , and assistant type message objects messages[].refusal - attribute in assistant type message objects messages[].audio - attribute in assistant type message objects messages[].tool_calls - attribute in assistant type message objects store n modalities response_format service_tier stream_options The following request parameters are supported only with Llama models: tools tool_choice parallel_tool_calls The following request parameters are deprecated : messages[].function_call - attribute in assistant type message objects max_tokens - use max_completion_tokens instead function_call functions For more information on these parameters, refer to OpenAI's API documentation on creating chat completions . Chat completion object ÔºÉ The following fields of the chat completion object are not supported: system_fingerprint usage.completion_tokens_details usage.prompt_tokens_details For more information on these parameters, refer to OpenAI's API documentation on the chat completion object .",
      "scraped_at": 1749147206.1898987
    },
    "https://docs.kluster.ai/": {
      "url": "https://docs.kluster.ai/",
      "title": "kluster.ai Documentation",
      "content": "Simplify deployments and management of AI workloads Explore how to use kluster.ai to run Large Language Models on a distributed AI platform from providers all around the globe. Try our API Get an API key Obtain your API key to access the kluster.ai platform and start building and manging your workflows via our API. Generate API key Start coding Test-drive kluster.ai capabilities with a pre-built Colab notebook that lets you explore without writing a single line of code. Check notebooks Questions? We're here to help! Get answers to common questions about kluster.ai . Join Discord",
      "scraped_at": 1749147206.0759046
    },
    "https://docs.kluster.ai/get-started/integrations/langchain/": {
      "url": "https://docs.kluster.ai/get-started/integrations/langchain/",
      "title": "Integrate LangChain with kluster.ai | kluster.ai Docs",
      "content": "Integrate LangChain with kluster.ai ÔºÉ LangChain offers a range of features‚Äîlike memory modules for context tracking, retrieval augmentation to feed external data into prompts, and customizable multi-step ‚Äúchains\" to break down complex tasks. By leveraging these capabilities with the kluster.ai API, you can build more robust and context-aware solutions that seamlessly handle everything from short-form answers to intricate conversations. This guide demonstrates how to integrate the ChatOpenAI class from the langchain_openai package with the kluster.ai API, then walks through building a multi-turn conversational agent that leverages LangChain's memory for context-aware interactions. Prerequisites ÔºÉ Before starting, ensure you have the following: A kluster.ai account : Sign up on the kluster.ai platform if you don't have one. A kluster.ai API key : After signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide. A python virtual environment - this is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial LangChain packages installed - install the langchain packages : pip install langchain langchain_community langchain_core langchain_openai As a shortcut, you can also run: pip install \"langchain[all]\" Quick Start ÔºÉ It's easy to integrate kluster.ai with LangChain‚Äîwhen configuring the chat model, point your ChatOpenAI instance to the correct base URL and configure the following settings: Base URL - use https://api.kluster.ai/v1 to send requests to the kluster.ai endpoint API key - replace INSERT_API_KEY in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide Select your model - choose one of kluster.ai's available models based on your use case from langchain_openai import ChatOpenAI llm = ChatOpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , # Replace with your actual API key model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , ) llm . invoke ( \"What is the capital of Nepal?\" ) That's all you need to start with LangChain and the kluster.ai API! Next, this guide will explore building a multi-turn conversational agent that showcases how memory and context can elevate your chatbot to a more interactive, intelligent experience. Build a multi-turn conversational agent ÔºÉ This section will explore what LangChain can do beyond a single prompt-and-response interaction. One standout feature of LangChain is its built-in memory, which tracks conversation context across multiple user queries. In the following steps, you'll set up a multi-turn conversational agent that takes advantage of this memory and seamlessly integrates with the kluster.ai API. Create file - create a new file called langchain-advanced.py using the following command in your terminal: touch langchain-advanced.py Import LangChain components - inside your new file, import the following components for memory management, prompt handling, and kluster.ai integration: from langchain.chains.conversation.memory import ConversationBufferMemory from langchain_community.chat_message_histories import ChatMessageHistory from langchain_core.messages import HumanMessage from langchain_core.prompts import ChatPromptTemplate , MessagesPlaceholder from langchain_openai import ChatOpenAI Create a memory instance - to store and manage the conversation's context, allowing the chatbot to remember previous user messages. # Create a memory instance to store the conversation message_history = ChatMessageHistory () memory = ConversationBufferMemory ( memory_key = \"chat_history\" , return_messages = True ) Configure the ChatOpenAI model - point to kluster.ai's endpoint with your API key and chosen model. Remember, you can always change the selected model based on your needs # Create your LLM, pointing to kluster.ai's endpoint llm = ChatOpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , ) Define a prompt template - include a system instruction for the assistant, a placeholder for the conversation history, and an input slot for the user's query # Define the prompt template, including the system instruction and placeholders prompt = ChatPromptTemplate . from_messages ([ ( \"system\" , \"You are a helpful assistant.\" ), MessagesPlaceholder ( variable_name = \"chat_history\" ), ( \"human\" , \" {input} \" ) ]) Create the ConversationChain - pass in the LLM, memory, and this prompt template so every new user query is automatically enriched with the stored conversation context and guided by the assistant's role # Create the conversation chain conversation = ConversationChain ( llm = llm , memory = memory , prompt = prompt ) Prompt the model with the first question - you can prompt the model with any question. The example chosen here is designed to demonstrate context awareness between questions # Send the first user prompt question1 = \"Hello! Can you tell me something interesting about the city of Kathmandu?\" print ( \"Question 1:\" , question1 ) response1 = conversation . predict ( input = question1 ) print ( \"Response 1:\" , response1 ) Pose a follow-up question - ask another question without resupplying the city name and notice how LangChain's memory implicitly handles the context. Return and print the questions and responses to see how the conversation informs each new query to create multi-turn interactions # Send a follow-up question referencing previous context question2 = \"What is the population of that city?\" print ( \" \\n Question 2:\" , question2 ) response2 = conversation . predict ( input = question2 ) print ( \"Response 2:\" , response2 ) View complete script langchain-advanced.py from langchain.chains import ConversationChain from langchain.chains.conversation.memory import ConversationBufferMemory from langchain_community.chat_message_histories import ChatMessageHistory from langchain_core.messages import HumanMessage from langchain_core.prompts import ChatPromptTemplate , MessagesPlaceholder from langchain_openai import ChatOpenAI # Create a memory instance to store the conversation message_history = ChatMessageHistory () memory = ConversationBufferMemory ( memory_key = \"chat_history\" , return_messages = True ) # Create your LLM, pointing to kluster.ai's endpoint llm = ChatOpenAI ( base_url = \"https://api.kluster.ai/v1\" , api_key = \"INSERT_API_KEY\" , model = \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" , ) # Define the prompt template, including the system instruction and placeholders prompt = ChatPromptTemplate . from_messages ([ ( \"system\" , \"You are a helpful assistant.\" ), MessagesPlaceholder ( variable_name = \"chat_history\" ), ( \"human\" , \" {input} \" ) ]) # Create the conversation chain conversation = ConversationChain ( llm = llm , memory = memory , prompt = prompt ) # Send the first user prompt question1 = \"Hello! Can you tell me something interesting about the city of Kathmandu?\" print ( \"Question 1:\" , question1 ) response1 = conversation . predict ( input = question1 ) print ( \"Response 1:\" , response1 ) # Send a follow-up question referencing previous context question2 = \"What is the population of that city?\" print ( \" \\n Question 2:\" , question2 ) response2 = conversation . predict ( input = question2 ) print ( \"Response 2:\" , response2 ) Put it all together ÔºÉ Use the following command to run your script: python langchain-advanced.py You should see output that resembles the following: python langchain.py Question 1: Hello! Can you tell me something interesting about the city of Kathmandu? Response 1: Kathmandu, the capital city of Nepal, is indeed a treasure trove of history, culture, and natural beauty. Here's something interesting: Kathmandu is home to the famous Boudhanath Stupa, a UNESCO World Heritage Site. It's one of the largest Buddhist stupas in the world and is considered a sacred site by Buddhists. The stupa is over 36 meters (118 feet) high and is built in a unique octagonal shape. Its massive size is so prominent that it can be seen from many parts of the city. Another fascinating fact is that Kathmandu has managed to conserve its rich cultural heritage, which dates back to the 12th century. You can see ancient temples, palaces, streets, and marketplaces that have been beautifully preserved and restored. Lastly, Kathmandu is also known for its Newar culture, which is the indigenous culture of the city. The Newars have a rich tradition of art, music, and cuisine, which is reflected in the vibrant festivals and celebrations that take place throughout the year. Would you like to know more about Kathmandu's culture, history, or maybe some of its modern attractions? Question 2: What is the population of that city? Response 2: Kathmandu, the capital city of Nepal, has a population of around 374,405 people (as per the 2021 estimates). However, the Kathmandu Valley, which includes the surrounding municipalities and areas, has a population of over 3.2 million people. When considering the larger metropolitan area that includes the neighboring cities like Lalitpur (Patan) and Bhaktapur, the population exceeds 5 million people, making it one of the largest urban agglomerations in Nepal. It's worth noting that Nepal's population density is relatively high, with many people living in urban areas. The Kathmandu Valley, in particular, is one of the most densely populated regions in the country. That's it! You've successfully integrated LangChain with the kluster.ai API, and your configured multi-turn conversational agent is ready to leverage the power of LangChain and the kluster.ai API. For more information about the capabilities of LangChain, be sure to check out the LangChain docs .",
      "scraped_at": 1749147206.2571282
    }
  }
}